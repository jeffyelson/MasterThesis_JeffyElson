{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ed78e0185729396\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 229.60it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d8a73572c5526b09.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71c34de47dd068c7.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9e4fe1ebeeeb9fd4.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='dataset_semanticattribution_nerfeatures_split.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"claim\", \"premise\", \"label\", \"category\", \"count_bf\", \"count_ca\", \"count_dis\",\n",
    "        \"count_food\", \"count_lipid\", \"count_treat\", \"pres_bf\", \"pres_ca\", \"pres_dis\",\n",
    "        \"pres_food\", \"pres_lipid\", \"pres_treat\", \"counte_bf\", \"counte_ca\", \"counte_dis\",\n",
    "        \"counte_food\", \"counte_lipid\", \"counte_treat\", \"prese_bf\", \"prese_ca\", \"prese_dis\",\n",
    "        \"prese_food\", \"prese_lipid\", \"prese_treat\", \"url\", \"entities\", \"entity_map\",\n",
    "        \"gold_exp\", \"gemini_exp\", \"gemini_label\",\"entity_ev\",\"entity_map_ev\", \"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f1db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\",\"category\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f56a0271a09887b4.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-80cde756ccac5cc2.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-4a220ab4c7e19e55.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 0,\n",
    "    \"entailment\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "\n",
    "        claim = item['claim']\n",
    "        premise = item['premise'].replace('\\n', '').replace('[','').replace(']','')\n",
    "        item['claim']=claim\n",
    "        item['premise']=premise\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            claim, premise,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "        item['input_ids'] = inputs['input_ids'].squeeze()  # Remove batch dimension\n",
    "        item['attention_mask']= inputs['attention_mask'].squeeze() # Remove batch dimension\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c5e0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.',\n",
       " 'premise': '; Unković, N.; Dimkić, I.; Janaćković, P.; Gavrilović, M.; Stanojević, O.; Vukojević, J. Frankincense and myrrh essential oils and burn incense fume against micro-inhabitants of sacral ambients.Shameem, I. Phytochemical & therapeutic potentials of Murr makki (.Oxidative stress and immunotoxic effects of lead and their amelioration with myrrh (Commiphora molmol) emulsion.Essential Oils: Magical Ingredients for Skin Care.Chakravarty, N.; Kellogg, C.; Alvarez, J.; Equils, O.; Morgan, M. UV Protection by Natural Products: C. myrrha Oil Versus Sunscreen.Hamidpour, R.; Hamidpour, S.; Hamidpour, M.; Shahlari, M. Frankincense (乳香 Rǔ Xiāng;.species): From the selection of traditional applications to the novel phytotherapy for the prevention and treatment of serious diseases.Chemistry and immunomodulatory activity of frankincense oil.Compositions containing Boswellia extracts.; Cooper, E. Frankincense and myrrh as remedies in children.',\n",
       " 'label': 1,\n",
       " 'category': 'General Health',\n",
       " 'input_ids': tensor([  499,    52,    52,   107,  1832,  1043,    19,  1664,   261,    16,\n",
       "         26309,   494,    12,   199,  1172,     8,  3179,    13,     8,  1133,\n",
       "             5,     1,     3,   117,   597,  9789,    23,     2,     6,   445,\n",
       "             5,   117, 18150,  2168,     2,     6,    27,     5,   117,  3049,\n",
       "             9,     2,  9789,    23,     2,     6,   276,     5,   117,   350,\n",
       "         31371,  9881,     2,     6,   283,     5,   117, 11469,    32,  1924,\n",
       "          2099,     2,     6,   411,     5,   117,   584,  1598,    32,  1924,\n",
       "          2099,     2,     6,   446,     5,  4937,    77,    75,  5167,    11,\n",
       "            82,    52,    52,   107,  1832, 10229,    11,  5958,    16,    75,\n",
       "          5167, 11349,    15,   581,  2179,    18,    77, 29884,     7,    13,\n",
       "          8802,  4900, 21128,     7,     5,   134,  1483,    15,    15,    51,\n",
       "             6,    27,     5,     3, 16977,   235, 14676,     3,   184, 12206,\n",
       "          1055,     7,    13,  8054,    52,   954,  8511,    23,    41,     5,\n",
       "           667,   226,    23,    26,  1528,  2189,    11, 17133, 14367,  1951,\n",
       "            13,   991,    11,    70, 28661,   257,    28,    82,    52,    52,\n",
       "           107,    41, 10205,    23, 19968,     9,     3,  4641,  4641,    61,\n",
       "             3,    15,  4115,  1938,     5,   427,     7,     7,  7220,  6067,\n",
       "             7,    10,  9222,   138, 23482,     7,    21, 13038,  2686,     5,\n",
       "           254, 15416, 11852,  1408,    63,     6,   445,     5,   117, 13329,\n",
       "          2152,   122,     6,   205,     5,   117,   901,  4331,   457,     6,\n",
       "           446,     5,   117, 21263,    40,     7,     6,   411,     5,   117,\n",
       "         11147,     6,   283,     5, 10060,  8009,    57,  6869,  7554,    10,\n",
       "           205,     5,    82,    52,    52,  1024,  6067, 13262,   302,  3068,\n",
       "          8527,     5,   566,     9,  6983, 13399,     6,   391,     5,   117,\n",
       "          1626,  6983, 13399,     6,   180,     5,   117,  1626,  6983, 13399,\n",
       "             6,   283,     5,   117, 19669,    40,  1665,     6,   283,     5,\n",
       "          4937,    77,    75,  5167,    41,     2,   391,     2,     3,     4,\n",
       "            23,     2,  1725,   117,     5,  7576,   725,    61,    10,  1029,\n",
       "             8,  1801,    13,  1435,  1564,    12,     8,  3714, 30512, 10896,\n",
       "            21,     8,  9793,    11,  1058,    13,  2261,  6716,     5,  3541,\n",
       "         12641,  8224,    11, 17133, 22763,  6546,  1756,    13,     3,  6296,\n",
       "          2917,    75,  5167,  1043,     5,  5890,  4718,     7,     3,  6443,\n",
       "          1491,     7,  2091,    23,     9,  5819,     7,     5,   117, 10078,\n",
       "             6,   262,     5,  4937,    77,    75,  5167,    11,    82,    52,\n",
       "            52,   107,    38, 20203,    16,   502,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 42:06, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.886300</td>\n",
       "      <td>0.799861</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.544130</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.572926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.764673</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.611819</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.630654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.870856</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.636418</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.644748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>1.096024</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.644415</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.647512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.564200</td>\n",
       "      <td>1.401636</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.651252</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.650711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>1.655139</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.649681</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.645948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>2.200283</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.644550</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.640666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>2.339462</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.651066</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.639789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>2.458558</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.642905</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.640741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>2.632831</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.649995</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.646080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>2.869454</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.641915</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.635542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>3.012104</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.649699</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.644817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>3.149813</td>\n",
       "      <td>0.636559</td>\n",
       "      <td>0.658422</td>\n",
       "      <td>0.636559</td>\n",
       "      <td>0.646361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>3.194929</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.657636</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.647118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>3.190753</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.656631</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.647811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.1_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.2.1_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.1_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.2.1_flant5/checkpoint-203 (score: 0.6795698924731183).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 01:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.2.1_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.2.1_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.2.1_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.2.1_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.2.1_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.2.1_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.2.1_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.2.1_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.2.1_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.2.1_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.2.1_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.2.1_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.2.1_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.2.1_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.2.1_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.2.1_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.2.1_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.2.1_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-0.18117598,  1.2022704 , -1.2650553 ],\n",
      "       [ 0.3142954 ,  1.6176083 , -1.980114  ],\n",
      "       [-0.46837243,  1.4232664 , -1.2079489 ],\n",
      "       [ 0.36779153,  1.5423898 , -2.0520165 ],\n",
      "       [ 0.6199041 ,  1.0615779 , -1.7161404 ],\n",
      "       [ 0.05210482,  2.1601913 , -2.326343  ],\n",
      "       [-0.18066828,  1.1886368 , -1.1513442 ],\n",
      "       [-0.53056544,  1.3160526 , -1.0660194 ],\n",
      "       [-0.45456278,  1.8161242 , -1.6636243 ],\n",
      "       [ 0.07286944,  1.1330833 , -1.3509222 ],\n",
      "       [-0.07864268,  1.167221  , -1.3292598 ],\n",
      "       [-0.14985503,  1.3873892 , -1.4818606 ],\n",
      "       [ 0.55160713,  1.2389324 , -1.7443116 ],\n",
      "       [ 0.59537894,  0.5789019 , -1.098087  ],\n",
      "       [-0.15284307,  0.7885192 , -0.81585026],\n",
      "       [-0.29941085,  1.1267908 , -1.0722989 ],\n",
      "       [-0.4688474 ,  1.7526101 , -1.50858   ],\n",
      "       [ 0.5605945 ,  1.2423924 , -1.7867109 ],\n",
      "       [-0.10827423,  1.4885786 , -1.45004   ],\n",
      "       [ 0.53863597,  1.8451033 , -2.3629932 ],\n",
      "       [ 0.23961347,  1.1835704 , -1.5198506 ],\n",
      "       [ 0.59271944,  1.178969  , -1.7166551 ],\n",
      "       [ 0.16442876,  1.4267545 , -1.6375287 ],\n",
      "       [ 0.15856002,  1.4067278 , -1.7364146 ],\n",
      "       [-0.23613015,  1.58676   , -1.5812843 ],\n",
      "       [ 0.23261118,  1.0389419 , -1.3662947 ],\n",
      "       [-0.20710419,  1.448812  , -1.5023385 ],\n",
      "       [ 0.43426245,  1.6678784 , -2.1080847 ],\n",
      "       [-0.29666403,  1.2256308 , -1.1600003 ],\n",
      "       [-0.69986784,  0.8593691 , -0.38446143],\n",
      "       [ 0.45207927,  1.28757   , -1.720584  ],\n",
      "       [ 0.04575938,  1.701587  , -1.9114707 ],\n",
      "       [ 1.0499301 ,  1.3268058 , -2.2058523 ],\n",
      "       [ 0.01726844,  1.0353559 , -1.2032411 ],\n",
      "       [-0.07176566,  0.8616828 , -0.9610915 ],\n",
      "       [-0.70708954,  1.5140655 , -1.179341  ],\n",
      "       [-0.647857  ,  1.2143669 , -0.8132285 ],\n",
      "       [ 0.04306619,  1.1115484 , -1.2051724 ],\n",
      "       [-0.09369   ,  1.2784228 , -1.2525761 ],\n",
      "       [-0.09397713,  1.1806716 , -1.2093571 ],\n",
      "       [ 0.30086076,  1.224048  , -1.6212673 ],\n",
      "       [-0.50641197,  1.5288069 , -1.4207649 ],\n",
      "       [-0.15302323,  1.7839267 , -1.790556  ],\n",
      "       [ 0.55718964,  1.1242311 , -1.6771622 ],\n",
      "       [-0.02341543,  1.0932603 , -1.2186838 ],\n",
      "       [-0.49440798,  1.7280226 , -1.4947647 ],\n",
      "       [-0.30529365,  1.375836  , -1.2382646 ],\n",
      "       [ 0.3687942 ,  1.2390442 , -1.5830926 ],\n",
      "       [ 0.54879504,  1.181652  , -1.696289  ],\n",
      "       [ 0.55498195,  1.4388149 , -2.010644  ],\n",
      "       [ 0.64503837,  1.4503789 , -2.0345109 ],\n",
      "       [ 0.03786825,  1.1343668 , -1.2568027 ],\n",
      "       [-0.07001824,  0.9312909 , -1.0262144 ],\n",
      "       [-0.31928822,  1.3124148 , -1.2380904 ],\n",
      "       [ 0.29835662,  1.3278657 , -1.7231947 ],\n",
      "       [-0.5265863 ,  1.5444292 , -1.4312526 ],\n",
      "       [ 0.14471067,  0.8333582 , -1.1390532 ],\n",
      "       [-0.46153918,  1.6504627 , -1.5053122 ],\n",
      "       [ 0.30591515,  1.4771969 , -1.8557965 ],\n",
      "       [-0.02866686,  1.2275746 , -1.2785279 ],\n",
      "       [-0.0260097 ,  1.5442436 , -1.6041725 ],\n",
      "       [ 0.58177215,  1.1603479 , -1.7879146 ],\n",
      "       [-0.3506971 ,  1.9414847 , -1.8271577 ],\n",
      "       [ 0.5319345 ,  1.308459  , -1.7976911 ],\n",
      "       [-0.34384122,  1.8625695 , -1.6060385 ],\n",
      "       [-0.44452423,  0.93841904, -0.70639044],\n",
      "       [ 0.30992693,  1.2652441 , -1.6640891 ],\n",
      "       [ 0.4072626 ,  1.2803837 , -1.7087501 ],\n",
      "       [-0.22165196,  1.5400933 , -1.5166137 ],\n",
      "       [ 0.3290779 ,  0.44703433, -0.86496854],\n",
      "       [-0.5409378 ,  1.2385685 , -0.98945034],\n",
      "       [-0.08569603,  1.1265161 , -1.0763769 ],\n",
      "       [-0.4673857 ,  1.6524104 , -1.449766  ],\n",
      "       [-0.1246298 ,  1.6397212 , -1.6212913 ],\n",
      "       [ 0.21224782,  1.069348  , -1.3655654 ],\n",
      "       [-0.06146326,  1.1095328 , -1.1975371 ],\n",
      "       [ 0.39097232,  1.130377  , -1.5292976 ],\n",
      "       [ 0.49363625,  0.8083091 , -1.3991889 ],\n",
      "       [ 0.2046568 ,  1.2008477 , -1.4594913 ],\n",
      "       [ 0.01636881,  0.92419094, -1.0182769 ],\n",
      "       [ 0.42239988,  0.90295875, -1.4020677 ],\n",
      "       [ 0.28149772,  0.9752774 , -1.2279675 ],\n",
      "       [ 0.5103037 ,  1.3964705 , -1.9295602 ],\n",
      "       [ 0.16701953,  1.4117715 , -1.7033547 ],\n",
      "       [-0.28011242,  1.5122278 , -1.4297385 ],\n",
      "       [ 0.18059345,  0.91935045, -1.2334886 ],\n",
      "       [ 0.21664019,  1.4339545 , -1.7645341 ],\n",
      "       [-0.5092583 ,  0.91740334, -0.71140563],\n",
      "       [-0.2623199 ,  1.5553018 , -1.5406023 ],\n",
      "       [-0.08690717,  1.3876151 , -1.4340161 ],\n",
      "       [-0.0145038 ,  1.4231242 , -1.5911804 ],\n",
      "       [-0.10591923,  1.1551366 , -1.1917422 ],\n",
      "       [-0.07979214,  1.0395664 , -1.1253649 ],\n",
      "       [ 0.2744152 ,  1.1491977 , -1.437594  ],\n",
      "       [ 0.8292179 ,  1.4882199 , -2.198179  ],\n",
      "       [-0.2913322 ,  1.3947427 , -1.3792697 ],\n",
      "       [ 0.3956401 ,  0.9901181 , -1.4553858 ],\n",
      "       [ 0.2884451 ,  1.400357  , -1.6911944 ],\n",
      "       [-0.02450753,  1.2312404 , -1.339567  ],\n",
      "       [ 0.1139128 ,  1.0757351 , -1.2310122 ],\n",
      "       [-0.32804763,  1.1789365 , -1.1215265 ],\n",
      "       [ 0.45266753,  1.2300551 , -1.6605736 ],\n",
      "       [-0.3851244 ,  1.5290933 , -1.3452047 ],\n",
      "       [-0.698152  ,  1.3263904 , -1.0038223 ],\n",
      "       [ 0.1342213 ,  0.19447342, -0.3022232 ],\n",
      "       [-0.55289614,  1.3951437 , -1.1097214 ],\n",
      "       [-0.00627624,  1.100312  , -1.1372409 ],\n",
      "       [ 0.8369397 ,  1.22696   , -2.0334942 ],\n",
      "       [ 0.16116029,  0.9270674 , -1.2002646 ],\n",
      "       [-0.1923295 ,  1.8678826 , -1.9310172 ],\n",
      "       [-0.9162699 ,  0.8601438 , -0.3923651 ],\n",
      "       [-0.2300602 ,  1.4884512 , -1.4221879 ],\n",
      "       [ 0.18969323,  2.1220613 , -2.5101526 ],\n",
      "       [ 0.03956843,  1.0453832 , -1.3310353 ],\n",
      "       [-0.08731974,  1.4162202 , -1.6034731 ],\n",
      "       [-0.0303713 ,  1.3340807 , -1.41332   ],\n",
      "       [-0.16712391,  1.6915648 , -1.8022171 ],\n",
      "       [ 0.5225644 ,  0.7938704 , -1.3819515 ],\n",
      "       [ 0.3661263 ,  1.1087352 , -1.5114738 ],\n",
      "       [-0.12349579,  0.57171994, -0.60249484],\n",
      "       [-0.42719132,  1.9444431 , -1.8699692 ],\n",
      "       [-0.04344582,  1.1381638 , -1.2746338 ],\n",
      "       [-0.55413   ,  2.4800527 , -2.0882192 ],\n",
      "       [-0.52985215,  1.6270832 , -1.3430427 ],\n",
      "       [-0.6637445 ,  1.0614464 , -0.7357618 ],\n",
      "       [ 0.601712  ,  1.5564348 , -2.126823  ],\n",
      "       [ 0.24904872,  0.94183034, -1.2358359 ],\n",
      "       [ 0.05818173,  1.9825217 , -2.2178702 ],\n",
      "       [ 0.19825122,  1.1392455 , -1.3798522 ],\n",
      "       [ 0.28800052,  1.4848077 , -1.8160645 ],\n",
      "       [ 0.03268182,  0.20865226, -0.30224437],\n",
      "       [-0.5678455 ,  0.8248053 , -0.6140846 ],\n",
      "       [-0.30968195,  0.96493113, -0.8752311 ],\n",
      "       [-0.31052738,  1.2113943 , -1.158299  ],\n",
      "       [ 0.15258081,  1.4536376 , -1.6273199 ],\n",
      "       [-0.14052935,  1.2459508 , -1.2811261 ],\n",
      "       [ 0.56535214,  1.1255538 , -1.7584653 ],\n",
      "       [-0.0041865 ,  1.4815558 , -1.6030582 ],\n",
      "       [-0.13751042,  1.1873266 , -1.2945871 ],\n",
      "       [ 0.10213616,  1.2799007 , -1.4832458 ],\n",
      "       [ 0.0857484 ,  1.3960072 , -1.5333073 ],\n",
      "       [-0.52435917,  1.6858588 , -1.4676486 ],\n",
      "       [ 0.1823912 ,  1.0954797 , -1.3210483 ],\n",
      "       [ 0.15166356,  1.2958504 , -1.652911  ],\n",
      "       [ 0.23092829,  0.4715665 , -0.7592248 ],\n",
      "       [ 0.01516386,  1.2271938 , -1.3673284 ],\n",
      "       [ 0.8135673 ,  1.2399389 , -1.9396601 ],\n",
      "       [-0.76096517,  1.6592036 , -1.1376419 ],\n",
      "       [ 0.24220876,  1.5122817 , -1.7979424 ],\n",
      "       [ 0.80724275,  0.30000675, -1.0965941 ],\n",
      "       [-0.19443725,  2.037079  , -1.9317802 ],\n",
      "       [-0.5848241 ,  1.1560283 , -0.8382434 ],\n",
      "       [-0.46864662,  2.2152665 , -1.9370869 ],\n",
      "       [ 0.16500056,  1.193314  , -1.5532846 ],\n",
      "       [ 0.4024284 ,  1.1381708 , -1.5760332 ],\n",
      "       [ 0.02872634,  1.043168  , -1.298406  ],\n",
      "       [-0.0894185 ,  1.1391534 , -1.2307988 ],\n",
      "       [ 0.7722942 ,  1.1779622 , -1.8511535 ],\n",
      "       [ 0.19837317,  0.6703929 , -0.90742385],\n",
      "       [-0.11047768,  0.99778754, -1.1072862 ],\n",
      "       [ 0.4604748 ,  0.8891224 , -1.3970187 ],\n",
      "       [-0.22409302,  0.9810213 , -0.93783706],\n",
      "       [ 0.45044535,  1.2258849 , -1.5980767 ],\n",
      "       [ 0.18066907,  0.67993724, -0.8124825 ],\n",
      "       [ 0.24683677,  1.6985942 , -1.8487402 ],\n",
      "       [ 0.05955892,  1.6365904 , -1.7880925 ],\n",
      "       [ 1.2004727 ,  0.7863244 , -1.8443137 ],\n",
      "       [ 0.06816881,  1.0747164 , -1.2845837 ],\n",
      "       [-0.29529986,  1.1060383 , -1.0063839 ],\n",
      "       [ 0.30774897,  1.5742941 , -1.9053559 ],\n",
      "       [-0.0458593 ,  1.2008743 , -1.3332978 ],\n",
      "       [ 0.47560185,  1.2926863 , -1.8033512 ],\n",
      "       [ 0.24304534,  1.6700108 , -2.0341492 ],\n",
      "       [ 0.22795634,  1.7265248 , -2.104437  ],\n",
      "       [-0.65980333,  1.6527938 , -1.221646  ],\n",
      "       [ 0.00347787,  1.0444089 , -1.1855906 ],\n",
      "       [ 0.03079105,  1.3159106 , -1.5031874 ],\n",
      "       [ 0.84794796,  0.6706646 , -1.4746263 ],\n",
      "       [-0.44154418,  1.1902575 , -1.0537518 ],\n",
      "       [-0.40142053,  1.366806  , -1.2852508 ],\n",
      "       [-0.13900703,  1.126373  , -1.091924  ],\n",
      "       [ 0.18448769,  0.953118  , -1.135685  ],\n",
      "       [ 0.38659394,  1.1134979 , -1.5227957 ],\n",
      "       [ 0.10043278,  1.1414932 , -1.3696564 ],\n",
      "       [-0.01879255,  1.0809572 , -1.2290407 ],\n",
      "       [ 0.23759869,  1.2260331 , -1.6687317 ],\n",
      "       [-0.25179878,  1.0408183 , -0.9487632 ],\n",
      "       [-0.04015718,  1.605638  , -1.6174729 ],\n",
      "       [-0.50343037,  0.75823015, -0.57393587],\n",
      "       [ 0.2652794 ,  1.5993162 , -1.902987  ],\n",
      "       [-1.0450479 ,  0.74801624, -0.05443506],\n",
      "       [ 0.13264368,  1.6368157 , -1.8821998 ],\n",
      "       [ 0.4874239 ,  1.5257123 , -1.9109175 ],\n",
      "       [ 0.18204753,  1.1738778 , -1.5693184 ],\n",
      "       [ 0.14982753,  0.90521437, -1.209512  ],\n",
      "       [ 0.35734314,  1.0797389 , -1.5887359 ],\n",
      "       [ 0.3926075 ,  0.7947764 , -1.2607208 ],\n",
      "       [-0.19357471,  1.4195305 , -1.3198754 ],\n",
      "       [-0.5335504 ,  1.6222177 , -1.4257118 ],\n",
      "       [-0.2675857 ,  1.2872386 , -1.1718179 ],\n",
      "       [ 0.11837011,  1.976252  , -2.1343534 ],\n",
      "       [ 0.12484717,  0.9779485 , -1.2243848 ],\n",
      "       [-0.28354853,  1.1559873 , -1.2273875 ],\n",
      "       [-0.27563453,  1.6111344 , -1.5706955 ],\n",
      "       [-0.73727554,  1.1312864 , -0.7514517 ],\n",
      "       [-0.31111255,  1.649942  , -1.6358354 ],\n",
      "       [ 0.40962088,  1.5843631 , -1.8971884 ],\n",
      "       [-0.27860805,  1.4898596 , -1.4856544 ],\n",
      "       [ 0.37770253,  1.4449785 , -1.8484893 ],\n",
      "       [ 0.34037387,  1.3323921 , -1.8161091 ],\n",
      "       [-0.11088821,  1.3508204 , -1.4351712 ],\n",
      "       [-0.01400774,  1.2374322 , -1.4109913 ],\n",
      "       [-0.047832  ,  1.2733861 , -1.4242733 ],\n",
      "       [ 0.06111455,  1.2328032 , -1.516854  ],\n",
      "       [ 0.413331  ,  0.48390737, -0.9665837 ],\n",
      "       [-0.29182407,  1.3904223 , -1.2732787 ],\n",
      "       [-0.27392557,  1.4633034 , -1.4632334 ],\n",
      "       [ 0.3283214 ,  1.4954484 , -1.8435616 ],\n",
      "       [-0.2688228 ,  1.6548891 , -1.7007499 ],\n",
      "       [ 1.0352937 ,  0.9011993 , -1.8631634 ],\n",
      "       [ 0.25394356,  1.6718909 , -2.017081  ],\n",
      "       [ 0.27987036,  1.3325583 , -1.765182  ],\n",
      "       [ 0.06302334,  1.2187324 , -1.4655018 ],\n",
      "       [-0.00409272,  1.8448312 , -1.847544  ],\n",
      "       [-0.36793998,  1.3989947 , -1.3338383 ],\n",
      "       [ 0.59093046,  1.1492274 , -1.7344941 ],\n",
      "       [-0.14718747,  0.78604275, -0.86439246],\n",
      "       [-0.20471361,  1.8033664 , -1.7679113 ],\n",
      "       [-0.6135823 ,  1.0559738 , -0.6999522 ],\n",
      "       [-0.15923996,  0.9912386 , -1.0923223 ],\n",
      "       [ 0.13953231,  1.1416146 , -1.3889029 ],\n",
      "       [ 0.3567164 ,  0.7639642 , -1.1625429 ],\n",
      "       [ 0.15156174,  1.4814496 , -1.6608374 ],\n",
      "       [ 0.19821699,  0.9322916 , -1.2186689 ]], dtype=float32), array([[[-6.59084588e-04, -1.69312377e-02, -2.65463293e-02, ...,\n",
      "          1.67239144e-01,  5.05507812e-02, -7.76135325e-02],\n",
      "        [ 6.23518750e-02, -8.94604698e-02,  2.77102664e-02, ...,\n",
      "          8.86492152e-03,  5.96144609e-02, -5.73330466e-03],\n",
      "        [ 8.68941769e-02, -7.97345564e-02,  3.81751359e-02, ...,\n",
      "          1.33160844e-01,  8.65461752e-02,  3.27664129e-02],\n",
      "        ...,\n",
      "        [ 1.87114552e-02,  9.48094204e-02, -4.33359928e-02, ...,\n",
      "          2.49064267e-01,  4.19087894e-02,  4.88436781e-02],\n",
      "        [ 1.87114552e-02,  9.48094204e-02, -4.33359928e-02, ...,\n",
      "          2.49064267e-01,  4.19087894e-02,  4.88436781e-02],\n",
      "        [ 1.87114552e-02,  9.48094204e-02, -4.33359928e-02, ...,\n",
      "          2.49064267e-01,  4.19087894e-02,  4.88436781e-02]],\n",
      "\n",
      "       [[-2.35255659e-01,  8.85727331e-02, -3.09383571e-02, ...,\n",
      "          6.25865012e-02,  6.28592074e-02, -3.86129902e-03],\n",
      "        [-1.39844030e-01,  1.12255872e-03,  5.48138544e-02, ...,\n",
      "          3.50464135e-02,  3.05235498e-02, -9.96490046e-02],\n",
      "        [-1.24352984e-01, -1.04325898e-01,  6.32238463e-02, ...,\n",
      "          1.06244013e-01,  1.32976577e-01,  4.23919521e-02],\n",
      "        ...,\n",
      "        [ 7.20313564e-02,  2.65867859e-01,  7.50665292e-02, ...,\n",
      "          1.90531462e-01,  1.44246340e-01,  2.91555710e-02],\n",
      "        [ 7.20313564e-02,  2.65867859e-01,  7.50665292e-02, ...,\n",
      "          1.90531462e-01,  1.44246340e-01,  2.91555710e-02],\n",
      "        [ 7.20313564e-02,  2.65867859e-01,  7.50665292e-02, ...,\n",
      "          1.90531462e-01,  1.44246340e-01,  2.91555710e-02]],\n",
      "\n",
      "       [[-1.78309262e-01, -4.03238200e-02, -1.35262102e-01, ...,\n",
      "          9.10840631e-02,  9.99469310e-03, -1.52867427e-02],\n",
      "        [-1.20210707e-01,  5.19803651e-02, -1.02185406e-01, ...,\n",
      "          1.69499032e-02, -6.65487275e-02, -2.93239485e-02],\n",
      "        [-5.86256664e-03,  5.73487729e-02, -9.78123117e-03, ...,\n",
      "          8.34675729e-02,  3.94102149e-02, -1.62380990e-02],\n",
      "        ...,\n",
      "        [ 2.13043019e-01, -6.55581504e-02,  7.51622617e-02, ...,\n",
      "          2.14664593e-01,  9.46058035e-02,  2.79603273e-01],\n",
      "        [ 2.13043019e-01, -6.55581504e-02,  7.51622617e-02, ...,\n",
      "          2.14664593e-01,  9.46058035e-02,  2.79603273e-01],\n",
      "        [ 2.13043019e-01, -6.55581504e-02,  7.51622617e-02, ...,\n",
      "          2.14664593e-01,  9.46058035e-02,  2.79603273e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-2.12350637e-01,  1.38444409e-01, -3.68318371e-02, ...,\n",
      "         -1.40441701e-01,  2.05237135e-01, -4.14062850e-02],\n",
      "        [-1.19150497e-01,  1.35465767e-02, -1.27161339e-01, ...,\n",
      "         -2.21390161e-04,  6.80157244e-02, -3.93977985e-02],\n",
      "        [ 8.53411946e-03, -9.98552144e-02, -2.05315143e-01, ...,\n",
      "         -4.14797105e-03, -5.69329932e-02,  2.17441633e-01],\n",
      "        ...,\n",
      "        [-4.32569310e-02,  1.90764263e-01,  8.21011961e-02, ...,\n",
      "          1.87408537e-01,  1.93055212e-01,  1.15320086e-01],\n",
      "        [-4.32569310e-02,  1.90764263e-01,  8.21011961e-02, ...,\n",
      "          1.87408537e-01,  1.93055212e-01,  1.15320086e-01],\n",
      "        [-4.32569310e-02,  1.90764263e-01,  8.21011961e-02, ...,\n",
      "          1.87408537e-01,  1.93055212e-01,  1.15320086e-01]],\n",
      "\n",
      "       [[-1.31014541e-01, -1.64668128e-01,  2.61011153e-01, ...,\n",
      "          4.63832244e-02, -9.99368429e-02, -4.76997979e-02],\n",
      "        [-2.39733756e-01, -1.06365457e-01,  1.98629484e-01, ...,\n",
      "          2.07349062e-02, -2.11903587e-01, -6.03350401e-02],\n",
      "        [-5.50089739e-02, -6.36144653e-02,  3.41979474e-01, ...,\n",
      "          6.38455078e-02, -2.02895135e-01, -2.48324171e-01],\n",
      "        ...,\n",
      "        [ 8.90373141e-02, -7.48165101e-02,  1.63516235e-02, ...,\n",
      "          6.55669868e-02,  1.11625701e-01,  1.01078823e-01],\n",
      "        [ 8.90373141e-02, -7.48165101e-02,  1.63516235e-02, ...,\n",
      "          6.55669868e-02,  1.11625701e-01,  1.01078823e-01],\n",
      "        [ 8.90373141e-02, -7.48165101e-02,  1.63516235e-02, ...,\n",
      "          6.55669868e-02,  1.11625701e-01,  1.01078823e-01]],\n",
      "\n",
      "       [[-2.45819718e-01,  1.03820913e-01,  4.03131209e-02, ...,\n",
      "         -5.91603480e-02,  1.18795654e-03, -1.48833945e-01],\n",
      "        [-3.52893621e-02,  1.39354363e-01, -1.85188562e-01, ...,\n",
      "         -8.47467035e-02, -9.81570184e-02, -4.69236216e-03],\n",
      "        [-9.30338749e-04,  1.72926620e-01, -9.24805328e-02, ...,\n",
      "         -1.40935317e-01, -7.71764815e-02,  8.69612843e-02],\n",
      "        ...,\n",
      "        [-7.15893432e-02,  2.20483601e-01, -1.52407226e-03, ...,\n",
      "          1.93156585e-01,  1.66690215e-01,  2.38966361e-01],\n",
      "        [-7.15893432e-02,  2.20483601e-01, -1.52407226e-03, ...,\n",
      "          1.93156585e-01,  1.66690215e-01,  2.38966361e-01],\n",
      "        [-7.15893432e-02,  2.20483601e-01, -1.52407226e-03, ...,\n",
      "          1.93156585e-01,  1.66690215e-01,  2.38966361e-01]]],\n",
      "      dtype=float32)), label_ids=array([1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 0,\n",
      "       1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 1, 2, 0,\n",
      "       1, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
      "       2, 1, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2,\n",
      "       1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0]), metrics={'test_loss': 1.0706764459609985, 'test_accuracy': 0.6282051282051282, 'test_precision': 0.44458627253387084, 'test_recall': 0.6282051282051282, 'test_f1': 0.5015150271985169, 'test_runtime': 7.9141, 'test_samples_per_second': 29.567, 'test_steps_per_second': 3.791})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd75bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n",
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "766c4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.2.1_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3e1cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afd884e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           32\n",
       "Fitness                  15\n",
       "Bone health              12\n",
       "Cancer                   11\n",
       "Throat                    9\n",
       "Hair                      9\n",
       "Diabetes                  9\n",
       "Skin                      8\n",
       "Neurological health       8\n",
       "Ear                       6\n",
       "COVID                     6\n",
       "Eye                       6\n",
       "Cardiovascular Health     6\n",
       "Mental Health             3\n",
       "Muscles                   3\n",
       "Blood                     3\n",
       "Men's health              1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6477f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           19\n",
       "Skin                     16\n",
       "Bone health               9\n",
       "Blood                     6\n",
       "Women' s Health           6\n",
       "Cardiovascular Health     6\n",
       "Men's health              5\n",
       "Vascular                  3\n",
       "Muscles                   3\n",
       "Eye                       3\n",
       "Dental Health             3\n",
       "Hair                      3\n",
       "Diabetes                  3\n",
       "Cancer                    1\n",
       "Neurological health       1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
