{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cfbf7584a6bb7c13\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 182.53it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv',data_files='dataset_sentenceattribution_nerfeatures_split.csv',delimiter=',',column_names=[\"claim\",\"premise\",\"label\",\"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\",\"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\",\"url\", \"entities\",\"entity_map\",\"gem_exp\",\"gem_label\",\"gpt_label\",\"gpt_exp\",\"gold_exp\",\"entity_map_ev\",\"entity_ev\",\"synonym\",\"voice\",\"split\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133f9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-aa1fd4d2889f655e.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9cd628f4430f2ace.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-df968f39da4ad12a.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01252e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\", \"category\",\"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-93047803ca0da6af.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-b4529f7a12de92ec.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-8b54a2ea45a9eb99.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 2, 'entailment': 0, 'neutral': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e15010ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4b7eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['prese_bf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "        claim = item['claim'].lower() \n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        additional_features_evidence = [\n",
    "            \"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "    \n",
    "        for feature in additional_features_evidence:\n",
    "            if feature in item:\n",
    "                evidences += \"[SEP]\" + str(item[feature])\n",
    "        additional_features = [\n",
    "            \"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "    \n",
    "        for feature_ev in additional_features:\n",
    "            if feature_ev in item:\n",
    "                claim += \"[SEP]\" + str(item[feature_ev])\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences,claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "         # Construct the output item\n",
    "        output_item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),  # Remove batch dimension\n",
    "            'claim': claim,  # Include augmented claim text\n",
    "            'evidences': evidences  # Include original evidence text\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            output_item['label'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return output_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f5cf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 1029,   389, 10432,    63,    12, 27155,    11,     3, 29809,  3318,\n",
       "          3920,    49, 27684,     6,    11, 13587,    10,  8618, 17481,    19,\n",
       "         12165,     6,  8618, 11584,     6, 21632,     6,    11,    86,   994,\n",
       "          3801,   757,     5,  6306,   134,  8569,   908,   632,  6306,   134,\n",
       "          8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,   134,\n",
       "          8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,   134,\n",
       "          8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,   134,\n",
       "          8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,   134,\n",
       "          8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,   134,\n",
       "          8569,   908,   632,     1, 23486,    19,     3,     9,  1346,    11,\n",
       "          1406,   924,  3979,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'claim': 'ultrasound is a safe and painless procedure.',\n",
       " 'evidences': 'From Anatomy to Functional and Molecular Biomarker Imaging, and Therapy: Ultrasound is Safe, Ultrafast, Portable, and Inexpensive.[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0',\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 41:43, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.881626</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.657528</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.639038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>1.182051</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.678490</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.662869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>1.655198</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.668070</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.634968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>1.871891</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.633194</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.629234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>2.277245</td>\n",
       "      <td>0.595699</td>\n",
       "      <td>0.620576</td>\n",
       "      <td>0.595699</td>\n",
       "      <td>0.602399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>2.709288</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.632951</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.624492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>2.681995</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.649807</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.630729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>2.688699</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.654963</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.630799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>3.060491</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.639423</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.621905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>3.017305</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>0.645163</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>0.626204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.995233</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>0.653123</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>0.634970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.039398</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.642563</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.629716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>3.199396</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.646491</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.148365</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.652219</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.640944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.186401</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.651535</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.640880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.5_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.1.5_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.5_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.1.5_flant5/checkpoint-203 (score: 0.6645161290322581).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 10:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.1.5_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.1.5_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.1.5_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.1.5_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.1.5_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.1.5_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.1.5_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.1.5_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.1.5_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.1.5_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.1.5_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.1.5_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.1.5_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.1.5_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.1.5_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.1.5_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.1.5_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.1.5_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[ 0.647171  ,  1.0109184 , -1.6820264 ],\n",
      "       [ 2.6099894 , -0.5844236 , -1.6325759 ],\n",
      "       [ 2.107962  , -0.4739062 , -1.4881946 ],\n",
      "       [ 0.6692691 , -0.8071631 ,  0.1525284 ],\n",
      "       [ 1.4609716 , -0.09708526, -1.2400793 ],\n",
      "       [ 3.5556934 , -0.90616006, -2.1739197 ],\n",
      "       [ 2.2930105 , -0.854317  , -1.1375899 ],\n",
      "       [ 3.297224  , -0.8847865 , -1.9693319 ],\n",
      "       [ 2.7767532 , -0.39171618, -2.222485  ],\n",
      "       [ 1.7406281 , -0.61881006, -0.90845716],\n",
      "       [ 2.4890635 , -1.0390552 , -1.0153664 ],\n",
      "       [ 3.057022  , -0.9153257 , -1.6078682 ],\n",
      "       [-1.0430439 ,  0.09517132,  0.615192  ],\n",
      "       [ 3.1596396 , -0.87406844, -1.8739742 ],\n",
      "       [ 1.1029562 , -0.312726  , -0.6342068 ],\n",
      "       [-1.0898511 , -0.05618058,  0.8685657 ],\n",
      "       [ 2.9972382 , -0.57073087, -1.9993256 ],\n",
      "       [ 0.9291526 , -0.5849353 , -0.2735659 ],\n",
      "       [ 3.094531  , -0.79302144, -1.8190238 ],\n",
      "       [ 2.1235678 , -0.13407806, -1.8470095 ],\n",
      "       [ 0.31096408,  0.06349628, -0.4681942 ],\n",
      "       [-0.38520837,  0.07653779,  0.16071957],\n",
      "       [ 1.4124881 , -0.09450521, -1.2593237 ],\n",
      "       [ 0.27393666,  0.0473349 , -0.5153719 ],\n",
      "       [ 0.66848445, -0.46063375, -0.19373868],\n",
      "       [-1.2440991 ,  0.05129617,  0.89625704],\n",
      "       [ 2.8624592 , -0.52809143, -1.9369314 ],\n",
      "       [ 0.8806243 , -0.27907076, -0.48217598],\n",
      "       [ 1.8474896 , -0.7947284 , -0.9116128 ],\n",
      "       [ 1.4658357 , -0.7779758 , -0.60395646],\n",
      "       [ 1.578053  , -0.16999045, -1.2626712 ],\n",
      "       [ 2.69635   , -0.44015744, -1.9737642 ],\n",
      "       [ 2.6171546 , -0.6805221 , -1.5462408 ],\n",
      "       [ 1.5286392 , -0.44807598, -0.8837463 ],\n",
      "       [-0.7201995 ,  0.3291508 ,  0.11997712],\n",
      "       [ 1.8312026 , -0.01119283, -1.6378924 ],\n",
      "       [ 0.9885516 , -0.0637833 , -0.9108202 ],\n",
      "       [ 2.6779566 , -0.4138666 , -2.05051   ],\n",
      "       [-0.73033917, -0.44452366,  0.9683937 ],\n",
      "       [ 1.8651948 , -0.08490589, -1.5748528 ],\n",
      "       [ 0.77086633,  0.51480955, -1.4350412 ],\n",
      "       [ 2.6434715 , -0.6276817 , -1.7496372 ],\n",
      "       [ 1.1336054 , -0.22199142, -0.8777939 ],\n",
      "       [-1.7711781 , -0.2340122 ,  1.6894497 ],\n",
      "       [-0.5767471 ,  0.15709059,  0.211442  ],\n",
      "       [ 2.0331075 , -0.33814666, -1.3863252 ],\n",
      "       [ 0.6573975 ,  0.34889284, -1.2271284 ],\n",
      "       [ 2.6189086 , -1.0423696 , -1.2568334 ],\n",
      "       [ 2.840388  , -0.6479367 , -1.9806541 ],\n",
      "       [ 0.56205523, -0.68894017,  0.15806644],\n",
      "       [ 1.4022382 , -0.3511128 , -0.9739511 ],\n",
      "       [ 1.3237486 , -0.19300586, -1.0091394 ],\n",
      "       [ 1.6481303 ,  0.33564755, -1.9752475 ],\n",
      "       [-1.5708109 , -0.38530973,  1.5987207 ],\n",
      "       [ 0.51661503,  0.06430819, -0.64815164],\n",
      "       [ 2.9801834 , -0.5338279 , -2.047079  ],\n",
      "       [-1.4539468 , -0.22872356,  1.3699802 ],\n",
      "       [ 2.3548117 ,  0.02993503, -2.0578928 ],\n",
      "       [ 1.6115485 , -0.04918186, -1.4785074 ],\n",
      "       [ 2.935226  , -0.7712539 , -1.8646054 ],\n",
      "       [ 1.34565   , -0.58047134, -0.7109835 ],\n",
      "       [-0.5520781 , -0.59793144,  0.94523406],\n",
      "       [ 2.4950178 , -0.7283532 , -1.4059091 ],\n",
      "       [ 0.90653205,  0.76661026, -1.8384976 ],\n",
      "       [ 0.8763571 ,  0.06576277, -0.93599504],\n",
      "       [ 3.4132607 , -0.92939717, -2.0616488 ],\n",
      "       [ 1.3778311 ,  0.25140843, -1.6444131 ],\n",
      "       [ 3.2636313 , -0.84491605, -1.9766482 ],\n",
      "       [ 1.7265654 , -0.64341825, -0.8329731 ],\n",
      "       [ 0.2974105 , -0.68681884,  0.42235047],\n",
      "       [ 2.312719  , -0.5682085 , -1.4717714 ],\n",
      "       [ 1.1366093 ,  0.346244  , -1.4702984 ],\n",
      "       [ 1.2855006 , -0.06223796, -1.1493152 ],\n",
      "       [-0.7204538 ,  0.15898387,  0.25221974],\n",
      "       [ 1.1633555 , -0.2608933 , -0.715526  ],\n",
      "       [ 1.2415156 ,  0.15763897, -1.3930717 ],\n",
      "       [ 0.63772464, -0.24668491, -0.34463638],\n",
      "       [ 1.1528226 , -0.3602813 , -0.7354358 ],\n",
      "       [ 3.5578926 , -1.2889314 , -1.7574009 ],\n",
      "       [ 2.6873949 , -0.6672133 , -1.6357484 ],\n",
      "       [ 2.887078  , -1.1736182 , -1.350254  ],\n",
      "       [ 2.0230575 , -0.6934939 , -1.1984317 ],\n",
      "       [ 2.3430386 , -0.5842459 , -1.4011784 ],\n",
      "       [ 2.2377944 , -0.84371   , -1.0572782 ],\n",
      "       [ 2.856771  , -0.537396  , -2.1603062 ],\n",
      "       [ 1.0090119 ,  0.22862285, -1.1624615 ],\n",
      "       [ 2.661824  , -0.69563603, -1.7657626 ],\n",
      "       [ 0.22279455,  0.8105249 , -1.4067372 ],\n",
      "       [ 1.907412  ,  0.09936258, -1.9163823 ],\n",
      "       [ 2.310152  , -0.67981017, -1.3193853 ],\n",
      "       [ 2.165549  , -0.5522439 , -1.448986  ],\n",
      "       [ 2.853288  , -0.9873688 , -1.5876174 ],\n",
      "       [ 0.4371552 , -0.6215836 ,  0.15049873],\n",
      "       [ 2.381102  , -0.75875455, -1.3489249 ],\n",
      "       [ 0.9268122 ,  0.09009186, -1.0066955 ],\n",
      "       [ 1.407666  , -0.28723624, -1.0687941 ],\n",
      "       [ 0.40593955,  0.0506884 , -0.58505785],\n",
      "       [ 1.3180586 , -0.08615728, -1.0651997 ],\n",
      "       [ 3.6596754 , -1.2444137 , -1.8908733 ],\n",
      "       [ 2.902983  , -0.96468145, -1.5398766 ],\n",
      "       [-1.1117743 , -0.8807388 ,  1.78139   ],\n",
      "       [ 2.1921647 , -0.46545267, -1.4653083 ],\n",
      "       [ 3.238284  , -1.0721208 , -1.7510244 ],\n",
      "       [ 1.5473859 ,  0.175222  , -1.6461898 ],\n",
      "       [ 0.940219  , -0.01756765, -0.9554545 ],\n",
      "       [ 2.0366538 , -0.72083724, -1.288066  ],\n",
      "       [ 0.65923405,  1.0250801 , -2.0077927 ],\n",
      "       [-0.3810917 ,  0.20709579, -0.03388896],\n",
      "       [ 2.6368694 , -0.96350336, -1.4890022 ],\n",
      "       [ 2.2164419 , -0.76353747, -1.2282907 ],\n",
      "       [-1.0980359 , -0.1642797 ,  0.993837  ],\n",
      "       [ 3.2991138 , -1.0055128 , -1.8881772 ],\n",
      "       [ 2.8765407 , -0.8069025 , -1.658274  ],\n",
      "       [-0.18179947, -0.11801787,  0.1271093 ],\n",
      "       [ 3.0587945 , -0.94520134, -1.7437097 ],\n",
      "       [ 2.4633086 , -0.15653795, -2.124607  ],\n",
      "       [ 3.2215266 , -1.091093  , -1.6437452 ],\n",
      "       [ 2.9086797 , -0.7073042 , -1.8782536 ],\n",
      "       [ 1.986001  , -0.6951946 , -0.98537743],\n",
      "       [ 2.945325  , -0.46402648, -2.113986  ],\n",
      "       [ 2.3942454 , -0.5015383 , -1.6325262 ],\n",
      "       [ 2.0718157 , -0.77895224, -1.1162248 ],\n",
      "       [ 2.263007  , -0.54221994, -1.5295402 ],\n",
      "       [ 1.9415807 ,  0.4479599 , -2.3260067 ],\n",
      "       [ 0.74972254, -0.3732978 , -0.40097767],\n",
      "       [ 1.8062637 , -0.49187225, -1.230594  ],\n",
      "       [ 0.5805423 ,  0.06733033, -0.60727406],\n",
      "       [ 3.0873446 , -0.8810799 , -1.8631574 ],\n",
      "       [ 0.9910827 , -0.6792098 , -0.28210092],\n",
      "       [ 1.5610279 , -0.49884713, -0.9086248 ],\n",
      "       [ 1.4340526 , -0.35671166, -0.92964077],\n",
      "       [-0.30722883, -0.36136964,  0.4293786 ],\n",
      "       [ 2.6239457 , -0.2952097 , -2.0968988 ],\n",
      "       [-0.20790485,  0.37135205, -0.4300664 ],\n",
      "       [ 2.0376866 , -0.44286606, -1.3503842 ],\n",
      "       [ 1.0221488 ,  0.23790045, -1.25195   ],\n",
      "       [ 3.0250852 , -0.74413466, -1.8204633 ],\n",
      "       [-1.1129812 ,  0.00936219,  0.85807645],\n",
      "       [ 0.27899265,  0.3453437 , -0.72572166],\n",
      "       [ 3.3862357 , -1.0401487 , -1.926065  ],\n",
      "       [ 1.2110775 , -0.17304009, -0.9000356 ],\n",
      "       [ 1.6311746 , -0.3878418 , -1.2228605 ],\n",
      "       [ 1.8441094 , -0.14490213, -1.5571157 ],\n",
      "       [ 2.972125  , -0.71697676, -1.9031473 ],\n",
      "       [ 2.6533744 , -0.78759944, -1.5389353 ],\n",
      "       [ 2.6417956 , -0.6952188 , -1.5626426 ],\n",
      "       [ 1.106137  ,  0.7646524 , -2.026586  ],\n",
      "       [-0.4171323 , -0.4374336 ,  0.6620242 ],\n",
      "       [ 2.321412  , -0.57258105, -1.5147858 ],\n",
      "       [ 0.8846942 , -0.41502506, -0.42626345],\n",
      "       [-0.39001143,  0.4207455 , -0.24824643],\n",
      "       [ 2.1934178 , -0.58910865, -1.4668934 ],\n",
      "       [ 3.2582252 , -1.0543226 , -1.810224  ],\n",
      "       [ 2.6895423 , -0.55443364, -1.7504218 ],\n",
      "       [ 3.0713048 , -0.6591127 , -2.1207285 ],\n",
      "       [ 3.0262356 , -1.1573199 , -1.5490661 ],\n",
      "       [ 1.7795521 , -1.138694  , -0.4050426 ],\n",
      "       [ 1.665408  ,  0.13512234, -1.6999918 ],\n",
      "       [ 2.2715018 , -0.4497215 , -1.6261332 ],\n",
      "       [-1.6932398 ,  0.03688673,  1.1708977 ],\n",
      "       [-0.66169024, -0.04754446,  0.54133105],\n",
      "       [-2.0986068 , -0.66437644,  2.5218637 ],\n",
      "       [ 2.8999317 , -0.9782935 , -1.468038  ],\n",
      "       [-1.8303448 , -0.36237022,  1.8785362 ],\n",
      "       [ 0.75442773, -0.19584775, -0.60289913],\n",
      "       [ 2.3152409 , -0.47593477, -1.6133225 ],\n",
      "       [ 1.4075235 , -0.7333403 , -0.49918485],\n",
      "       [-1.1729861 ,  0.35917416,  0.4669626 ],\n",
      "       [-2.1690676 , -0.45162585,  2.246532  ],\n",
      "       [-0.4672752 , -0.52738047,  0.83963245],\n",
      "       [-1.2296476 ,  0.03801812,  0.94374436],\n",
      "       [ 2.1004279 , -0.45350936, -1.426595  ],\n",
      "       [ 0.08091492,  0.32809648, -0.57345855],\n",
      "       [ 3.478742  , -0.9314363 , -2.0618293 ],\n",
      "       [ 2.405666  , -0.35706407, -1.7668425 ],\n",
      "       [ 0.28701374,  0.4317468 , -0.93051004],\n",
      "       [ 2.7244391 , -0.9308302 , -1.4401947 ],\n",
      "       [ 0.31008968, -0.5197868 ,  0.2102799 ],\n",
      "       [ 2.8008862 , -0.58506936, -2.0170815 ],\n",
      "       [ 0.7392323 , -0.29845047, -0.42065465],\n",
      "       [-0.20956641, -0.07846683,  0.10016471],\n",
      "       [ 0.266606  , -0.4176741 ,  0.1737168 ],\n",
      "       [ 0.522234  , -0.42207286, -0.17547122],\n",
      "       [ 1.0360956 ,  1.199187  , -2.2292113 ],\n",
      "       [ 0.92363465,  0.28546724, -1.2596419 ],\n",
      "       [ 2.7452667 , -0.7712083 , -1.5603489 ],\n",
      "       [ 0.9160978 , -0.00737207, -0.9425213 ],\n",
      "       [ 2.811529  , -0.28006673, -2.331737  ],\n",
      "       [ 0.68634576,  0.40809616, -1.2269694 ],\n",
      "       [ 2.3498015 , -0.70912653, -1.2678149 ],\n",
      "       [-0.34817356,  0.12267532,  0.06396796],\n",
      "       [ 1.5032803 , -0.14393122, -1.298529  ],\n",
      "       [ 2.0572214 , -0.62750953, -1.1692865 ],\n",
      "       [ 2.53201   , -0.700863  , -1.508616  ],\n",
      "       [-0.44583037, -0.78417474,  1.0366117 ],\n",
      "       [ 3.4749594 , -0.9268768 , -2.007451  ],\n",
      "       [ 2.1077085 , -0.24323677, -1.7436723 ],\n",
      "       [ 3.2824662 , -0.925147  , -1.9719319 ],\n",
      "       [ 2.0797427 , -0.6299859 , -1.2702839 ],\n",
      "       [ 1.7647519 ,  0.03452279, -1.6398609 ],\n",
      "       [ 2.7462876 , -0.9555611 , -1.3563992 ],\n",
      "       [ 3.1843534 , -0.7140165 , -2.1143084 ],\n",
      "       [ 1.1613415 ,  0.86756253, -2.1143386 ],\n",
      "       [ 3.3435535 , -0.77382326, -2.1645935 ],\n",
      "       [-0.65864676,  0.20204392,  0.18677123],\n",
      "       [ 0.3350732 ,  0.06591566, -0.5506476 ],\n",
      "       [ 2.5159483 , -0.07044897, -2.281485  ],\n",
      "       [ 2.158571  , -0.62695104, -1.2051982 ],\n",
      "       [ 3.342496  , -0.61712766, -2.36649   ],\n",
      "       [ 2.45111   , -0.39219293, -1.694589  ],\n",
      "       [ 0.77697474, -0.14848316, -0.607601  ],\n",
      "       [ 1.1817751 , -0.4616901 , -0.6386553 ],\n",
      "       [ 2.96295   , -0.97958845, -1.5192212 ],\n",
      "       [ 1.5652022 ,  0.13004181, -1.7296258 ],\n",
      "       [ 3.091872  , -0.8050116 , -2.0472639 ],\n",
      "       [ 0.21479765,  0.04682277, -0.34525782],\n",
      "       [ 0.4944404 ,  0.12380966, -0.64743346],\n",
      "       [-0.335554  , -0.6403796 ,  0.7638848 ],\n",
      "       [ 3.4740355 , -0.8958749 , -2.0537612 ],\n",
      "       [-0.09066156,  0.16190502, -0.30825943],\n",
      "       [ 2.9750018 , -0.8155832 , -1.8186024 ],\n",
      "       [ 2.4432368 , -0.58099735, -1.5414847 ],\n",
      "       [ 1.320302  , -0.2795981 , -0.8658168 ],\n",
      "       [ 3.6478434 , -1.2392977 , -1.868492  ],\n",
      "       [ 2.864218  , -0.4531464 , -2.1551917 ],\n",
      "       [ 3.6923969 , -0.87833077, -2.2843735 ],\n",
      "       [ 3.2088075 , -1.0544693 , -1.7751062 ],\n",
      "       [-0.46049854,  0.08731595,  0.1660958 ],\n",
      "       [-0.51996225, -0.26956114,  0.6480253 ],\n",
      "       [ 2.9562345 , -0.7366304 , -1.8929135 ],\n",
      "       [ 1.8064873 , -0.52720726, -1.0261977 ],\n",
      "       [ 2.9809926 , -0.8236346 , -1.8763069 ],\n",
      "       [ 2.6017854 , -0.29776925, -2.108241  ],\n",
      "       [ 1.329474  , -0.6686792 , -0.5843407 ]], dtype=float32), array([[[-1.10762134e-01, -4.81568165e-02, -3.42581235e-02, ...,\n",
      "          2.99746636e-02, -4.54042368e-02,  4.31202305e-03],\n",
      "        [-8.36533085e-02,  5.29631898e-02, -1.63578093e-01, ...,\n",
      "          2.33240947e-02, -4.17175926e-02, -4.97687384e-02],\n",
      "        [ 6.62619472e-02, -7.71669894e-02, -4.52521369e-02, ...,\n",
      "         -7.25877360e-02, -8.24937597e-02, -1.79543737e-02],\n",
      "        ...,\n",
      "        [ 6.06790073e-02,  9.96039286e-02, -2.32766718e-01, ...,\n",
      "          2.97679216e-01, -1.04435086e-01,  1.59173772e-01],\n",
      "        [ 6.06790073e-02,  9.96039286e-02, -2.32766718e-01, ...,\n",
      "          2.97679216e-01, -1.04435086e-01,  1.59173772e-01],\n",
      "        [ 6.06790073e-02,  9.96039286e-02, -2.32766718e-01, ...,\n",
      "          2.97679216e-01, -1.04435086e-01,  1.59173772e-01]],\n",
      "\n",
      "       [[-1.04646504e-01,  2.60820743e-02, -1.32004693e-01, ...,\n",
      "         -3.75735871e-02, -6.41814619e-02, -7.08466768e-02],\n",
      "        [ 5.78385852e-02, -2.39339069e-01, -5.75171188e-02, ...,\n",
      "         -8.14447924e-02, -9.39051881e-02,  3.26044150e-02],\n",
      "        [-1.82110462e-02, -1.37121290e-01, -1.61008701e-01, ...,\n",
      "          1.52059838e-01,  7.22922683e-02, -4.34386618e-02],\n",
      "        ...,\n",
      "        [ 5.18948212e-02,  1.27566278e-01, -7.60173798e-02, ...,\n",
      "          2.37618953e-01,  1.00181568e-02,  5.19084707e-02],\n",
      "        [ 5.18948212e-02,  1.27566278e-01, -7.60173798e-02, ...,\n",
      "          2.37618953e-01,  1.00181568e-02,  5.19084707e-02],\n",
      "        [ 5.18948212e-02,  1.27566278e-01, -7.60173798e-02, ...,\n",
      "          2.37618953e-01,  1.00181568e-02,  5.19084707e-02]],\n",
      "\n",
      "       [[-2.00525075e-01, -8.50663148e-03, -1.74923271e-01, ...,\n",
      "          1.33227706e-01, -7.91824330e-03, -9.96335745e-02],\n",
      "        [-7.10988641e-02,  1.57713629e-02, -1.09484330e-01, ...,\n",
      "          1.74585208e-01,  7.77641758e-02,  2.64429692e-02],\n",
      "        [-2.27750525e-01, -7.72158727e-02, -1.28900275e-01, ...,\n",
      "          8.23549926e-02, -8.60559866e-02, -6.09890930e-02],\n",
      "        ...,\n",
      "        [-2.46154633e-03,  9.86716002e-02, -2.16133833e-01, ...,\n",
      "          3.15695107e-01,  3.02627143e-02,  1.12449281e-01],\n",
      "        [-2.46154633e-03,  9.86716002e-02, -2.16133833e-01, ...,\n",
      "          3.15695107e-01,  3.02627143e-02,  1.12449281e-01],\n",
      "        [-2.46154633e-03,  9.86716002e-02, -2.16133833e-01, ...,\n",
      "          3.15695107e-01,  3.02627143e-02,  1.12449281e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-2.90369644e-04, -1.74070522e-02, -1.10290706e-01, ...,\n",
      "         -4.30128984e-02, -2.61726733e-02,  1.03710918e-03],\n",
      "        [ 5.84613159e-02, -1.18560381e-01,  2.15321425e-02, ...,\n",
      "          2.22734939e-02,  8.11963156e-03,  2.18422964e-01],\n",
      "        [ 1.56625450e-01, -2.69755095e-01,  4.86745574e-02, ...,\n",
      "         -4.11374271e-02, -2.26247340e-01,  2.45687574e-01],\n",
      "        ...,\n",
      "        [ 8.66492540e-02, -2.95852274e-02, -3.36391702e-02, ...,\n",
      "          3.49360198e-01, -1.05429560e-01,  2.31839538e-01],\n",
      "        [ 8.66492540e-02, -2.95852274e-02, -3.36391702e-02, ...,\n",
      "          3.49360198e-01, -1.05429560e-01,  2.31839538e-01],\n",
      "        [ 8.66492540e-02, -2.95852274e-02, -3.36391702e-02, ...,\n",
      "          3.49360198e-01, -1.05429560e-01,  2.31839538e-01]],\n",
      "\n",
      "       [[-8.90067816e-02,  2.87213884e-02, -1.15465485e-01, ...,\n",
      "          1.48579121e-01, -1.28192753e-01, -7.21417144e-02],\n",
      "        [-2.89283972e-02,  2.51050834e-02, -2.76859384e-02, ...,\n",
      "          1.65881053e-01, -5.80900721e-02,  1.34153605e-01],\n",
      "        [ 2.58604698e-02, -8.98594409e-02, -9.89153050e-03, ...,\n",
      "          5.18296212e-02, -1.64665990e-02,  4.87837717e-02],\n",
      "        ...,\n",
      "        [ 9.93053392e-02, -6.37052953e-02, -1.12424210e-01, ...,\n",
      "          1.92345291e-01, -2.39803165e-01,  4.86071855e-02],\n",
      "        [ 9.93053392e-02, -6.37052953e-02, -1.12424210e-01, ...,\n",
      "          1.92345291e-01, -2.39803165e-01,  4.86071855e-02],\n",
      "        [ 9.93053392e-02, -6.37052953e-02, -1.12424210e-01, ...,\n",
      "          1.92345291e-01, -2.39803165e-01,  4.86071855e-02]],\n",
      "\n",
      "       [[-1.43496394e-01, -8.00028145e-02, -2.18410924e-01, ...,\n",
      "          2.65551116e-02,  1.80753171e-02, -3.10853839e-01],\n",
      "        [-6.22184947e-03,  6.16079681e-02, -1.84834033e-01, ...,\n",
      "         -1.07134148e-01, -3.92924100e-02, -6.50660023e-02],\n",
      "        [-7.13269487e-02,  2.05225069e-02, -1.80134714e-01, ...,\n",
      "         -1.90728649e-01, -3.43164317e-02, -1.35725990e-01],\n",
      "        ...,\n",
      "        [-1.28429562e-01,  5.36583699e-02, -2.07545087e-01, ...,\n",
      "          1.21587411e-01, -5.51070087e-02,  9.90653038e-02],\n",
      "        [-1.28429562e-01,  5.36583699e-02, -2.07545087e-01, ...,\n",
      "          1.21587411e-01, -5.51070087e-02,  9.90653038e-02],\n",
      "        [-1.28429562e-01,  5.36583699e-02, -2.07545087e-01, ...,\n",
      "          1.21587411e-01, -5.51070087e-02,  9.90653038e-02]]],\n",
      "      dtype=float32)), label_ids=array([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 0,\n",
      "       1, 0, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2]), metrics={'test_loss': 0.9603372812271118, 'test_accuracy': 0.5854700854700855, 'test_precision': 0.6537630032253688, 'test_recall': 0.5854700854700855, 'test_f1': 0.5067389875082183, 'test_runtime': 7.663, 'test_samples_per_second': 30.536, 'test_steps_per_second': 3.915})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48fb2163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed6cfde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")  # Adjust xticklabels and yticklabels as needed\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "346c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.1.4_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4543aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad099d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           36\n",
       "Cancer                   12\n",
       "Fitness                  11\n",
       "Diabetes                 10\n",
       "Bone health               9\n",
       "Hair                      8\n",
       "Neurological health       6\n",
       "Ear                       6\n",
       "Throat                    6\n",
       "Cardiovascular Health     6\n",
       "Skin                      5\n",
       "COVID                     5\n",
       "Eye                       4\n",
       "Blood                     4\n",
       "Mental Health             3\n",
       "Vascular                  2\n",
       "Women' s Health           2\n",
       "Men's health              2\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d45817c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skin                     19\n",
       "General Health           15\n",
       "Bone health              12\n",
       "Cardiovascular Health     6\n",
       "Muscles                   6\n",
       "Eye                       5\n",
       "Blood                     5\n",
       "Women' s Health           4\n",
       "Fitness                   4\n",
       "Men's health              4\n",
       "Hair                      4\n",
       "Neurological health       3\n",
       "Dental Health             3\n",
       "Throat                    3\n",
       "Diabetes                  2\n",
       "Vascular                  1\n",
       "COVID                     1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12933273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfApi, HfFolder\n",
    "\n",
    "# Define the model and tokenizer names or paths\n",
    "model_name = model_path\n",
    "\n",
    "# Define your Hugging Face model repository id\n",
    "repo_id = \"jeffyelson03/FLANT5_SentenceLevel_NER_Both\"\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Log in using your Hugging Face credentials\n",
    "username = \"jeffyelson03\"\n",
    "api_token = HfFolder.get_token()\n",
    "if not api_token:\n",
    "    api_token = api.login(username, password='Ovgujeff03#')\n",
    "\n",
    "# Upload the model to Hugging Face\n",
    "api.upload_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    folder_path=repo_id,\n",
    "    commit_message=\"Initial commit\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
