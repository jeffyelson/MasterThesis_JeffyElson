{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-927ab0163adb9fdb\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv',data_files='dataset_propositionattribution_nerfeatures.csv',delimiter=',',column_names=[\"claim\",\"premise\",\"label\",\"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\",\"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\",\"url\", \"entities\",\"entity_map\",\"gem_exp\",\"gem_label\",\"gpt_label\",\"gpt_exp\",\"gold_exp\",\"entity_map_ev\",\"entity_ev\",\"split\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133f9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d013d5114fa105ab.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-16f9acbdd82fea07.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-47c7de469e4087a2.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01252e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\",\"category\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-c65b58ebdf930a7e.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-479aed1e2b2c1583.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-ebdf88aafa3c37f3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 2, 'entailment': 0, 'neutral': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15010ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "\n",
    "        claim = item['claim']\n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences, claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "        item['input_ids'] = inputs['input_ids'].squeeze()  # Remove batch dimension\n",
    "        item['attention_mask']= inputs['attention_mask'].squeeze() # Remove batch dimension\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f5cf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.',\n",
       " 'premise': 'The essential oils of frankincense and myrrh increase the fluidity of the lipid bilayer in the cuticle and change the orderly and dense structure to increase the permeability of the skin and decrease the barrier effect.',\n",
       " 'label': 0,\n",
       " 'category': 'General Health',\n",
       " 'input_ids': tensor([   37,  1832, 10229,    13,     3,  6296,  2917,    75,  5167,    11,\n",
       "            82,    52,    52,   107,   993,     8,  5798,   485,    13,     8,\n",
       "             3, 19437,  2647, 18270,    16,     8,   123,  1225,   109,    11,\n",
       "           483,     8,   455,   120,    11, 13809,  1809,    12,   993,     8,\n",
       "             3, 31970,   485,    13,     8,  1133,    11,  6313,     8, 12515,\n",
       "          1504,     5,     1,   499,    52,    52,   107,  1832,  1043,    19,\n",
       "          1664,   261,    16, 26309,   494,    12,   199,  1172,     8,  3179,\n",
       "            13,     8,  1133,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 41:45, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>3.647157</td>\n",
       "      <td>0.660215</td>\n",
       "      <td>0.719802</td>\n",
       "      <td>0.660215</td>\n",
       "      <td>0.684721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>2.298374</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.692425</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.664441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>2.592527</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.700679</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.689248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>2.823298</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.705527</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.685283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>3.305382</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.719310</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.676285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.171287</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.717726</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.694256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.423495</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.723489</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.677785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.567796</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.718372</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.696738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.576445</td>\n",
       "      <td>0.681720</td>\n",
       "      <td>0.712269</td>\n",
       "      <td>0.681720</td>\n",
       "      <td>0.692064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.437078</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.706004</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.690269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.425606</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.717691</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.697485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.602653</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.711424</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.697207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.600889</td>\n",
       "      <td>0.694624</td>\n",
       "      <td>0.716985</td>\n",
       "      <td>0.694624</td>\n",
       "      <td>0.704053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.613823</td>\n",
       "      <td>0.694624</td>\n",
       "      <td>0.716985</td>\n",
       "      <td>0.694624</td>\n",
       "      <td>0.704053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.632369</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.713554</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.698693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-203] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-812/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-609] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-1827/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-2842/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.1_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.4.1_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.1_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.4.1_flant5/checkpoint-2639 (score: 0.6946236559139785).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 04:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.4.1_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.4.1_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.4.1_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.4.1_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.4.1_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.4.1_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.4.1_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.4.1_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.4.1_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.4.1_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.4.1_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.4.1_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.4.1_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.4.1_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.4.1_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.4.1_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.4.1_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.4.1_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-4.098204  ,  0.81646335,  2.6193185 ],\n",
      "       [ 8.224777  , -2.0527797 , -5.4967074 ],\n",
      "       [ 9.210207  , -4.131249  , -4.561612  ],\n",
      "       [-6.4353485 , -5.1626196 ,  9.505923  ],\n",
      "       [-6.5897136 , -4.4063234 ,  9.090062  ],\n",
      "       [ 9.304172  , -3.9524217 , -4.78301   ],\n",
      "       [ 8.625301  , -3.4569292 , -4.599187  ],\n",
      "       [ 9.177647  , -3.8702638 , -4.8093863 ],\n",
      "       [ 9.489122  , -3.9074597 , -4.956617  ],\n",
      "       [ 7.829866  , -3.9745634 , -3.4230092 ],\n",
      "       [ 8.612855  , -6.249879  , -2.288308  ],\n",
      "       [ 8.204347  , -3.2432468 , -4.4704933 ],\n",
      "       [-2.4161184 ,  1.2551827 ,  0.81301546],\n",
      "       [ 9.187547  , -4.0792246 , -4.6603146 ],\n",
      "       [ 8.104209  , -2.1459942 , -5.3122134 ],\n",
      "       [ 0.90828973, -4.1698046 ,  2.6792934 ],\n",
      "       [ 9.03685   , -2.88301   , -5.460697  ],\n",
      "       [ 8.650586  , -3.4630563 , -4.623122  ],\n",
      "       [ 8.705671  , -3.7183433 , -4.4367805 ],\n",
      "       [ 9.000557  , -4.0015616 , -4.525126  ],\n",
      "       [ 7.668458  , -3.557137  , -3.7555592 ],\n",
      "       [ 9.281573  , -4.3924384 , -4.4410725 ],\n",
      "       [ 7.968856  , -2.4615612 , -4.9038234 ],\n",
      "       [-5.915754  , -3.6323087 ,  7.9333467 ],\n",
      "       [-4.328102  , -6.3794794 ,  8.659126  ],\n",
      "       [-6.0788803 , -2.556228  ,  7.122785  ],\n",
      "       [ 9.254102  , -3.489669  , -5.155894  ],\n",
      "       [ 0.33377767,  9.522085  , -7.7549396 ],\n",
      "       [ 9.357353  , -3.7409863 , -5.0636225 ],\n",
      "       [ 9.063819  , -3.9901245 , -4.5757422 ],\n",
      "       [-4.864364  ,  9.070084  , -2.9409099 ],\n",
      "       [ 8.919256  , -3.1008873 , -5.2186046 ],\n",
      "       [ 8.331878  , -3.4281962 , -4.3446608 ],\n",
      "       [-1.4306924 , -4.1543756 ,  4.211256  ],\n",
      "       [-5.789484  , -4.1743956 ,  8.243626  ],\n",
      "       [ 8.828373  , -2.6085658 , -5.507627  ],\n",
      "       [-2.214724  , -4.156236  ,  5.01984   ],\n",
      "       [ 9.256318  , -3.707374  , -4.9666753 ],\n",
      "       [-6.58274   , -5.0786767 ,  9.577459  ],\n",
      "       [ 8.637865  , -2.500304  , -5.466498  ],\n",
      "       [-5.9708047 , -4.724628  ,  8.739262  ],\n",
      "       [ 7.873546  , -2.751235  , -4.6615024 ],\n",
      "       [ 8.1963825 , -2.8264143 , -4.6935678 ],\n",
      "       [-6.5185194 , -4.5848117 ,  9.1516905 ],\n",
      "       [-5.7860823 , -4.4161763 ,  8.474622  ],\n",
      "       [ 8.915483  , -3.7657695 , -4.615241  ],\n",
      "       [ 7.7927065 , -3.8186913 , -3.674282  ],\n",
      "       [ 9.237948  , -3.222695  , -5.3545775 ],\n",
      "       [ 9.093128  , -4.311753  , -4.397226  ],\n",
      "       [-5.830837  , -5.4651446 ,  9.258192  ],\n",
      "       [ 5.898932  , -4.8865085 , -1.1148176 ],\n",
      "       [-0.13784042,  5.715264  , -4.5985904 ],\n",
      "       [ 8.094779  , -4.7613287 , -3.0730882 ],\n",
      "       [-5.6271486 , -5.3082466 ,  8.94556   ],\n",
      "       [-2.372063  , -3.1732063 ,  4.32639   ],\n",
      "       [ 7.639624  , -6.1634026 , -1.4304249 ],\n",
      "       [-5.943867  , 10.813801  , -3.4550722 ],\n",
      "       [ 8.896144  , -3.8544328 , -4.5423856 ],\n",
      "       [ 7.5970554 , -3.7968707 , -3.3670578 ],\n",
      "       [ 8.87317   , -3.910012  , -4.512773  ],\n",
      "       [ 8.951362  , -3.51918   , -4.9303446 ],\n",
      "       [-5.8134036 , -5.0827656 ,  8.879203  ],\n",
      "       [ 7.573756  , -2.4634855 , -4.555471  ],\n",
      "       [-6.3136344 , -4.501658  ,  8.920526  ],\n",
      "       [-2.0008504 , 10.819683  , -6.643841  ],\n",
      "       [ 8.930182  , -4.1607637 , -4.3703084 ],\n",
      "       [ 9.371647  , -3.778659  , -5.023457  ],\n",
      "       [ 9.422069  , -4.084907  , -4.7595487 ],\n",
      "       [ 8.5013485 , -5.649664  , -2.6460507 ],\n",
      "       [-4.123683  , -6.200833  ,  8.239448  ],\n",
      "       [ 9.3545265 , -4.6264834 , -4.3526454 ],\n",
      "       [-3.7221863 , -4.7642136 ,  6.8561254 ],\n",
      "       [ 6.008677  ,  1.2062116 , -6.141806  ],\n",
      "       [ 7.6949024 , -2.742189  , -4.461401  ],\n",
      "       [ 7.642458  , -4.045893  , -3.3596861 ],\n",
      "       [ 7.9876766 , -5.7668505 , -2.2514262 ],\n",
      "       [ 9.431118  , -4.075515  , -4.8397126 ],\n",
      "       [ 8.817212  , -3.4312882 , -4.9241753 ],\n",
      "       [ 9.12639   , -4.1153755 , -4.607241  ],\n",
      "       [ 7.3167877 , -3.754521  , -3.240908  ],\n",
      "       [ 9.261112  , -4.5322833 , -4.3179426 ],\n",
      "       [ 8.408953  , -3.4708803 , -4.451906  ],\n",
      "       [ 6.972186  , -3.8274314 , -2.9716194 ],\n",
      "       [ 8.675667  , -4.0901713 , -4.1189003 ],\n",
      "       [ 9.350926  , -3.827604  , -5.000736  ],\n",
      "       [-3.4283116 ,  7.6112843 , -3.1226592 ],\n",
      "       [ 9.315096  , -3.975538  , -4.842929  ],\n",
      "       [ 7.8482995 , -3.9564617 , -3.6282475 ],\n",
      "       [ 9.008137  , -3.7885463 , -4.6730046 ],\n",
      "       [ 6.676623  , -2.3784153 , -3.8476534 ],\n",
      "       [ 9.249662  , -4.2172017 , -4.5078235 ],\n",
      "       [ 9.399161  , -4.2824717 , -4.6408634 ],\n",
      "       [-6.4156127 , -5.109163  ,  9.443819  ],\n",
      "       [ 9.121174  , -3.0275025 , -5.3982496 ],\n",
      "       [ 8.263294  , -4.29075   , -3.675898  ],\n",
      "       [ 8.419436  , -3.0413516 , -4.738124  ],\n",
      "       [-5.132953  , -5.772821  ,  8.775648  ],\n",
      "       [ 8.584943  , -4.100401  , -4.084557  ],\n",
      "       [ 9.054454  , -3.8271964 , -4.758574  ],\n",
      "       [ 8.1573305 , -2.8289623 , -4.8101993 ],\n",
      "       [-5.90953   , -5.399548  ,  9.239856  ],\n",
      "       [ 7.6474905 , -2.3091528 , -4.706794  ],\n",
      "       [ 9.530539  , -4.4446516 , -4.581124  ],\n",
      "       [ 8.670433  , -3.11226   , -5.0275717 ],\n",
      "       [ 9.385793  , -3.8708894 , -4.911209  ],\n",
      "       [ 9.2493    , -4.465916  , -4.344199  ],\n",
      "       [ 8.741033  , -4.2561283 , -4.1362076 ],\n",
      "       [-5.7889667 , -4.517601  ,  8.317329  ],\n",
      "       [ 9.149802  , -4.305144  , -4.4156165 ],\n",
      "       [ 9.089089  , -4.0934    , -4.5763927 ],\n",
      "       [-5.8910265 , -4.807726  ,  8.723883  ],\n",
      "       [ 9.344844  , -4.12727   , -4.7403755 ],\n",
      "       [ 8.324015  , -3.4244275 , -4.4972296 ],\n",
      "       [ 8.909122  , -3.6606882 , -4.7738347 ],\n",
      "       [ 9.380539  , -3.676199  , -5.083441  ],\n",
      "       [ 9.218533  , -3.9346182 , -4.778582  ],\n",
      "       [ 8.893947  , -3.9090626 , -4.51088   ],\n",
      "       [ 8.703608  , -4.5192127 , -3.8517113 ],\n",
      "       [ 8.208147  , -5.8738604 , -2.1502802 ],\n",
      "       [ 8.888021  , -4.158675  , -4.336429  ],\n",
      "       [-3.153542  , 11.559035  , -6.183211  ],\n",
      "       [ 8.969254  , -4.346367  , -4.2735443 ],\n",
      "       [ 8.672082  , -3.272645  , -4.883118  ],\n",
      "       [-5.041991  , 10.880867  , -4.4734383 ],\n",
      "       [ 9.12904   , -4.079393  , -4.5794945 ],\n",
      "       [ 7.9204307 , -3.4549654 , -4.0769534 ],\n",
      "       [ 2.520478  , -2.217069  , -0.30477828],\n",
      "       [ 9.085851  , -4.017285  , -4.5758834 ],\n",
      "       [ 9.173492  , -4.4695992 , -4.3342233 ],\n",
      "       [ 9.105451  , -3.9322045 , -4.689961  ],\n",
      "       [-0.06786394, -5.292476  ,  4.232649  ],\n",
      "       [-6.7880545 , -4.841045  ,  9.523244  ],\n",
      "       [ 9.022616  , -3.6090913 , -4.863787  ],\n",
      "       [-2.9761658 ,  9.600394  , -5.128728  ],\n",
      "       [ 7.1350694 , -2.5373766 , -4.007149  ],\n",
      "       [-5.1270986 , -3.794499  ,  7.240373  ],\n",
      "       [ 8.100074  , -4.80096   , -3.0870378 ],\n",
      "       [-6.514767  , -4.8915353 ,  9.29625   ],\n",
      "       [ 3.5032709 , -4.5504866 ,  0.53080577],\n",
      "       [ 9.524277  , -4.5569654 , -4.5204725 ],\n",
      "       [ 8.611156  , -3.744694  , -4.360132  ],\n",
      "       [-3.2264743 , 10.037365  , -5.2658095 ],\n",
      "       [-1.899194  ,  0.6698999 ,  0.7988058 ],\n",
      "       [ 8.998515  , -3.471282  , -5.0006914 ],\n",
      "       [ 7.582533  , -5.560201  , -2.0068238 ],\n",
      "       [ 7.9846787 , -1.9718624 , -5.23498   ],\n",
      "       [-6.1799035 , -4.48592   ,  8.804682  ],\n",
      "       [-6.330948  , -4.8571653 ,  9.195859  ],\n",
      "       [ 8.972549  , -3.5449395 , -4.858041  ],\n",
      "       [-6.362933  , -4.802244  ,  9.045435  ],\n",
      "       [ 7.5563226 , -1.4477587 , -5.2161303 ],\n",
      "       [ 9.317958  , -4.1720343 , -4.590045  ],\n",
      "       [ 9.27733   , -3.7993217 , -4.9199476 ],\n",
      "       [ 8.642754  , -4.041692  , -4.2127914 ],\n",
      "       [ 8.765869  , -4.35016   , -4.0223236 ],\n",
      "       [ 9.0909195 , -4.1772156 , -4.465243  ],\n",
      "       [ 8.803664  , -4.659827  , -3.7441056 ],\n",
      "       [ 8.800369  , -3.9779081 , -4.3753204 ],\n",
      "       [ 5.547416  , -1.7817323 , -3.3752587 ],\n",
      "       [-6.4397116 , -4.132967  ,  8.709031  ],\n",
      "       [-5.950998  , -2.913759  ,  7.3310003 ],\n",
      "       [-6.4098825 , -5.151396  ,  9.527682  ],\n",
      "       [ 7.1072717 , -2.532773  , -4.137154  ],\n",
      "       [-5.816758  , -3.6409905 ,  7.7104897 ],\n",
      "       [ 8.452845  , -2.9934638 , -4.806226  ],\n",
      "       [ 7.6210303 , -3.7513356 , -3.598263  ],\n",
      "       [ 9.368934  , -4.5633035 , -4.4269757 ],\n",
      "       [ 4.2517347 , -4.378869  ,  0.09626354],\n",
      "       [-6.599346  , -4.593892  ,  9.235905  ],\n",
      "       [ 0.73429024, -7.306107  ,  5.093841  ],\n",
      "       [-5.8330913 , -2.3115125 ,  6.7226753 ],\n",
      "       [ 6.1945853 , -5.5194817 , -0.7073202 ],\n",
      "       [-6.4220934 , -3.1786385 ,  7.782786  ],\n",
      "       [ 9.222455  , -3.9518313 , -4.7267594 ],\n",
      "       [ 9.546799  , -4.419396  , -4.630986  ],\n",
      "       [-1.1418518 ,  3.6578112 , -1.8714052 ],\n",
      "       [ 8.821655  , -4.2804184 , -4.1886067 ],\n",
      "       [-4.318297  , -5.9790497 ,  8.302557  ],\n",
      "       [ 9.365553  , -3.7551908 , -5.048638  ],\n",
      "       [-3.2786791 , -5.812547  ,  7.1859345 ],\n",
      "       [ 1.8432643 , -5.661387  ,  3.1093092 ],\n",
      "       [-1.6893433 , -5.2926164 ,  5.601079  ],\n",
      "       [-6.199521  , -5.238429  ,  9.384323  ],\n",
      "       [-3.9772155 ,  8.067415  , -3.2654626 ],\n",
      "       [-6.6170664 , -1.8158579 ,  6.8843236 ],\n",
      "       [ 8.665292  , -3.4850707 , -4.6050286 ],\n",
      "       [ 8.830487  , -3.7595992 , -4.6145196 ],\n",
      "       [ 8.8748255 , -4.1020837 , -4.351212  ],\n",
      "       [-6.5158677 , -4.520739  ,  9.07068   ],\n",
      "       [ 7.912324  , -2.755123  , -4.5949626 ],\n",
      "       [-7.8170233 ,  4.4938645 ,  2.7091157 ],\n",
      "       [ 3.30461   ,  5.259141  , -6.8774633 ],\n",
      "       [ 8.883438  , -3.6398945 , -4.6466794 ],\n",
      "       [-3.2383835 ,  9.40949   , -4.810241  ],\n",
      "       [-6.2883716 , -5.2123384 ,  9.440365  ],\n",
      "       [ 9.005364  , -3.4984376 , -4.98062   ],\n",
      "       [ 9.129568  , -3.663546  , -4.949617  ],\n",
      "       [ 9.486549  , -4.3420486 , -4.6286964 ],\n",
      "       [ 8.954143  , -4.5742583 , -3.8936436 ],\n",
      "       [-6.417024  , -4.9295497 ,  9.235102  ],\n",
      "       [ 8.6552105 , -3.1827967 , -4.8566303 ],\n",
      "       [ 9.088654  , -3.586902  , -4.9095674 ],\n",
      "       [-4.346464  , -3.8450093 ,  6.5221057 ],\n",
      "       [ 9.257309  , -3.7756364 , -4.8968587 ],\n",
      "       [-5.434125  , -4.6296206 ,  8.324091  ],\n",
      "       [-5.9337564 , -2.0609298 ,  6.5510674 ],\n",
      "       [ 8.922405  , -3.83474   , -4.5947037 ],\n",
      "       [ 8.4363165 , -2.7879856 , -5.044415  ],\n",
      "       [ 9.536038  , -4.093663  , -4.905143  ],\n",
      "       [ 9.058575  , -3.982892  , -4.6076493 ],\n",
      "       [-6.2909465 , -4.9855723 ,  9.273408  ],\n",
      "       [-6.488031  , -4.952557  ,  9.331142  ],\n",
      "       [ 8.494416  , -4.1233287 , -4.0194616 ],\n",
      "       [ 9.423942  , -3.688498  , -5.0833673 ],\n",
      "       [ 9.377547  , -3.878113  , -4.9723544 ],\n",
      "       [-6.40505   ,  5.261179  ,  0.8046131 ],\n",
      "       [ 0.7922311 ,  2.508156  , -2.95816   ],\n",
      "       [ 7.2673006 , -5.6480026 , -1.6804743 ],\n",
      "       [ 9.188189  , -3.6575665 , -4.976626  ],\n",
      "       [ 1.5763764 , -6.9101357 ,  4.102668  ],\n",
      "       [ 8.710024  , -3.0700424 , -5.09982   ],\n",
      "       [ 8.2536335 , -3.0723672 , -4.5943336 ],\n",
      "       [-5.3792253 , -5.7629724 ,  8.917978  ],\n",
      "       [ 9.116674  , -4.3668213 , -4.344249  ],\n",
      "       [ 9.344564  , -4.07918   , -4.7901726 ],\n",
      "       [ 8.98426   , -3.2105038 , -5.1197934 ],\n",
      "       [ 9.616508  , -4.097413  , -4.9012065 ],\n",
      "       [-5.634202  , -2.4320793 ,  6.618617  ],\n",
      "       [ 5.300278  , -4.299265  , -1.2025688 ],\n",
      "       [ 6.247231  , -3.1426158 , -2.9187033 ],\n",
      "       [ 8.990582  , -3.76534   , -4.6609926 ],\n",
      "       [ 8.869018  , -3.60105   , -4.813185  ],\n",
      "       [ 9.26783   , -3.824377  , -4.865643  ],\n",
      "       [-1.5480216 , -6.480052  ,  6.287965  ]], dtype=float32), array([[[ 0.02925041,  0.14605844, -0.00107968, ...,  0.07773403,\n",
      "          0.00500753,  0.01615521],\n",
      "        [ 0.09902526,  0.2681084 ,  0.0326094 , ...,  0.01406843,\n",
      "         -0.10444833,  0.03135377],\n",
      "        [ 0.00291098,  0.0120521 ,  0.00874721, ...,  0.0023787 ,\n",
      "         -0.01176831, -0.01087558],\n",
      "        ...,\n",
      "        [ 0.05288431,  0.20301938, -0.30017734, ...,  0.15145081,\n",
      "          0.05544138,  0.08176798],\n",
      "        [ 0.05288431,  0.20301938, -0.30017734, ...,  0.15145081,\n",
      "          0.05544138,  0.08176798],\n",
      "        [ 0.05288431,  0.20301938, -0.30017734, ...,  0.15145081,\n",
      "          0.05544138,  0.08176798]],\n",
      "\n",
      "       [[-0.23642375,  0.12482841, -0.12593825, ...,  0.13533124,\n",
      "          0.04988595, -0.16200136],\n",
      "        [-0.10167097, -0.14088577, -0.05341827, ...,  0.05882797,\n",
      "         -0.04696382, -0.03057453],\n",
      "        [-0.12540655,  0.07397258,  0.0194851 , ...,  0.2489803 ,\n",
      "          0.03821881,  0.03481887],\n",
      "        ...,\n",
      "        [-0.11284326,  0.02006532,  0.05916857, ...,  0.260855  ,\n",
      "          0.00385711, -0.10479033],\n",
      "        [-0.11284326,  0.02006532,  0.05916857, ...,  0.260855  ,\n",
      "          0.00385711, -0.10479033],\n",
      "        [-0.11284326,  0.02006532,  0.05916857, ...,  0.260855  ,\n",
      "          0.00385711, -0.10479033]],\n",
      "\n",
      "       [[-0.22172381, -0.01827926, -0.18267727, ...,  0.024403  ,\n",
      "          0.11832666, -0.02023961],\n",
      "        [ 0.00573234,  0.04699137, -0.11943387, ..., -0.02443992,\n",
      "          0.02226641,  0.01320788],\n",
      "        [-0.14322655, -0.10764527, -0.1395766 , ..., -0.03553759,\n",
      "         -0.00975142, -0.04911245],\n",
      "        ...,\n",
      "        [-0.1511645 ,  0.07397079, -0.01591383, ...,  0.1842227 ,\n",
      "          0.11804804,  0.08750983],\n",
      "        [-0.1511645 ,  0.07397079, -0.01591383, ...,  0.1842227 ,\n",
      "          0.11804804,  0.08750983],\n",
      "        [-0.1511645 ,  0.07397079, -0.01591383, ...,  0.1842227 ,\n",
      "          0.11804804,  0.08750983]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.13559578, -0.06977092, -0.08923847, ..., -0.0077413 ,\n",
      "          0.10101784, -0.02966363],\n",
      "        [ 0.04794916, -0.06400797,  0.03704565, ..., -0.04739844,\n",
      "          0.08038653,  0.21148819],\n",
      "        [ 0.08307284, -0.291278  ,  0.01818199, ..., -0.03084777,\n",
      "          0.02628259,  0.13757718],\n",
      "        ...,\n",
      "        [-0.26645523, -0.04466375, -0.03811134, ...,  0.16724676,\n",
      "         -0.08173689, -0.03793138],\n",
      "        [-0.26645523, -0.04466375, -0.03811134, ...,  0.16724676,\n",
      "         -0.08173689, -0.03793138],\n",
      "        [-0.26645523, -0.04466375, -0.03811134, ...,  0.16724676,\n",
      "         -0.08173689, -0.03793138]],\n",
      "\n",
      "       [[-0.16604209, -0.14856146,  0.08915804, ...,  0.09677237,\n",
      "          0.04858662, -0.16190843],\n",
      "        [-0.12218083, -0.10043473, -0.03565686, ..., -0.00623966,\n",
      "         -0.06751597, -0.00412864],\n",
      "        [-0.00092277, -0.07755251,  0.03904325, ..., -0.15488793,\n",
      "         -0.03421928, -0.07554848],\n",
      "        ...,\n",
      "        [-0.18435904,  0.05185454, -0.10037331, ...,  0.02477617,\n",
      "         -0.10344537,  0.07705072],\n",
      "        [-0.18435904,  0.05185454, -0.10037331, ...,  0.02477617,\n",
      "         -0.10344537,  0.07705072],\n",
      "        [-0.18435904,  0.05185454, -0.10037331, ...,  0.02477617,\n",
      "         -0.10344537,  0.07705072]],\n",
      "\n",
      "       [[-0.01400461,  0.02309338, -0.07884307, ..., -0.00655183,\n",
      "         -0.01333022, -0.05203077],\n",
      "        [ 0.04244164, -0.03864559, -0.21993874, ...,  0.01083278,\n",
      "         -0.13166599, -0.06158834],\n",
      "        [ 0.0990539 ,  0.00749852, -0.21456666, ..., -0.03156208,\n",
      "         -0.21262166, -0.06183182],\n",
      "        ...,\n",
      "        [ 0.05574864,  0.03847413, -0.12819122, ...,  0.24483429,\n",
      "         -0.0895048 , -0.05082082],\n",
      "        [ 0.05574864,  0.03847413, -0.12819122, ...,  0.24483429,\n",
      "         -0.0895048 , -0.05082082],\n",
      "        [ 0.05574864,  0.03847413, -0.12819122, ...,  0.24483429,\n",
      "         -0.0895048 , -0.05082082]]], dtype=float32)), label_ids=array([0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2,\n",
      "       0, 1, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 1, 0, 0,\n",
      "       0, 2, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 2,\n",
      "       0, 0, 2, 2, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       2, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 1, 0, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "       0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0,\n",
      "       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 0,\n",
      "       0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2]), metrics={'test_loss': 4.145283222198486, 'test_accuracy': 0.6282051282051282, 'test_precision': 0.6241179048871357, 'test_recall': 0.6282051282051282, 'test_f1': 0.6141444612032849, 'test_runtime': 7.6757, 'test_samples_per_second': 30.486, 'test_steps_per_second': 3.908})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "808dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48fb2163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed6cfde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")  # Adjust xticklabels and yticklabels as needed\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "346c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = test_data.__getitem__(idx)\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.4.1_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4543aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad099d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           34\n",
       "Cancer                   12\n",
       "Bone health              12\n",
       "Skin                     11\n",
       "Cardiovascular Health    10\n",
       "Fitness                  10\n",
       "Throat                    9\n",
       "Hair                      8\n",
       "Diabetes                  6\n",
       "Ear                       6\n",
       "Neurological health       6\n",
       "Women' s Health           5\n",
       "Men's health              4\n",
       "Blood                     4\n",
       "COVID                     3\n",
       "Eye                       3\n",
       "Mental Health             3\n",
       "Muscles                   1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d45817c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           17\n",
       "Skin                     13\n",
       "Bone health               9\n",
       "Eye                       6\n",
       "Diabetes                  6\n",
       "Blood                     5\n",
       "Muscles                   5\n",
       "Fitness                   5\n",
       "Hair                      4\n",
       "Dental Health             3\n",
       "Neurological health       3\n",
       "Vascular                  3\n",
       "COVID                     3\n",
       "Men's health              2\n",
       "Cardiovascular Health     2\n",
       "Women' s Health           1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
