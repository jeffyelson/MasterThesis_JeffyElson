{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ed78e0185729396\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 214.61it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d8a73572c5526b09.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71c34de47dd068c7.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9e4fe1ebeeeb9fd4.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='dataset_semanticattribution_nerfeatures_split.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"claim\", \"premise\", \"label\", \"category\", \"count_bf\", \"count_ca\", \"count_dis\",\n",
    "        \"count_food\", \"count_lipid\", \"count_treat\", \"pres_bf\", \"pres_ca\", \"pres_dis\",\n",
    "        \"pres_food\", \"pres_lipid\", \"pres_treat\", \"counte_bf\", \"counte_ca\", \"counte_dis\",\n",
    "        \"counte_food\", \"counte_lipid\", \"counte_treat\", \"prese_bf\", \"prese_ca\", \"prese_dis\",\n",
    "        \"prese_food\", \"prese_lipid\", \"prese_treat\", \"url\", \"entities\", \"entity_map\",\n",
    "        \"gold_exp\", \"gemini_exp\", \"gemini_label\",\"entity_ev\",\"entity_map_ev\", \"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f1db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\", \"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-cd62fee432318997.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d93ed7a61272b334.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-edc4f9887fcc00ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 0,\n",
    "    \"entailment\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "        claim = item['claim'] \n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        additional_features = [\n",
    "            \"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "    \n",
    "        for feature in additional_features:\n",
    "            if feature in item:\n",
    "                evidences += \"[SEP]\" + str(item[feature])\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences, claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "         # Construct the output item\n",
    "        output_item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),  # Remove batch dimension\n",
    "            'claim': claim,  # Include augmented claim text\n",
    "            'evidences': evidences  # Include original evidence text\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            output_item['label'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return output_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c5e0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    3,   117,   597,  9789,    23,     2,     6,   445,     5,   117,\n",
       "         18150,  2168,     2,     6,    27,     5,   117,  3049,     9,     2,\n",
       "          9789,    23,     2,     6,   276,     5,   117,   350, 31371,  9881,\n",
       "             2,     6,   283,     5,   117, 11469,    32,  1924,  2099,     2,\n",
       "             6,   411,     5,   117,   584,  1598,    32,  1924,  2099,     2,\n",
       "             6,   446,     5,  4937,    77,    75,  5167,    11,    82,    52,\n",
       "            52,   107,  1832, 10229,    11,  5958,    16,    75,  5167, 11349,\n",
       "            15,   581,  2179,    18,    77, 29884,     7,    13,  8802,  4900,\n",
       "         21128,     7,     5, 23518,    15,    15,    51,     6,    27,     5,\n",
       "             3, 16977,   235, 14676,     3,   184, 12206,  1055,     7,    13,\n",
       "          8054,    52,   954,  8511,    23,    41,     5, 20919,    23,    26,\n",
       "          1528,  2189,    11, 17133, 14367,  1951,    13,   991,    11,    70,\n",
       "         28661,   257,    28,    82,    52,    52,   107,    41, 10205,    23,\n",
       "         19968,     9,     3,  4641,  4641,    61,     3,    15,  4115,  1938,\n",
       "             5, 17135,  6067,     7,    10,  9222,   138, 23482,     7,    21,\n",
       "         13038,  2686,     5,  3643,  9669,  4331,    17,    63,     6,   445,\n",
       "             5,   117, 13329,  2152,   122,     6,   205,     5,   117,   901,\n",
       "          4331,   457,     6,   446,     5,   117, 21263,    40,     7,     6,\n",
       "           411,     5,   117, 11147,     6,   283,     5, 10060,  8009,    57,\n",
       "          6869,  7554,    10,   205,     5,    82,    52,    52,  1024,  6067,\n",
       "         13262,   302,  3068,  8527,     5,  1626,  6983, 13399,     6,   391,\n",
       "             5,   117,  1626,  6983, 13399,     6,   180,     5,   117,  1626,\n",
       "          6983, 13399,     6,   283,     5,   117, 19669,    40,  1665,     6,\n",
       "           283,     5,  4937,    77,    75,  5167,    41,     2,   391,     2,\n",
       "             3,     4,    23,     2,  1725,   117,     5,  3244,    61,    10,\n",
       "          1029,     8,  1801,    13,  1435,  1564,    12,     8,  3714, 30512,\n",
       "         10896,    21,     8,  9793,    11,  1058,    13,  2261,  6716,     5,\n",
       "         21299,    11, 17133, 22763,  6546,  1756,    13,     3,  6296,  2917,\n",
       "            75,  5167,  1043,     5,  2570,  4718,     7,     3,  6443,  1491,\n",
       "             7,  2091,    23,     9,  5819,     7,     5,     3,   117, 10078,\n",
       "             6,   262,     5,  4937,    77,    75,  5167,    11,    82,    52,\n",
       "            52,   107,    38, 20203,    16,   502,     5,     1,   499,    52,\n",
       "            52,   107,  1832,  1043,    19,  1664,   261,    16, 26309,   494,\n",
       "            12,   199,  1172,     8,  3179,    13,     8,  1133,     5,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'claim': 'Myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.',\n",
       " 'evidences': '; Unković, N.; Dimkić, I.; Janaćković, P.; Gavrilović, M.; Stanojević, O.; Vukojević, J. Frankincense and myrrh essential oils and burn incense fume against micro-inhabitants of sacral ambients.\\nShameem, I. Phytochemical & therapeutic potentials of Murr makki (.\\nOxidative stress and immunotoxic effects of lead and their amelioration with myrrh (Commiphora molmol) emulsion.\\nEssential Oils: Magical Ingredients for Skin Care.\\nChakravarty, N.; Kellogg, C.; Alvarez, J.; Equils, O.; Morgan, M. UV Protection by Natural Products: C. myrrha Oil Versus Sunscreen.\\nHamidpour, R.; Hamidpour, S.; Hamidpour, M.; Shahlari, M. Frankincense (乳香 Rǔ Xiāng;.\\nspecies): From the selection of traditional applications to the novel phytotherapy for the prevention and treatment of serious diseases.\\nChemistry and immunomodulatory activity of frankincense oil.\\nCompositions containing Boswellia extracts.\\n; Cooper, E. Frankincense and myrrh as remedies in children.',\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 42:04, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.822800</td>\n",
       "      <td>0.780363</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.569087</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.571116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606800</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.670447</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.670604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.928849</td>\n",
       "      <td>0.662366</td>\n",
       "      <td>0.671012</td>\n",
       "      <td>0.662366</td>\n",
       "      <td>0.665632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>1.351873</td>\n",
       "      <td>0.696774</td>\n",
       "      <td>0.686235</td>\n",
       "      <td>0.696774</td>\n",
       "      <td>0.690824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>1.766587</td>\n",
       "      <td>0.660215</td>\n",
       "      <td>0.693390</td>\n",
       "      <td>0.660215</td>\n",
       "      <td>0.673473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>2.390091</td>\n",
       "      <td>0.636559</td>\n",
       "      <td>0.681759</td>\n",
       "      <td>0.636559</td>\n",
       "      <td>0.655335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>2.786201</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.697948</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.663358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>3.029095</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.682790</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.658304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>3.212965</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.695038</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.661831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>3.356976</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.689280</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.663787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>3.496253</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.681534</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.651326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>3.562442</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.690498</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.662969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>3.623065</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.688781</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.666338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>3.632169</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.683375</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.659459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>3.629786</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.681771</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.658727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-203] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-1015/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.4_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.2.4_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.4_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.2.4_flant5/checkpoint-812 (score: 0.6967741935483871).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.2.4_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.2.4_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.2.4_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.2.4_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.2.4_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.2.4_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.2.4_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.2.4_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.2.4_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.2.4_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.2.4_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.2.4_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.2.4_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.2.4_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.2.4_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.2.4_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.2.4_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.2.4_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-3.83540344e+00,  4.86130762e+00, -1.23755240e+00],\n",
      "       [ 7.71199241e-02,  1.70639610e+00, -1.75163686e+00],\n",
      "       [-5.06953096e+00,  5.64914274e+00, -9.28451478e-01],\n",
      "       [ 4.17503506e-01,  1.41150907e-01, -8.07291567e-01],\n",
      "       [-1.49685597e+00,  3.72357035e+00, -2.24614978e+00],\n",
      "       [-5.35433054e+00,  6.48080778e+00, -1.15698147e+00],\n",
      "       [-2.40498818e-02,  5.08156443e+00, -3.94305348e+00],\n",
      "       [-4.83381939e+00,  6.65056992e+00, -1.76568079e+00],\n",
      "       [-5.34573698e+00,  5.27206802e+00, -4.21321720e-01],\n",
      "       [-3.44454265e+00,  5.88130760e+00, -2.10971379e+00],\n",
      "       [-2.73802185e+00,  4.87423325e+00, -1.84870958e+00],\n",
      "       [-2.90683794e+00,  5.26317549e+00, -2.05194879e+00],\n",
      "       [-2.31355739e+00,  3.82293129e+00, -1.45545793e+00],\n",
      "       [-4.08987570e+00,  5.67057991e+00, -1.50933409e+00],\n",
      "       [-2.31789970e+00,  3.32271194e+00, -1.16467309e+00],\n",
      "       [ 4.25330281e-01,  6.88394606e-01, -1.26526475e+00],\n",
      "       [-3.40782547e+00,  2.41760039e+00,  6.14653289e-01],\n",
      "       [-1.76310849e+00,  2.96621275e+00, -1.14003801e+00],\n",
      "       [-4.36039495e+00,  7.06258774e+00, -2.07232118e+00],\n",
      "       [-5.92459059e+00,  7.41026306e+00, -1.32693028e+00],\n",
      "       [-3.35015726e+00,  4.25189781e+00, -1.16055822e+00],\n",
      "       [-3.79171848e+00,  4.68950748e+00, -1.34710693e+00],\n",
      "       [-1.26882088e+00,  4.13176203e+00, -2.44961190e+00],\n",
      "       [ 1.27624655e+00,  3.33023548e-01, -1.51573431e+00],\n",
      "       [ 2.57878613e+00, -2.85227132e+00,  1.30042702e-01],\n",
      "       [ 2.68044996e+00, -3.28071260e+00,  5.51660001e-01],\n",
      "       [-2.64131975e+00,  5.67180014e+00, -2.41212320e+00],\n",
      "       [-3.59135079e+00,  5.02622223e+00, -1.27473199e+00],\n",
      "       [-1.07599437e+00,  1.15871064e-01,  4.48302060e-01],\n",
      "       [-2.12065077e+00,  4.92407751e+00, -2.33547044e+00],\n",
      "       [-8.04204583e-01,  3.35603166e+00, -2.26967716e+00],\n",
      "       [-6.26329565e+00,  7.25158787e+00, -1.06111419e+00],\n",
      "       [-3.50482321e+00,  6.16248369e+00, -2.13300538e+00],\n",
      "       [-3.91128826e+00,  5.45202923e+00, -1.60160351e+00],\n",
      "       [-5.57107401e+00,  4.75541496e+00,  2.77604192e-01],\n",
      "       [-4.95373917e+00,  5.75297832e+00, -9.14194882e-01],\n",
      "       [-1.53653669e+00,  1.94371068e+00, -5.78018010e-01],\n",
      "       [-3.55173111e+00,  5.87680483e+00, -2.10150290e+00],\n",
      "       [ 3.52821684e+00, -1.80662644e+00, -1.56190073e+00],\n",
      "       [-4.05967236e+00,  4.08432007e+00, -2.87508219e-01],\n",
      "       [-9.97458756e-01,  2.67989707e+00, -1.67460144e+00],\n",
      "       [-2.52908993e+00,  2.76900196e+00, -6.41402006e-01],\n",
      "       [-3.43658519e+00,  5.33490229e+00, -1.84765089e+00],\n",
      "       [ 2.36713886e+00, -9.41889107e-01, -1.14974272e+00],\n",
      "       [-5.44611406e+00,  4.63371944e+00,  2.77153194e-01],\n",
      "       [-5.77678442e+00,  6.78997517e+00, -1.04379630e+00],\n",
      "       [-3.89116573e+00,  4.09932947e+00, -5.63099563e-01],\n",
      "       [-1.44963503e+00,  3.36841154e+00, -1.75291324e+00],\n",
      "       [-4.59407234e+00,  7.11875772e+00, -2.12703681e+00],\n",
      "       [ 3.65841341e+00, -3.24669623e+00, -3.48394066e-01],\n",
      "       [-2.04608893e+00,  3.14557409e+00, -1.34369147e+00],\n",
      "       [-6.13423049e-01,  3.94575000e+00, -3.01843548e+00],\n",
      "       [-1.34772122e+00,  2.37537169e+00, -1.35349953e+00],\n",
      "       [-4.73238659e+00,  5.88756561e+00, -1.19695365e+00],\n",
      "       [ 1.31188869e+00,  9.71154749e-01, -2.04223561e+00],\n",
      "       [-4.11249733e+00,  5.78853798e+00, -1.58459854e+00],\n",
      "       [-1.29246879e+00,  2.30952382e+00, -1.16670454e+00],\n",
      "       [-4.33317137e+00,  4.75329781e+00, -5.01828849e-01],\n",
      "       [-5.46994162e+00,  6.64093685e+00, -1.21459889e+00],\n",
      "       [-3.87032986e+00,  5.87396622e+00, -1.98018754e+00],\n",
      "       [-4.73864317e+00,  6.74630308e+00, -1.85892630e+00],\n",
      "       [ 3.13194561e+00, -2.70533872e+00, -3.45933408e-01],\n",
      "       [-4.07038689e+00,  5.67323971e+00, -1.61723971e+00],\n",
      "       [-2.03689981e+00,  4.88550043e+00, -2.52609253e+00],\n",
      "       [-4.89524364e+00,  4.48846579e+00, -8.63581598e-02],\n",
      "       [-3.97591376e+00,  5.51981211e+00, -1.60415292e+00],\n",
      "       [-5.19097805e+00,  6.76239920e+00, -1.52268696e+00],\n",
      "       [-4.73229837e+00,  6.93186188e+00, -2.04922938e+00],\n",
      "       [-3.36216688e+00,  5.19771528e+00, -1.73414302e+00],\n",
      "       [ 3.39438772e+00, -2.41923618e+00, -7.46202111e-01],\n",
      "       [-2.82220674e+00,  3.02025676e+00, -6.02577925e-01],\n",
      "       [-3.25046921e+00,  3.35358405e+00, -5.17723978e-01],\n",
      "       [-4.56046057e+00,  2.93474627e+00,  9.06554461e-01],\n",
      "       [-5.36056936e-01,  2.03415245e-01, -9.40859914e-02],\n",
      "       [ 1.19307864e+00,  1.43911004e+00, -2.26524901e+00],\n",
      "       [ 2.23142242e+00, -1.62489367e+00, -6.06814444e-01],\n",
      "       [-4.03828287e+00,  5.57301760e+00, -1.64769316e+00],\n",
      "       [-3.93636346e+00,  6.30649805e+00, -2.17881107e+00],\n",
      "       [-4.46317053e+00,  7.38383770e+00, -2.72442222e+00],\n",
      "       [ 2.03721380e+00,  1.25158858e+00, -2.76856875e+00],\n",
      "       [-3.08396220e+00,  6.87822914e+00, -2.95122313e+00],\n",
      "       [-4.93832016e+00,  7.41692686e+00, -2.05697846e+00],\n",
      "       [-5.83388090e+00,  7.22050858e+00, -1.22270036e+00],\n",
      "       [-2.07230759e+00,  3.06313825e+00, -1.18257499e+00],\n",
      "       [-4.49221039e+00,  5.93356848e+00, -1.52178240e+00],\n",
      "       [ 1.69003713e+00,  1.66693723e+00, -2.77522922e+00],\n",
      "       [-3.96318626e+00,  6.15104294e+00, -1.99582458e+00],\n",
      "       [-2.45434451e+00,  2.60399532e+00, -5.07058203e-01],\n",
      "       [-4.17832756e+00,  4.88533783e+00, -1.01478362e+00],\n",
      "       [-4.37800360e+00,  4.82604599e+00, -6.75387621e-01],\n",
      "       [-5.72220945e+00,  6.09225082e+00, -8.91353548e-01],\n",
      "       [-1.65260422e+00,  4.17019463e+00, -2.24776006e+00],\n",
      "       [-5.49989998e-01,  3.28566670e+00, -2.38663435e+00],\n",
      "       [-2.51022720e+00,  4.87853336e+00, -2.09812260e+00],\n",
      "       [-1.21288872e+00,  3.94728971e+00, -2.19449711e+00],\n",
      "       [-2.10693860e+00,  3.84902549e+00, -2.00952697e+00],\n",
      "       [ 6.90542519e-01,  1.98295784e+00, -2.34461856e+00],\n",
      "       [-1.65776396e+00,  3.49023890e+00, -1.65549660e+00],\n",
      "       [-3.85699677e+00,  6.86371994e+00, -2.70779586e+00],\n",
      "       [-2.03036809e+00,  3.75476813e+00, -1.37755060e+00],\n",
      "       [ 4.14208317e+00, -3.02610540e+00, -7.48376131e-01],\n",
      "       [-4.08921909e+00,  6.22380400e+00, -1.78752708e+00],\n",
      "       [-6.08277988e+00,  6.07027149e+00, -3.78057122e-01],\n",
      "       [-4.59087133e+00,  4.96260834e+00, -6.02691531e-01],\n",
      "       [-3.69757786e-02,  8.95562768e-02, -4.91763115e-01],\n",
      "       [-4.69604778e+00,  5.14429951e+00, -8.48881185e-01],\n",
      "       [-3.87952352e+00,  4.53494263e+00, -8.60620797e-01],\n",
      "       [ 6.71865046e-01,  9.98000741e-01, -1.72765124e+00],\n",
      "       [-4.01531029e+00,  5.96691227e+00, -1.76945412e+00],\n",
      "       [-5.28214455e+00,  6.71644926e+00, -1.50750780e+00],\n",
      "       [-2.46110010e+00,  3.16436911e+00, -9.89171982e-01],\n",
      "       [-5.52038717e+00,  6.86317158e+00, -1.33316648e+00],\n",
      "       [-4.54987669e+00,  6.31694889e+00, -1.75227380e+00],\n",
      "       [-3.35374928e+00,  5.17336369e+00, -1.85010362e+00],\n",
      "       [-4.38500023e+00,  5.69024086e+00, -1.25635958e+00],\n",
      "       [-4.67432976e+00,  5.38003731e+00, -1.01955998e+00],\n",
      "       [-4.49932051e+00,  6.91801739e+00, -2.25212574e+00],\n",
      "       [ 4.87756543e-02,  2.28844523e+00, -2.07589793e+00],\n",
      "       [-1.01629448e+00,  4.34995842e+00, -2.69383979e+00],\n",
      "       [-7.22022772e-01,  3.58552670e+00, -2.62667561e+00],\n",
      "       [-5.16889954e+00,  5.96800995e+00, -1.14189708e+00],\n",
      "       [-3.53701448e+00,  5.68117094e+00, -1.92055273e+00],\n",
      "       [-5.49620962e+00,  5.70352936e+00, -8.17074716e-01],\n",
      "       [-5.07173920e+00,  5.48019123e+00, -7.69690037e-01],\n",
      "       [-5.29660320e+00,  6.25798273e+00, -1.24868524e+00],\n",
      "       [-4.57342339e+00,  8.20351315e+00, -2.60865021e+00],\n",
      "       [-2.68719649e+00,  2.85181832e+00, -6.77586436e-01],\n",
      "       [-2.95717859e+00,  4.69034576e+00, -1.76931977e+00],\n",
      "       [-1.50780833e+00,  3.07656455e+00, -1.50612402e+00],\n",
      "       [-2.03690268e-02,  2.83412528e+00, -2.57580900e+00],\n",
      "       [ 1.77523446e+00,  8.79974365e-02, -1.68242681e+00],\n",
      "       [ 2.24674964e+00,  3.80058110e-01, -2.32320142e+00],\n",
      "       [-4.99482346e+00,  6.21944571e+00, -1.39514232e+00],\n",
      "       [ 9.10731852e-01,  2.09491420e+00, -2.65391302e+00],\n",
      "       [-1.82133067e+00,  2.57172799e+00, -1.00156605e+00],\n",
      "       [-3.71253586e+00,  4.52369356e+00, -9.48821604e-01],\n",
      "       [-2.21117449e+00,  4.18204069e+00, -1.81400490e+00],\n",
      "       [ 2.41085887e+00, -2.33584666e+00, -1.38318509e-01],\n",
      "       [ 2.37654543e+00, -1.02626765e+00, -1.29157186e+00],\n",
      "       [-4.33226585e+00,  4.79167795e+00, -7.27792919e-01],\n",
      "       [-3.93122149e+00,  5.72410488e+00, -1.66486466e+00],\n",
      "       [-4.33248758e+00,  4.38608456e+00, -5.27115464e-01],\n",
      "       [-4.21139002e+00,  5.59463310e+00, -1.49830186e+00],\n",
      "       [-3.37077188e+00,  7.26958656e+00, -3.24453855e+00],\n",
      "       [ 1.95194805e+00, -2.71776652e+00,  5.28851151e-01],\n",
      "       [ 1.28629994e+00, -2.49276578e-01, -1.04884124e+00],\n",
      "       [-2.02421260e+00,  4.32421684e+00, -2.21756935e+00],\n",
      "       [ 2.56128401e-01,  1.51219380e+00, -1.61010158e+00],\n",
      "       [-4.96464682e+00,  6.35260105e+00, -1.30961657e+00],\n",
      "       [ 1.54088914e+00,  1.01440227e+00, -2.16614151e+00],\n",
      "       [-2.85921693e+00,  3.66765475e+00, -9.65124190e-01],\n",
      "       [-4.79548740e+00,  6.30375862e+00, -1.51908064e+00],\n",
      "       [-4.96389914e+00,  5.40661621e+00, -6.03748083e-01],\n",
      "       [-9.32164490e-01,  3.31472993e+00, -2.23283124e+00],\n",
      "       [-5.44050455e+00,  6.39711905e+00, -1.08837819e+00],\n",
      "       [ 4.44323756e-02, -1.28704399e-01, -3.38916928e-01],\n",
      "       [-3.61343813e+00,  4.20539999e+00, -8.24433565e-01],\n",
      "       [-3.68738317e+00,  4.53268957e+00, -9.93128657e-01],\n",
      "       [ 1.53610027e+00,  1.28221512e+00, -2.47152495e+00],\n",
      "       [ 2.04018021e+00, -2.02455807e+00, -1.06952831e-01],\n",
      "       [ 2.98380828e+00, -1.97008681e+00, -7.33413994e-01],\n",
      "       [ 3.93157816e+00, -3.06102324e+00, -7.67087698e-01],\n",
      "       [-1.92578757e+00,  6.02908230e+00, -3.51156378e+00],\n",
      "       [ 2.61040950e+00, -7.49404192e-01, -1.55326760e+00],\n",
      "       [-9.59033072e-01,  2.65991354e+00, -1.75073409e+00],\n",
      "       [-1.26132798e+00,  2.67734790e+00, -1.43213642e+00],\n",
      "       [-2.05607486e+00,  4.97361135e+00, -2.54560232e+00],\n",
      "       [ 1.32563984e+00,  2.44003087e-01, -1.45585358e+00],\n",
      "       [-1.14632333e-02,  1.23239219e+00, -1.24731600e+00],\n",
      "       [-4.41272306e+00,  6.43590546e+00, -1.83149540e+00],\n",
      "       [ 2.82783008e+00, -9.90428567e-01, -1.35912931e+00],\n",
      "       [-3.20915103e+00,  7.24491692e+00, -3.28313231e+00],\n",
      "       [ 1.20370007e+00,  4.22184736e-01, -1.63474512e+00],\n",
      "       [-3.69957995e+00,  7.01675367e+00, -2.84595442e+00],\n",
      "       [-4.34477282e+00,  5.03539896e+00, -8.09638739e-01],\n",
      "       [ 1.66643536e+00, -1.74546885e+00,  1.63209587e-01],\n",
      "       [-4.51507950e+00,  6.22073364e+00, -1.94441366e+00],\n",
      "       [ 2.93158793e+00, -3.03667498e+00,  5.58505766e-02],\n",
      "       [-3.35097146e+00,  5.25801277e+00, -2.07993269e+00],\n",
      "       [ 2.38561606e+00, -2.19581917e-01, -1.90550661e+00],\n",
      "       [ 1.13424218e+00,  2.96196970e-03, -1.19755137e+00],\n",
      "       [-2.36566114e+00,  3.40812635e+00, -1.06276095e+00],\n",
      "       [ 3.76956916e+00, -2.52629209e+00, -1.08400667e+00],\n",
      "       [-3.60937047e+00,  4.25273371e+00, -9.50289011e-01],\n",
      "       [-5.75573874e+00,  5.42093468e+00,  1.06046505e-01],\n",
      "       [-3.95207953e+00,  6.59675121e+00, -2.62312675e+00],\n",
      "       [-7.29963601e-01,  1.53463161e+00, -8.27083886e-01],\n",
      "       [-5.58244324e+00,  5.93613815e+00, -4.73210603e-01],\n",
      "       [ 1.04883075e+00,  3.57097566e-01, -1.56303322e+00],\n",
      "       [-4.91843271e+00,  6.45845890e+00, -1.39372671e+00],\n",
      "       [-4.06990671e+00,  3.81733084e+00, -2.39530146e-01],\n",
      "       [-1.49934161e+00,  3.04675889e+00, -1.63707244e+00],\n",
      "       [-2.26939869e+00,  6.75429916e+00, -3.74906135e+00],\n",
      "       [-8.42936635e-01,  2.67236495e+00, -1.70400286e+00],\n",
      "       [ 3.65528822e+00, -1.64842045e+00, -1.73373139e+00],\n",
      "       [-5.09015465e+00,  6.52305460e+00, -1.66924202e+00],\n",
      "       [-2.57290292e+00,  3.82622933e+00, -1.23624468e+00],\n",
      "       [-5.85949469e+00,  6.03533936e+00, -4.88847405e-01],\n",
      "       [-1.84030116e+00,  5.53563721e-02,  1.16746831e+00],\n",
      "       [-3.44120121e+00,  3.55185556e+00, -4.76023763e-01],\n",
      "       [-4.63213253e+00,  6.72858000e+00, -2.03819704e+00],\n",
      "       [-5.32897091e+00,  7.36168957e+00, -2.04329157e+00],\n",
      "       [-4.03293610e+00,  4.13939381e+00, -3.28334957e-01],\n",
      "       [-4.36532497e+00,  6.26217937e+00, -1.73437655e+00],\n",
      "       [-4.38549423e+00,  4.04032803e+00, -1.19904473e-01],\n",
      "       [-6.93258047e-02,  1.61043227e+00, -1.64673495e+00],\n",
      "       [-3.56132293e+00,  6.17621517e+00, -2.20401764e+00],\n",
      "       [-3.64876866e-01,  5.86921751e-01, -4.51084197e-01],\n",
      "       [-5.34981966e+00,  6.10613537e+00, -1.29758811e+00],\n",
      "       [-1.95854712e+00,  5.25792217e+00, -2.86715508e+00],\n",
      "       [-2.08725882e+00,  2.60302186e+00, -7.88696051e-01],\n",
      "       [-1.60431993e+00, -1.27888257e-02,  1.00741434e+00],\n",
      "       [-4.25577593e+00,  6.93957138e+00, -2.32663608e+00],\n",
      "       [-5.12194061e+00,  5.31745672e+00, -7.11819053e-01],\n",
      "       [-4.95735359e+00,  6.06128216e+00, -1.02523017e+00],\n",
      "       [-1.96442854e+00,  2.99065924e+00, -1.16275942e+00],\n",
      "       [ 3.53801966e-01,  1.84980571e+00, -1.94777632e+00],\n",
      "       [-3.19183016e+00,  5.22142792e+00, -1.74040902e+00],\n",
      "       [-5.45043421e+00,  7.11233807e+00, -1.70078230e+00],\n",
      "       [ 3.17379761e+00, -3.09267807e+00, -1.64650083e-01],\n",
      "       [-3.12425685e+00,  8.05112457e+00, -3.96786880e+00],\n",
      "       [-5.26091146e+00,  6.82207012e+00, -1.69878960e+00],\n",
      "       [ 1.48882747e+00, -3.43577266e-02, -1.42920256e+00],\n",
      "       [-4.16698265e+00,  7.09004307e+00, -2.67934108e+00],\n",
      "       [-3.51193333e+00,  4.87086678e+00, -1.53351665e+00],\n",
      "       [-4.67079592e+00,  7.91107321e+00, -2.86215901e+00],\n",
      "       [-4.77936792e+00,  6.23529625e+00, -1.51456952e+00],\n",
      "       [ 2.53176403e+00, -2.84174395e+00,  1.64514869e-01],\n",
      "       [-1.58819151e+00,  4.56579304e+00, -2.59836435e+00],\n",
      "       [ 2.36105442e+00, -1.41209579e+00, -9.07504022e-01],\n",
      "       [-5.80039930e+00,  6.97464895e+00, -1.28738546e+00],\n",
      "       [ 1.38252056e+00,  2.46311593e+00, -3.27944231e+00],\n",
      "       [-5.57426834e+00,  6.33374071e+00, -9.36694622e-01],\n",
      "       [ 2.77277064e+00, -3.61895323e+00,  5.61420739e-01]], dtype=float32), array([[[ 0.14157051,  0.08714639,  0.02949239, ..., -0.03583585,\n",
      "          0.05857265,  0.06867012],\n",
      "        [ 0.03140193, -0.05388371, -0.07571559, ...,  0.1425772 ,\n",
      "          0.02385107, -0.02797378],\n",
      "        [-0.02629095, -0.08135016,  0.02042531, ..., -0.09928636,\n",
      "         -0.1211649 ,  0.12108563],\n",
      "        ...,\n",
      "        [ 0.18127343,  0.03568403,  0.07067025, ...,  0.3103419 ,\n",
      "         -0.05801573,  0.15890998],\n",
      "        [ 0.18127343,  0.03568403,  0.07067025, ...,  0.3103419 ,\n",
      "         -0.05801573,  0.15890998],\n",
      "        [ 0.18127343,  0.03568403,  0.07067025, ...,  0.3103419 ,\n",
      "         -0.05801573,  0.15890998]],\n",
      "\n",
      "       [[-0.04748142,  0.17535292, -0.06705817, ...,  0.1153188 ,\n",
      "          0.05716052,  0.08501639],\n",
      "        [ 0.03302673,  0.18435584, -0.11690614, ..., -0.16522756,\n",
      "          0.04059344,  0.11786968],\n",
      "        [ 0.0113388 ,  0.17503512, -0.09978501, ..., -0.09331216,\n",
      "         -0.0793355 ,  0.14049692],\n",
      "        ...,\n",
      "        [ 0.07493044,  0.20187832,  0.1136708 , ...,  0.24964064,\n",
      "          0.14214426,  0.12402254],\n",
      "        [ 0.07493044,  0.20187832,  0.1136708 , ...,  0.24964064,\n",
      "          0.14214426,  0.12402254],\n",
      "        [ 0.07493044,  0.20187832,  0.1136708 , ...,  0.24964064,\n",
      "          0.14214426,  0.12402254]],\n",
      "\n",
      "       [[-0.04753138,  0.13170007,  0.06783294, ...,  0.01979077,\n",
      "          0.2322019 , -0.01148932],\n",
      "        [ 0.04052077,  0.3016903 , -0.13427024, ...,  0.15268631,\n",
      "          0.15495487,  0.14337116],\n",
      "        [ 0.05846765,  0.22627306, -0.0654975 , ..., -0.10020762,\n",
      "          0.09449156,  0.05636106],\n",
      "        ...,\n",
      "        [ 0.3269167 , -0.12046231,  0.05772842, ...,  0.2672864 ,\n",
      "          0.13116208,  0.19627039],\n",
      "        [ 0.3269167 , -0.12046231,  0.05772842, ...,  0.2672864 ,\n",
      "          0.13116208,  0.19627039],\n",
      "        [ 0.3269167 , -0.12046231,  0.05772842, ...,  0.2672864 ,\n",
      "          0.13116208,  0.19627039]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.22426593,  0.23175664, -0.12618299, ...,  0.28848916,\n",
      "         -0.18474251,  0.04494556],\n",
      "        [-0.12299956,  0.28472224, -0.0268499 , ...,  0.1671966 ,\n",
      "         -0.00922001,  0.2557267 ],\n",
      "        [-0.07781825,  0.29886323, -0.01334541, ...,  0.17695518,\n",
      "         -0.04459051,  0.02288089],\n",
      "        ...,\n",
      "        [ 0.0337238 ,  0.18458323,  0.13415879, ...,  0.2986228 ,\n",
      "          0.13997018, -0.03656774],\n",
      "        [ 0.0337238 ,  0.18458323,  0.13415879, ...,  0.2986228 ,\n",
      "          0.13997018, -0.03656774],\n",
      "        [ 0.0337238 ,  0.18458323,  0.13415879, ...,  0.2986228 ,\n",
      "          0.13997018, -0.03656774]],\n",
      "\n",
      "       [[-0.1606259 , -0.0155895 , -0.07871813, ...,  0.26318958,\n",
      "          0.00515565, -0.0048097 ],\n",
      "        [ 0.05906457,  0.02094847, -0.07175189, ...,  0.15278299,\n",
      "          0.00398454,  0.0914189 ],\n",
      "        [ 0.13870782, -0.11856655, -0.04612475, ...,  0.01321788,\n",
      "          0.05617856, -0.05356995],\n",
      "        ...,\n",
      "        [-0.01408316, -0.00432171,  0.05856919, ...,  0.2892809 ,\n",
      "          0.16137455,  0.09605386],\n",
      "        [-0.01408316, -0.00432171,  0.05856919, ...,  0.2892809 ,\n",
      "          0.16137455,  0.09605386],\n",
      "        [-0.01408316, -0.00432171,  0.05856919, ...,  0.2892809 ,\n",
      "          0.16137455,  0.09605386]],\n",
      "\n",
      "       [[-0.1279866 , -0.05504436,  0.09185877, ..., -0.00039184,\n",
      "          0.2676438 ,  0.18787068],\n",
      "        [-0.08288949, -0.0040804 ,  0.07198349, ...,  0.14991464,\n",
      "          0.21952172, -0.03550014],\n",
      "        [-0.12505178,  0.20642443,  0.09906775, ..., -0.25230387,\n",
      "          0.19945389,  0.02416509],\n",
      "        ...,\n",
      "        [-0.07731782,  0.23997685, -0.0732178 , ...,  0.21879692,\n",
      "          0.17964989,  0.08158271],\n",
      "        [-0.07731782,  0.23997685, -0.0732178 , ...,  0.21879692,\n",
      "          0.17964989,  0.08158271],\n",
      "        [-0.07731782,  0.23997685, -0.0732178 , ...,  0.21879692,\n",
      "          0.17964989,  0.08158271]]], dtype=float32)), label_ids=array([1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 0,\n",
      "       1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 1, 2, 0,\n",
      "       1, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
      "       2, 1, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2,\n",
      "       1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0]), metrics={'test_loss': 1.6600600481033325, 'test_accuracy': 0.6495726495726496, 'test_precision': 0.5904897571564238, 'test_recall': 0.6495726495726496, 'test_f1': 0.5893628822832363, 'test_runtime': 7.8293, 'test_samples_per_second': 29.888, 'test_steps_per_second': 3.832})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd75bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n",
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "766c4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.2.4_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3e1cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afd884e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           32\n",
       "Fitness                  14\n",
       "Bone health              14\n",
       "Cancer                   12\n",
       "Diabetes                 10\n",
       "Throat                    9\n",
       "Cardiovascular Health     8\n",
       "Hair                      7\n",
       "Eye                       7\n",
       "Neurological health       7\n",
       "Skin                      6\n",
       "Ear                       6\n",
       "Women' s Health           5\n",
       "COVID                     4\n",
       "Mental Health             3\n",
       "Men's health              3\n",
       "Blood                     3\n",
       "Muscles                   1\n",
       "Vascular                  1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6477f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           19\n",
       "Skin                     18\n",
       "Bone health               7\n",
       "Blood                     6\n",
       "Muscles                   5\n",
       "Hair                      5\n",
       "Cardiovascular Health     4\n",
       "Men's health              3\n",
       "Dental Health             3\n",
       "Diabetes                  2\n",
       "Eye                       2\n",
       "Vascular                  2\n",
       "COVID                     2\n",
       "Neurological health       2\n",
       "Fitness                   1\n",
       "Women' s Health           1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
