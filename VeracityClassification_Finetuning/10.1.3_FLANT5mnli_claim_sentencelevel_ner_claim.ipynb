{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0a075649ed143edc\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-0a075649ed143edc/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 366.99it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv',data_files='dataset_sentenceattribution_nerfeatures_split.csv',delimiter=',',column_names=[\"claim\",\"premise\",\"label\",\"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\",\"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\",\"url\", \"entities\",\"entity_map\",\"gem_exp\",\"gem_label\",\"gpt_label\",\"gpt_exp\",\"gold_exp\",\"entity_map_ev\",\"entity_ev\",\"split\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133f9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-0a075649ed143edc/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-448ce854513aff4b.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-0a075649ed143edc/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-439001714e68e972.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-0a075649ed143edc/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-6290d99fae9b056f.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01252e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\", \"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-b638575a29a2a369.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-c40e649fdc04fbe6.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-b1db050fd6262d30.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 2, 'entailment': 0, 'neutral': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15010ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "        claim = item['claim'] \n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        additional_features = [\n",
    "            \"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "    \n",
    "        for feature in additional_features:\n",
    "            if feature in item:\n",
    "                claim += \"[SEP]\" + str(item[feature])\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences,claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "         # Construct the output item\n",
    "        output_item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),  # Remove batch dimension\n",
    "            'claim': claim,  # Include augmented claim text\n",
    "            'evidences': evidences  # Include original evidence text\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            output_item['label'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return output_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5cf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 5433,     6,  6124,   103,  1572,  1171,  1717,  2537,  9753,  3217,\n",
       "            24,     8,     3,  6296,  2917,    75,  5167,    11,    82,    52,\n",
       "            52,   107,  1832,  1043, 12771,   228,  2519,     8, 23458,    13,\n",
       "           212, 12851,   725,    45,  1133,  9241,   588,    51,   159,    12,\n",
       "            74,    51,   159,    57,  3094,  1133,  1717,  2537,     5,     1,\n",
       "           499,    52,    52,   107,  1832,  1043,    19,  1664,   261,    16,\n",
       "         26309,   494,    12,   199,  1172,     8,  3179,    13,     8,  1133,\n",
       "             5,  6306,   134,  8569,   908,   632,  6306,   134,  8569,   908,\n",
       "           632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,   908,\n",
       "           632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,   908,\n",
       "           632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,   908,\n",
       "           632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,   908,\n",
       "           632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,   908,\n",
       "           632,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'claim': 'Myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0',\n",
       " 'evidences': 'Additionally, laser doppler blood flow measurement showed that the frankincense and myrrh essential oil compound could promote the elimination of capillaries from skin epidermis to dermis by increasing skin blood flow.',\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 41:37, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>1.073729</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.640511</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.562778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>1.218670</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.657164</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.633390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>1.405876</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.652112</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.636642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>1.806652</td>\n",
       "      <td>0.636559</td>\n",
       "      <td>0.653900</td>\n",
       "      <td>0.636559</td>\n",
       "      <td>0.643595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>1.941594</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.646303</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.631578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>2.585385</td>\n",
       "      <td>0.662366</td>\n",
       "      <td>0.663937</td>\n",
       "      <td>0.662366</td>\n",
       "      <td>0.663116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>2.618884</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.649345</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.644618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>2.703184</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.656138</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.654403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>2.914392</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.651615</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.644924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>3.060194</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.647638</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.643593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.306949</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.638920</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.639768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.170617</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.638049</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.635978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>3.379047</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.640241</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.641281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.454317</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.640796</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.640717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.465007</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.645836</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.644348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-203/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-3045] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-203] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-1015/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-812] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-1421/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.3_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.1.3_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.3_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.1.3_flant5/checkpoint-1218 (score: 0.6623655913978495).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 14:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.1.3_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.1.3_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.1.3_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.1.3_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.1.3_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.1.3_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.1.3_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.1.3_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.1.3_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.1.3_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.1.3_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.1.3_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.1.3_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.1.3_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.1.3_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.1.3_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.1.3_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.1.3_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-2.328543  ,  8.076148  , -5.2518816 ],\n",
      "       [ 7.3858523 , -2.0310957 , -4.12126   ],\n",
      "       [ 6.6720023 , -2.1127446 , -3.4866529 ],\n",
      "       [-2.9567912 , -2.0963333 ,  4.5888467 ],\n",
      "       [ 6.3452563 , -2.6649902 , -2.714566  ],\n",
      "       [ 7.705632  , -2.1723049 , -4.245939  ],\n",
      "       [ 5.550607  , -1.7924566 , -2.8793795 ],\n",
      "       [ 7.8499603 , -2.658928  , -3.868109  ],\n",
      "       [ 7.8620496 , -2.569414  , -4.045555  ],\n",
      "       [-1.1250873 , -3.9609318 ,  4.8864317 ],\n",
      "       [ 7.2833805 , -2.7848308 , -3.3884718 ],\n",
      "       [ 7.8818684 , -2.7022886 , -3.841032  ],\n",
      "       [-4.417572  ,  6.0192795 , -2.1287441 ],\n",
      "       [ 7.8589244 , -2.4616768 , -4.051708  ],\n",
      "       [ 6.4330983 , -1.8808599 , -3.3916771 ],\n",
      "       [-3.773398  , -0.50689125,  3.7947004 ],\n",
      "       [ 7.6178603 , -2.4674802 , -3.9064453 ],\n",
      "       [ 7.038708  , -2.8982158 , -3.0352445 ],\n",
      "       [ 7.936483  , -2.7645056 , -3.8910325 ],\n",
      "       [ 6.8663926 , -1.2705654 , -4.450298  ],\n",
      "       [ 0.11994997,  0.5980051 , -0.7401458 ],\n",
      "       [ 6.0793586 , -2.4183033 , -2.8855765 ],\n",
      "       [-4.2644906 ,  8.356482  , -4.2436376 ],\n",
      "       [-2.9492912 ,  3.2130818 , -0.61163324],\n",
      "       [ 0.4522024 , -2.2953289 ,  1.7575336 ],\n",
      "       [-4.48509   ,  4.967387  , -1.0887017 ],\n",
      "       [ 7.860828  , -1.9958526 , -4.493307  ],\n",
      "       [ 4.889224  , -2.3390357 , -1.8778061 ],\n",
      "       [ 7.7395463 , -2.522466  , -3.97889   ],\n",
      "       [-0.48581508, -2.1541607 ,  2.4887605 ],\n",
      "       [ 0.7751279 ,  3.785118  , -4.313847  ],\n",
      "       [ 7.8683352 , -2.379577  , -4.105913  ],\n",
      "       [ 7.6061234 , -2.4792426 , -3.9078774 ],\n",
      "       [ 1.6182948 , -4.7615423 ,  3.3372445 ],\n",
      "       [-2.7723835 ,  7.445527  , -4.7041354 ],\n",
      "       [ 7.167752  , -1.5632951 , -4.458166  ],\n",
      "       [ 2.070999  ,  2.164937  , -3.8874373 ],\n",
      "       [ 7.389608  , -1.9456301 , -4.195364  ],\n",
      "       [-3.9677625 , -3.133115  ,  6.4443593 ],\n",
      "       [ 7.5622287 , -2.450683  , -3.8511014 ],\n",
      "       [-3.6045907 ,  8.040155  , -4.4955144 ],\n",
      "       [ 7.4850354 , -2.5047464 , -3.7605124 ],\n",
      "       [ 7.160557  , -2.6299293 , -3.3303468 ],\n",
      "       [-3.3165996 ,  4.1612496 , -1.3266449 ],\n",
      "       [-3.321313  ,  8.198221  , -4.9935846 ],\n",
      "       [ 5.8840275 , -0.15139502, -4.6964498 ],\n",
      "       [-2.9449701 ,  7.977474  , -5.0980735 ],\n",
      "       [ 7.1077685 , -2.2374747 , -3.7749977 ],\n",
      "       [ 6.877833  , -2.0182228 , -3.6749182 ],\n",
      "       [-1.3141631 , -1.937956  ,  3.0500302 ],\n",
      "       [ 5.395014  , -0.90250653, -3.6828463 ],\n",
      "       [ 1.8253404 , -0.6214986 , -0.990179  ],\n",
      "       [-3.737597  ,  8.1715975 , -4.6668124 ],\n",
      "       [-3.984284  ,  5.140788  , -1.5787814 ],\n",
      "       [-2.601955  ,  4.3008676 , -2.0068254 ],\n",
      "       [ 7.797169  , -2.4665246 , -4.011257  ],\n",
      "       [-4.3181887 , -0.61012256,  4.207239  ],\n",
      "       [ 6.655646  , -1.6042373 , -3.9817135 ],\n",
      "       [ 5.7681823 , -2.948674  , -1.877897  ],\n",
      "       [ 7.660999  , -2.4198802 , -3.9165525 ],\n",
      "       [ 7.39113   , -2.7982829 , -3.4014964 ],\n",
      "       [-1.9911631 ,  0.95130503,  0.8296681 ],\n",
      "       [ 2.5611076 ,  4.037748  , -5.8806133 ],\n",
      "       [-3.8405511 ,  7.2111106 , -3.8132899 ],\n",
      "       [ 5.3323603 , -1.5392023 , -3.062917  ],\n",
      "       [ 7.6702976 , -2.3035707 , -4.027906  ],\n",
      "       [-1.6004653 ,  7.7396317 , -5.7306604 ],\n",
      "       [ 7.991416  , -2.346105  , -4.280846  ],\n",
      "       [ 7.719373  , -2.8995817 , -3.5394576 ],\n",
      "       [-2.2166715 , -3.3385284 ,  5.2073264 ],\n",
      "       [ 7.7013435 , -2.3398824 , -3.9512343 ],\n",
      "       [-3.030272  ,  3.2548473 , -0.6074236 ],\n",
      "       [ 6.7424865 , -2.1873786 , -3.510614  ],\n",
      "       [-4.3199077 ,  6.955477  , -3.0006866 ],\n",
      "       [ 3.9246862 , -0.5962687 , -2.6591563 ],\n",
      "       [ 6.636527  ,  2.8298607 , -7.9968305 ],\n",
      "       [ 6.288184  , -2.5516071 , -2.6462138 ],\n",
      "       [ 7.35828   , -2.4445243 , -3.6461563 ],\n",
      "       [ 8.082266  , -2.3014755 , -4.3875103 ],\n",
      "       [ 7.281164  , -2.1047547 , -3.845818  ],\n",
      "       [ 7.920342  , -2.8882303 , -3.7662358 ],\n",
      "       [ 6.1093874 , -2.2087042 , -2.9377792 ],\n",
      "       [ 7.5719476 , -2.3931963 , -3.8556778 ],\n",
      "       [ 7.7838087 , -2.3256805 , -4.19074   ],\n",
      "       [ 7.541812  , -1.5877762 , -4.832542  ],\n",
      "       [ 0.02561117,  4.3818617 , -4.237361  ],\n",
      "       [ 6.840094  , -2.2609022 , -3.4578636 ],\n",
      "       [-3.5818846 ,  7.4200273 , -4.186049  ],\n",
      "       [ 6.887873  , -2.1348066 , -3.636236  ],\n",
      "       [-0.4389488 ,  3.1520836 , -2.734649  ],\n",
      "       [ 7.208257  , -2.8863184 , -3.1693468 ],\n",
      "       [ 6.804606  , -2.1521568 , -3.5811865 ],\n",
      "       [-1.6950305 , -1.986504  ,  3.416642  ],\n",
      "       [ 6.7979455 , -2.0319176 , -3.671634  ],\n",
      "       [ 3.2908847 ,  3.8140743 , -6.208802  ],\n",
      "       [ 6.8251734 , -2.3634284 , -3.2770264 ],\n",
      "       [-1.5532556 ,  5.7420073 , -4.004506  ],\n",
      "       [ 5.9840274 ,  2.5089977 , -7.06016   ],\n",
      "       [ 7.730532  , -2.0992706 , -4.2968636 ],\n",
      "       [ 7.7795534 , -2.5279396 , -3.9143875 ],\n",
      "       [-3.3319173 , -3.9166903 ,  6.724194  ],\n",
      "       [ 7.6348186 , -2.1935272 , -4.204353  ],\n",
      "       [ 7.5773563 , -2.7459102 , -3.6341982 ],\n",
      "       [ 6.7964716 , -2.0580761 , -3.6769695 ],\n",
      "       [ 7.757604  , -1.8506896 , -4.628325  ],\n",
      "       [ 6.03697   ,  0.33247426, -5.613914  ],\n",
      "       [-3.6614242 ,  7.7816925 , -4.415475  ],\n",
      "       [-4.6704893 ,  6.5436063 , -2.3726556 ],\n",
      "       [ 0.11221144,  5.7674894 , -5.561742  ],\n",
      "       [ 1.314347  ,  2.4534168 , -3.434578  ],\n",
      "       [-2.8558407 ,  6.0446606 , -3.5422165 ],\n",
      "       [ 8.079377  , -2.6502879 , -4.074379  ],\n",
      "       [ 7.509523  , -2.7121048 , -3.6114173 ],\n",
      "       [ 6.2196975 , -1.325391  , -3.8976126 ],\n",
      "       [ 7.865683  , -2.2732673 , -4.271991  ],\n",
      "       [ 7.4296527 , -1.7951947 , -4.4619517 ],\n",
      "       [ 8.073659  , -2.3870494 , -4.308194  ],\n",
      "       [ 7.4498057 , -2.2939763 , -3.8401158 ],\n",
      "       [ 7.7469068 , -2.6266716 , -3.785262  ],\n",
      "       [ 7.4119916 , -2.0798647 , -3.9882329 ],\n",
      "       [-2.188366  ,  7.122774  , -4.867179  ],\n",
      "       [-1.8933821 ,  3.5779283 , -1.9330084 ],\n",
      "       [ 6.709876  , -0.98647594, -4.6818724 ],\n",
      "       [-2.4706354 ,  7.9622493 , -5.3096356 ],\n",
      "       [-4.2851095 ,  7.223337  , -3.345324  ],\n",
      "       [ 7.305278  , -2.1588144 , -3.993275  ],\n",
      "       [ 1.040761  ,  3.519142  , -4.2984185 ],\n",
      "       [ 7.5470295 , -2.4514685 , -3.8536081 ],\n",
      "       [ 7.986337  , -2.7130005 , -3.943129  ],\n",
      "       [ 6.995854  , -3.03999   , -2.8938856 ],\n",
      "       [ 3.6063108 , -0.86985564, -2.2136588 ],\n",
      "       [-4.347257  ,  5.826229  , -1.9404274 ],\n",
      "       [ 7.119143  , -0.76642025, -5.090316  ],\n",
      "       [-4.0363846 ,  7.004614  , -3.3033667 ],\n",
      "       [ 0.05936014,  3.4744587 , -3.4011269 ],\n",
      "       [ 0.5793796 ,  5.7509837 , -6.064895  ],\n",
      "       [ 7.5052257 , -2.6794252 , -3.6117084 ],\n",
      "       [-3.3506227 ,  6.0100384 , -2.9768872 ],\n",
      "       [-3.2368193 ,  6.1560125 , -3.160522  ],\n",
      "       [ 7.5256543 , -2.5293229 , -3.8491888 ],\n",
      "       [ 6.6595445 , -1.4078169 , -4.029295  ],\n",
      "       [ 4.999294  ,  0.04137858, -4.2966595 ],\n",
      "       [ 4.0360017 , -0.3696258 , -3.1673245 ],\n",
      "       [ 7.929215  , -2.241143  , -4.3396635 ],\n",
      "       [ 6.4321256 , -2.2838097 , -3.2010906 ],\n",
      "       [ 6.4982333 , -1.3937657 , -4.1957808 ],\n",
      "       [ 7.3994207 , -1.1480366 , -4.9491916 ],\n",
      "       [-4.079322  ,  3.0667865 ,  0.4902383 ],\n",
      "       [ 6.5076    , -1.6300384 , -3.803061  ],\n",
      "       [-3.9009564 ,  6.8935304 , -3.3694882 ],\n",
      "       [-4.71122   ,  7.4395704 , -3.2220528 ],\n",
      "       [ 6.981154  , -2.5472536 , -3.3391771 ],\n",
      "       [ 7.8968143 , -2.4080222 , -4.270312  ],\n",
      "       [ 7.7898126 , -2.342779  , -4.092547  ],\n",
      "       [ 7.4468045 , -2.0070617 , -4.21194   ],\n",
      "       [ 7.265838  , -2.881131  , -3.291058  ],\n",
      "       [ 6.7480373 , -2.4282594 , -3.250159  ],\n",
      "       [ 2.749848  ,  4.651874  , -6.707294  ],\n",
      "       [ 6.8629036 , -2.1443417 , -3.5417922 ],\n",
      "       [-4.5729456 ,  4.445427  , -0.56042206],\n",
      "       [-4.3752427 ,  2.3133883 ,  1.4743006 ],\n",
      "       [-3.856926  , -4.249748  ,  7.6266055 ],\n",
      "       [ 7.7398987 , -2.7189531 , -3.7794144 ],\n",
      "       [-4.179049  , -3.3951714 ,  6.8568244 ],\n",
      "       [ 2.2547956 , -2.6695216 ,  0.69359356],\n",
      "       [ 7.2169714 , -1.9020921 , -4.0883975 ],\n",
      "       [ 7.6266975 , -2.319246  , -4.020637  ],\n",
      "       [-3.6665769 ,  7.8681374 , -4.1929073 ],\n",
      "       [-4.88812   , -2.9990616 ,  7.2149725 ],\n",
      "       [ 0.37377352, -3.412441  ,  2.910099  ],\n",
      "       [-4.621297  ,  4.9174557 , -0.943985  ],\n",
      "       [ 5.6848607 , -2.0448804 , -2.8782642 ],\n",
      "       [-4.325366  ,  7.6751904 , -3.6449695 ],\n",
      "       [ 7.978411  , -2.4206195 , -4.2346697 ],\n",
      "       [-0.5368864 ,  5.6878757 , -4.985571  ],\n",
      "       [-3.4736454 ,  6.9968066 , -3.8021483 ],\n",
      "       [ 7.034248  , -2.474791  , -3.5312765 ],\n",
      "       [-1.4975564 , -2.4173572 ,  3.6385386 ],\n",
      "       [ 7.140188  , -2.1730576 , -3.8870482 ],\n",
      "       [-1.6081221 , -2.0443165 ,  3.2702482 ],\n",
      "       [-2.2478225 ,  3.8095765 , -1.7273136 ],\n",
      "       [ 3.8480642 , -1.338291  , -2.0884337 ],\n",
      "       [-3.4529536 ,  1.8990806 ,  1.0860714 ],\n",
      "       [-2.4102914 ,  8.550469  , -5.6166887 ],\n",
      "       [-3.5260673 ,  8.249847  , -4.6465974 ],\n",
      "       [ 7.6295223 , -2.6470075 , -3.7274723 ],\n",
      "       [ 6.428718  ,  0.25594613, -5.5393744 ],\n",
      "       [ 7.4659343 ,  0.33672914, -6.463822  ],\n",
      "       [-3.0123024 ,  8.096268  , -5.1623573 ],\n",
      "       [ 7.0762625 , -2.2118082 , -3.6260831 ],\n",
      "       [-3.7068067 ,  7.452506  , -3.8131328 ],\n",
      "       [-4.2012787 ,  8.620878  , -4.487229  ],\n",
      "       [ 5.210128  , -0.88782823, -3.3842587 ],\n",
      "       [ 6.205095  , -1.4129349 , -4.0001683 ],\n",
      "       [-3.464838  , -2.3797529 ,  5.219112  ],\n",
      "       [ 7.905136  , -2.4396012 , -4.1138186 ],\n",
      "       [ 7.533029  , -1.8725313 , -4.536282  ],\n",
      "       [ 7.7516623 , -2.662695  , -3.852243  ],\n",
      "       [ 7.380382  , -2.0536463 , -4.149754  ],\n",
      "       [ 0.94178075,  0.87776685, -1.6663452 ],\n",
      "       [ 7.3897424 , -2.5093799 , -3.5932684 ],\n",
      "       [ 6.8998213 , -0.91351575, -4.787763  ],\n",
      "       [-1.8392894 ,  8.3210945 , -5.7145963 ],\n",
      "       [ 7.8103137 , -2.1794991 , -4.374912  ],\n",
      "       [-3.6646295 ,  7.873599  , -4.3581805 ],\n",
      "       [-3.3833656 ,  7.2745543 , -4.019243  ],\n",
      "       [ 7.495783  , -2.0017545 , -4.2075825 ],\n",
      "       [ 4.1465964 , -0.57900614, -3.1441643 ],\n",
      "       [ 7.8229966 , -2.3568065 , -4.225346  ],\n",
      "       [-2.3872364 ,  7.9946733 , -5.3213196 ],\n",
      "       [ 1.4873743 ,  4.5208535 , -5.6449294 ],\n",
      "       [ 1.3485727 , -1.8817071 ,  0.67440665],\n",
      "       [ 7.7882295 , -2.8993866 , -3.5947442 ],\n",
      "       [-0.5873684 ,  7.800626  , -6.564241  ],\n",
      "       [ 7.7111316 , -2.0255966 , -4.547431  ],\n",
      "       [-2.2881296 ,  5.8490715 , -3.601336  ],\n",
      "       [-4.199356  ,  5.839885  , -2.065854  ],\n",
      "       [-2.7329009 , -4.1928005 ,  6.364419  ],\n",
      "       [ 8.019868  , -2.4821126 , -4.156026  ],\n",
      "       [-2.6114812 , -2.2282922 ,  4.335789  ],\n",
      "       [ 7.8934608 , -2.4928508 , -4.0790987 ],\n",
      "       [ 7.0985475 , -0.49604258, -5.286782  ],\n",
      "       [-0.65803534,  2.3383422 , -1.7025493 ],\n",
      "       [ 6.8722243 , -1.7014138 , -4.0834866 ],\n",
      "       [ 7.1916766 , -2.2527668 , -3.8254762 ],\n",
      "       [ 7.68231   , -2.0267987 , -4.305501  ],\n",
      "       [ 7.999829  , -2.6514614 , -4.0815287 ],\n",
      "       [-3.435143  ,  6.1492863 , -2.9638453 ],\n",
      "       [-1.1265429 ,  2.1666715 , -1.1726393 ],\n",
      "       [ 7.439613  , -2.328795  , -3.8004146 ],\n",
      "       [ 7.011694  , -2.185989  , -3.7733357 ],\n",
      "       [ 6.584388  , -1.4656408 , -4.1422124 ],\n",
      "       [ 6.89393   , -1.6025555 , -4.255268  ],\n",
      "       [ 0.77495325, -1.4519876 ,  0.6453576 ]], dtype=float32), array([[[-0.14669292,  0.01646642,  0.01229651, ...,  0.03966467,\n",
      "          0.04336039,  0.05867287],\n",
      "        [-0.0048346 ,  0.08091938, -0.20281796, ..., -0.00726285,\n",
      "          0.05080399, -0.04205454],\n",
      "        [ 0.04364139, -0.00670911, -0.04675738, ..., -0.08924413,\n",
      "         -0.03302529,  0.04467475],\n",
      "        ...,\n",
      "        [ 0.10158052,  0.138285  , -0.11203545, ...,  0.24427603,\n",
      "         -0.18295479,  0.04303718],\n",
      "        [ 0.10158052,  0.138285  , -0.11203545, ...,  0.24427603,\n",
      "         -0.18295479,  0.04303718],\n",
      "        [ 0.10158052,  0.138285  , -0.11203545, ...,  0.24427603,\n",
      "         -0.18295479,  0.04303718]],\n",
      "\n",
      "       [[-0.00835773,  0.0194542 , -0.11089128, ..., -0.10767957,\n",
      "          0.03913358, -0.05962249],\n",
      "        [ 0.13632648, -0.16415793, -0.16394718, ..., -0.19893336,\n",
      "         -0.00757814,  0.04729588],\n",
      "        [ 0.10381808, -0.104412  , -0.16080976, ...,  0.01031663,\n",
      "          0.05566747, -0.03343856],\n",
      "        ...,\n",
      "        [-0.01680968,  0.02790547, -0.15697561, ...,  0.10058288,\n",
      "         -0.07602499, -0.10747552],\n",
      "        [-0.01680968,  0.02790547, -0.15697561, ...,  0.10058288,\n",
      "         -0.07602499, -0.10747552],\n",
      "        [-0.01680968,  0.02790547, -0.15697561, ...,  0.10058288,\n",
      "         -0.07602499, -0.10747552]],\n",
      "\n",
      "       [[-0.12487734,  0.02344465, -0.12822624, ...,  0.06876904,\n",
      "          0.08444252, -0.08440086],\n",
      "        [ 0.03670115,  0.07835336, -0.1036841 , ...,  0.13789167,\n",
      "          0.0592483 , -0.04304598],\n",
      "        [-0.11512495,  0.0097402 , -0.11898703, ..., -0.01510474,\n",
      "          0.00798764, -0.08601376],\n",
      "        ...,\n",
      "        [ 0.01687297,  0.15348393, -0.12092312, ...,  0.0385163 ,\n",
      "          0.13942756, -0.0566868 ],\n",
      "        [ 0.01687297,  0.15348393, -0.12092312, ...,  0.0385163 ,\n",
      "          0.13942756, -0.0566868 ],\n",
      "        [ 0.01687297,  0.15348393, -0.12092312, ...,  0.0385163 ,\n",
      "          0.13942756, -0.0566868 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.00059512,  0.00594485, -0.17085606, ..., -0.1754735 ,\n",
      "          0.13638812,  0.0046435 ],\n",
      "        [ 0.0654937 , -0.08108498, -0.00584091, ...,  0.02501701,\n",
      "          0.01702359,  0.19412759],\n",
      "        [ 0.18283062, -0.26596218,  0.02511209, ..., -0.1190465 ,\n",
      "         -0.15660848,  0.10667979],\n",
      "        ...,\n",
      "        [ 0.11719593,  0.01259995, -0.01641316, ...,  0.27124816,\n",
      "         -0.24297838,  0.0104319 ],\n",
      "        [ 0.11719593,  0.01259995, -0.01641316, ...,  0.27124816,\n",
      "         -0.24297838,  0.0104319 ],\n",
      "        [ 0.11719593,  0.01259995, -0.01641316, ...,  0.27124816,\n",
      "         -0.24297838,  0.0104319 ]],\n",
      "\n",
      "       [[-0.08790433,  0.01937573, -0.10594236, ...,  0.1122613 ,\n",
      "         -0.12864208, -0.10059655],\n",
      "        [ 0.02998326, -0.00805368, -0.03946203, ...,  0.06616308,\n",
      "         -0.00846762,  0.13693644],\n",
      "        [ 0.04267754, -0.04246489, -0.03546405, ...,  0.05101369,\n",
      "          0.09206738,  0.00242575],\n",
      "        ...,\n",
      "        [ 0.3225933 ,  0.02396434,  0.01618661, ...,  0.20626031,\n",
      "         -0.24393098, -0.1023806 ],\n",
      "        [ 0.3225933 ,  0.02396434,  0.01618661, ...,  0.20626031,\n",
      "         -0.24393098, -0.1023806 ],\n",
      "        [ 0.3225933 ,  0.02396434,  0.01618661, ...,  0.20626031,\n",
      "         -0.24393098, -0.1023806 ]],\n",
      "\n",
      "       [[-0.06677054, -0.19490623, -0.16258681, ..., -0.02703558,\n",
      "          0.18315957, -0.3820922 ],\n",
      "        [ 0.0155537 , -0.00587842, -0.168331  , ..., -0.0199865 ,\n",
      "          0.08113698,  0.00740434],\n",
      "        [-0.05628609, -0.05300026, -0.10463937, ..., -0.13609362,\n",
      "          0.14600772, -0.07261202],\n",
      "        ...,\n",
      "        [-0.10089979, -0.02849394, -0.20464776, ..., -0.02816498,\n",
      "         -0.14864956, -0.13365464],\n",
      "        [-0.10089979, -0.02849394, -0.20464776, ..., -0.02816498,\n",
      "         -0.14864956, -0.13365464],\n",
      "        [-0.10089979, -0.02849394, -0.20464776, ..., -0.02816498,\n",
      "         -0.14864956, -0.13365464]]], dtype=float32)), label_ids=array([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 0,\n",
      "       1, 0, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2]), metrics={'test_loss': 2.915266275405884, 'test_accuracy': 0.6282051282051282, 'test_precision': 0.6222949627204947, 'test_recall': 0.6282051282051282, 'test_f1': 0.6208344125010792, 'test_runtime': 7.6528, 'test_samples_per_second': 30.577, 'test_steps_per_second': 3.92})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "808dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48fb2163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed6cfde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")  # Adjust xticklabels and yticklabels as needed\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "346c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.1.3_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4543aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad099d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           38\n",
       "Bone health              14\n",
       "Cancer                   10\n",
       "Skin                     10\n",
       "Diabetes                  8\n",
       "Fitness                   8\n",
       "Hair                      7\n",
       "Neurological health       7\n",
       "Eye                       7\n",
       "Throat                    7\n",
       "Ear                       6\n",
       "COVID                     4\n",
       "Blood                     4\n",
       "Cardiovascular Health     4\n",
       "Muscles                   4\n",
       "Women' s Health           3\n",
       "Mental Health             2\n",
       "Vascular                  2\n",
       "Men's health              2\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d45817c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skin                     14\n",
       "General Health           13\n",
       "Cardiovascular Health     8\n",
       "Fitness                   7\n",
       "Bone health               7\n",
       "Hair                      5\n",
       "Blood                     5\n",
       "Diabetes                  4\n",
       "Men's health              4\n",
       "Dental Health             3\n",
       "Women' s Health           3\n",
       "Neurological health       2\n",
       "Cancer                    2\n",
       "COVID                     2\n",
       "Muscles                   2\n",
       "Throat                    2\n",
       "Eye                       2\n",
       "Vascular                  1\n",
       "Mental Health             1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af780f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
