{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-927ab0163adb9fdb\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 214.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv',data_files='dataset_propositionattribution_nerfeatures.csv',delimiter=',',column_names=[\"claim\",\"premise\",\"label\",\"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\",\"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\",\"url\", \"entities\",\"entity_map\",\"gem_exp\",\"gem_label\",\"gpt_label\",\"gpt_exp\",\"gold_exp\",\"entity_map_ev\",\"entity_ev\",\"split\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133f9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d013d5114fa105ab.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-16f9acbdd82fea07.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-927ab0163adb9fdb/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-47c7de469e4087a2.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01252e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\", \"category\",\"count_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:00<00:00, 6523.38ex/s]\n",
      "100%|██████████| 465/465 [00:00<00:00, 6396.69ex/s]\n",
      "100%|██████████| 234/234 [00:00<00:00, 5764.05ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 2, 'entailment': 0, 'neutral': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15010ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['counte_dis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "        claim = item['claim'] \n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        additional_features = [\n",
    "            \"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "    \n",
    "        for feature in additional_features:\n",
    "            if feature in item:\n",
    "                evidences += \"[SEP]\" + str(item[feature])\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences, claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "         # Construct the output item\n",
    "        output_item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),  # Remove batch dimension\n",
    "            'claim': claim,  # Include augmented claim text\n",
    "            'evidences': evidences  # Include original evidence text\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            output_item['label'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return output_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category', 'count_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5cf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   37,  2942,    13, 25567,  1874,     7,    33,  4313,   778,   116,\n",
       "            79,    54,    36,  4260,    28,  9109,   757,  9508,     5,  6306,\n",
       "           134,  8569,   908,   632,  6306,   134,  8569,   908,   536,  6306,\n",
       "           134,  8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,\n",
       "           134,  8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,\n",
       "           134,  8569,   908,   632,  6306,   134,  8569,   908,   536,  6306,\n",
       "           134,  8569,   908,   632,  6306,   134,  8569,   908,   632,  6306,\n",
       "           134,  8569,   908,   632,     1,   749,  5540,  1874,    54,    36,\n",
       "          4260,    28,  3730,     6, 11423,  3918,     6, 11932,  3918,     6,\n",
       "            11, 26324,     5,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'claim': 'Prostate cancer can be treated with surgery, radiation therapy, hormone therapy, and chemotherapy.',\n",
       " 'evidences': 'The majority of prostate cancers are identified early when they can be treated with curative intent.[SEP]0[SEP]1[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]1[SEP]0[SEP]0[SEP]0',\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 41:33, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.949164</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.688141</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.681177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>1.633018</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.717555</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>1.972440</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.715066</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.677165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>2.824972</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.718393</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.679069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>2.962908</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.697081</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.672592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>2.932268</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.698133</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.670085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>3.129114</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.713213</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.696146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.591147</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.704797</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.681186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.654133</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.701581</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.684746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.625828</td>\n",
       "      <td>0.668817</td>\n",
       "      <td>0.704585</td>\n",
       "      <td>0.668817</td>\n",
       "      <td>0.682311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.669946</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.709669</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.685772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.643206</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.702775</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.686379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>3.655682</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.715725</td>\n",
       "      <td>0.679570</td>\n",
       "      <td>0.693344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.682002</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.706643</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.686349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.696915</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.711298</td>\n",
       "      <td>0.675269</td>\n",
       "      <td>0.689065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.4.4_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.4.4_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.4.4_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.4.4_flant5/checkpoint-203 (score: 0.9491642713546753).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.4.4_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.4.4_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.4.4_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.4.4_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.4.4_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.4.4_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.4.4_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.4.4_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.4.4_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.4.4_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.4.4_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.4.4_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.4.4_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.4.4_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.4.4_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.4.4_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.4.4_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.4.4_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[ 1.82380438e-01,  2.19237924e-01, -4.93456930e-01],\n",
      "       [ 1.57344878e+00, -2.21707076e-01, -1.17678535e+00],\n",
      "       [ 1.78822601e+00, -5.93788207e-01, -9.88538384e-01],\n",
      "       [-7.22612739e-01, -1.85643101e+00,  2.19228482e+00],\n",
      "       [ 1.34679213e-01,  9.55292523e-01, -1.19357371e+00],\n",
      "       [ 3.18843317e+00, -3.75741720e-01, -2.14226151e+00],\n",
      "       [ 2.18622184e+00, -9.79754686e-01, -8.90202224e-01],\n",
      "       [ 2.72610545e+00, -3.93217474e-01, -1.88445246e+00],\n",
      "       [ 1.33498323e+00,  1.56132311e-01, -1.39680994e+00],\n",
      "       [ 1.85499775e+00, -2.69828409e-01, -1.30239809e+00],\n",
      "       [ 2.21858764e+00, -1.13931704e+00, -6.09766841e-01],\n",
      "       [ 2.75477457e+00, -4.70849127e-01, -1.78603935e+00],\n",
      "       [ 5.40732026e-01,  3.86230230e-01, -9.68296647e-01],\n",
      "       [ 2.99166131e+00, -4.47955608e-01, -2.01673865e+00],\n",
      "       [ 2.26809525e+00, -2.55450517e-01, -1.57084489e+00],\n",
      "       [-6.15752578e-01, -4.99445498e-01,  8.64118576e-01],\n",
      "       [ 1.93438160e+00,  1.89403761e-02, -1.62122357e+00],\n",
      "       [ 2.24043727e+00, -6.44769609e-01, -1.17508101e+00],\n",
      "       [ 2.61653423e+00, -5.66763222e-01, -1.57104957e+00],\n",
      "       [ 1.93255532e+00,  2.05244869e-01, -1.81557238e+00],\n",
      "       [-1.50266230e+00, -5.23598313e-01,  1.54491830e+00],\n",
      "       [ 1.47010493e+00, -2.62346357e-01, -1.07183897e+00],\n",
      "       [ 2.23097682e+00, -7.59756684e-01, -1.23109639e+00],\n",
      "       [-2.92762071e-01, -1.55571342e+00,  1.50663221e+00],\n",
      "       [ 5.50524652e-01, -1.67227638e+00,  1.08349741e+00],\n",
      "       [-1.29244983e+00, -1.03029716e+00,  1.94031644e+00],\n",
      "       [ 3.14992380e+00, -3.64440322e-01, -2.16643882e+00],\n",
      "       [ 5.75057268e-01,  1.13433075e+00, -1.67103219e+00],\n",
      "       [ 2.66425776e+00, -1.14896059e-01, -1.95985198e+00],\n",
      "       [-2.72429176e-02, -1.38001323e+00,  1.10339642e+00],\n",
      "       [ 2.56081700e-01,  8.19687784e-01, -1.06376243e+00],\n",
      "       [ 2.70953727e+00, -3.56480867e-01, -1.76971626e+00],\n",
      "       [ 2.18155146e+00, -4.78198409e-01, -1.27700508e+00],\n",
      "       [ 1.25582397e+00,  3.02173048e-01, -1.31318617e+00],\n",
      "       [-1.46457314e+00, -8.35352957e-01,  1.79061151e+00],\n",
      "       [ 8.55562806e-01,  1.36040756e-02, -7.05153584e-01],\n",
      "       [ 5.21206617e-01, -5.44365823e-01, -3.93159650e-02],\n",
      "       [ 2.83362794e+00, -3.87581855e-01, -1.94200706e+00],\n",
      "       [-4.24690664e-01, -1.44535124e+00,  1.49377882e+00],\n",
      "       [ 1.36118519e+00,  3.31777185e-01, -1.36903286e+00],\n",
      "       [-7.16202080e-01, -5.97213842e-02,  4.90206689e-01],\n",
      "       [ 1.97818398e+00, -2.41661191e-01, -1.51226330e+00],\n",
      "       [ 2.72088349e-01,  9.18021142e-01, -1.29544723e+00],\n",
      "       [-3.09891701e-01, -1.19254875e+00,  1.29856861e+00],\n",
      "       [-1.66545212e+00, -8.83821905e-01,  2.08839035e+00],\n",
      "       [ 2.44565010e+00, -4.74038213e-01, -1.47784412e+00],\n",
      "       [ 4.77076650e-01, -4.73515809e-01, -1.18506543e-01],\n",
      "       [ 2.79608321e+00, -6.90359950e-01, -1.55774343e+00],\n",
      "       [ 2.95439386e+00, -6.61432505e-01, -1.70253301e+00],\n",
      "       [ 3.06074232e-01, -1.20005858e+00,  7.88021386e-01],\n",
      "       [-1.38083041e+00, -7.05297768e-01,  1.62974989e+00],\n",
      "       [ 2.37442896e-01,  1.11143386e+00, -1.38997054e+00],\n",
      "       [ 1.29560161e+00, -1.06263913e-01, -1.08703637e+00],\n",
      "       [ 1.91174221e+00, -5.09902120e-01, -1.03794634e+00],\n",
      "       [-1.00306682e-01, -3.59288394e-01,  3.02433044e-01],\n",
      "       [ 1.90789413e+00, -3.70161623e-01, -1.28431785e+00],\n",
      "       [-5.72722614e-01,  7.95322835e-01, -4.01751369e-01],\n",
      "       [ 2.30289865e+00, -4.01539475e-01, -1.58045697e+00],\n",
      "       [ 1.23924339e+00,  4.06012535e-02, -1.12899160e+00],\n",
      "       [ 2.33571458e+00, -4.29847687e-01, -1.54600692e+00],\n",
      "       [ 2.38981414e+00, -6.43689930e-01, -1.49278104e+00],\n",
      "       [ 7.44215071e-01, -9.36430633e-01,  1.49354681e-01],\n",
      "       [ 1.90212405e+00, -1.11331329e-01, -1.41849077e+00],\n",
      "       [ 2.52700716e-01,  7.60624588e-01, -1.04624677e+00],\n",
      "       [-1.37013406e-01,  1.65309712e-01, -1.38528466e-01],\n",
      "       [ 2.53363132e+00, -1.79625869e-01, -1.92791188e+00],\n",
      "       [ 3.05925703e+00, -5.41805804e-01, -2.01456881e+00],\n",
      "       [ 2.59358859e+00, -1.26494234e-02, -2.12891912e+00],\n",
      "       [ 2.06940031e+00, -9.52670217e-01, -7.20258474e-01],\n",
      "       [ 4.75867897e-01, -1.54854143e+00,  9.58861232e-01],\n",
      "       [ 2.56951809e+00, -9.49606776e-01, -1.27075577e+00],\n",
      "       [-8.75258446e-02, -8.02650988e-01,  6.72636092e-01],\n",
      "       [ 1.11583972e+00,  1.91774949e-01, -1.10245192e+00],\n",
      "       [ 1.56230772e+00, -3.18620428e-02, -1.34835136e+00],\n",
      "       [ 1.44143498e+00, -2.56026953e-01, -9.14615393e-01],\n",
      "       [ 2.31493545e+00, -1.05789602e+00, -9.55160201e-01],\n",
      "       [ 1.99512565e+00, -5.03707230e-01, -1.14138329e+00],\n",
      "       [ 2.55784249e+00, -5.21383286e-01, -1.68402183e+00],\n",
      "       [ 2.49392128e+00, -1.10809255e+00, -9.94385958e-01],\n",
      "       [ 1.18901360e+00,  1.47454172e-01, -1.09962606e+00],\n",
      "       [ 2.76919103e+00, -4.39239025e-01, -1.81867540e+00],\n",
      "       [ 2.19053817e+00, -2.50931621e-01, -1.53725970e+00],\n",
      "       [ 2.81111896e-01, -1.23982720e-01, -2.02002957e-01],\n",
      "       [ 1.78532970e+00, -1.06568301e+00, -5.14389813e-01],\n",
      "       [ 3.04350567e+00, -5.81567585e-01, -1.92825282e+00],\n",
      "       [ 4.58070040e-01,  4.64175373e-01, -9.02822733e-01],\n",
      "       [ 1.78719223e+00, -7.45562911e-01, -8.58435392e-01],\n",
      "       [ 4.42219853e-01, -4.06356961e-01, -1.39196247e-01],\n",
      "       [ 1.68697357e+00, -6.47149011e-02, -1.46985340e+00],\n",
      "       [ 1.84779346e+00,  4.62751612e-02, -1.51895273e+00],\n",
      "       [ 2.09007430e+00, -7.67531514e-01, -9.82281029e-01],\n",
      "       [ 2.12831521e+00, -8.72905552e-01, -1.11555660e+00],\n",
      "       [ 2.00294569e-01, -1.35026991e+00,  9.43939269e-01],\n",
      "       [ 2.79579711e+00, -7.24513173e-01, -1.53307724e+00],\n",
      "       [ 1.53936315e+00,  1.46333963e-01, -1.39346433e+00],\n",
      "       [-3.88292700e-01,  1.31639767e+00, -1.03233659e+00],\n",
      "       [ 1.69442758e-01, -1.68739653e+00,  1.21979344e+00],\n",
      "       [ 1.78776765e+00, -2.59925723e-01, -1.22486615e+00],\n",
      "       [ 2.31120420e+00, -9.15542185e-01, -1.09145010e+00],\n",
      "       [ 1.82536006e+00, -3.81086379e-01, -1.14781916e+00],\n",
      "       [-7.14256346e-01, -2.00725746e+00,  2.32248211e+00],\n",
      "       [ 2.07133079e+00, -3.77336234e-01, -1.34003639e+00],\n",
      "       [ 3.02619195e+00, -7.95080125e-01, -1.65974164e+00],\n",
      "       [ 8.39605093e-01,  2.64119297e-01, -9.57116306e-01],\n",
      "       [ 2.19661903e+00, -3.66408497e-01, -1.46255934e+00],\n",
      "       [ 2.25159097e+00, -6.06761277e-01, -1.47762764e+00],\n",
      "       [ 1.76192057e+00, -1.33083928e+00, -2.99511760e-01],\n",
      "       [-4.35737252e-01, -9.24422801e-01,  1.11232972e+00],\n",
      "       [ 1.49805295e+00, -5.43745697e-01, -8.92312765e-01],\n",
      "       [ 2.05797887e+00, -5.05564630e-01, -1.27467227e+00],\n",
      "       [ 1.27896905e+00, -1.26159057e-01, -1.02341115e+00],\n",
      "       [ 2.92306590e+00, -3.88651818e-01, -1.96835268e+00],\n",
      "       [ 2.08700728e+00, -4.15347397e-01, -1.35028589e+00],\n",
      "       [ 2.13977599e+00, -6.91546857e-01, -1.09372926e+00],\n",
      "       [ 2.43353963e+00, -2.91554838e-01, -1.68615210e+00],\n",
      "       [ 2.81802154e+00, -6.00774229e-01, -1.76258123e+00],\n",
      "       [ 2.66636634e+00, -7.39607930e-01, -1.46690953e+00],\n",
      "       [ 2.09277606e+00, -3.20009142e-01, -1.52423203e+00],\n",
      "       [ 1.65753663e+00, -6.06022060e-01, -7.01451123e-01],\n",
      "       [ 2.64709687e+00, -5.45932889e-01, -1.63162649e+00],\n",
      "       [ 4.84224200e-01,  1.30664685e-03, -4.30659622e-01],\n",
      "       [ 2.62288117e+00, -6.61004126e-01, -1.56299543e+00],\n",
      "       [ 1.98494565e+00, -1.41014487e-01, -1.54554057e+00],\n",
      "       [-1.23458374e+00,  2.82968700e-01,  5.94953895e-01],\n",
      "       [ 4.51575905e-01, -1.11915981e-02, -4.13599730e-01],\n",
      "       [ 2.11492753e+00, -3.71858716e-01, -1.31209934e+00],\n",
      "       [ 2.18865238e-02, -7.96493769e-01,  6.27296925e-01],\n",
      "       [ 2.65426517e+00, -5.83055675e-01, -1.63916183e+00],\n",
      "       [ 2.58598971e+00, -8.55830193e-01, -1.35900545e+00],\n",
      "       [ 2.24149156e+00, -4.12108630e-01, -1.42842555e+00],\n",
      "       [ 1.34579289e+00, -4.87749010e-01, -7.30265498e-01],\n",
      "       [-1.00089240e+00, -1.84852886e+00,  2.30983210e+00],\n",
      "       [ 2.86915135e+00, -3.76822352e-01, -2.01909757e+00],\n",
      "       [-1.08231294e+00, -4.90610480e-01,  1.13275433e+00],\n",
      "       [ 1.67973280e+00, -6.89896524e-01, -7.55634725e-01],\n",
      "       [-9.56962585e-01, -4.82780427e-01,  1.15169704e+00],\n",
      "       [ 1.43054998e+00, -4.79663372e-01, -7.46473312e-01],\n",
      "       [ 2.30453029e-01, -9.35926318e-01,  6.01835310e-01],\n",
      "       [ 3.64563689e-02, -1.22525252e-01, -2.09883004e-02],\n",
      "       [ 3.00290418e+00, -7.02583313e-01, -1.72318614e+00],\n",
      "       [ 2.48025417e+00, -4.06726450e-01, -1.60012972e+00],\n",
      "       [ 8.13464999e-01, -7.74493992e-01, -1.48185045e-01],\n",
      "       [ 2.44095206e-01,  1.33155942e+00, -1.59102368e+00],\n",
      "       [ 3.23405981e+00, -2.92784274e-01, -2.31339717e+00],\n",
      "       [ 2.02803302e+00, -7.96951115e-01, -9.16686058e-01],\n",
      "       [ 1.36379445e+00, -3.22075874e-01, -9.15938377e-01],\n",
      "       [-1.97034270e-01,  1.31553635e-01, -1.56104034e-02],\n",
      "       [-3.47967774e-01, -1.35287964e+00,  1.38724065e+00],\n",
      "       [ 1.75561094e+00, -6.82742059e-01, -8.00854087e-01],\n",
      "       [-1.46547362e-01, -2.30397359e-01,  2.41468444e-01],\n",
      "       [ 8.99291396e-01, -6.98416710e-01, -1.44150510e-01],\n",
      "       [ 2.11296105e+00, -7.82866418e-01, -1.01594365e+00],\n",
      "       [ 2.19615030e+00,  1.12778926e-02, -1.93702376e+00],\n",
      "       [ 1.45623755e+00, -5.17923534e-01, -7.09489942e-01],\n",
      "       [ 2.73064041e+00, -7.61260033e-01, -1.50514579e+00],\n",
      "       [ 2.30068135e+00, -1.11131561e+00, -9.60366666e-01],\n",
      "       [ 2.16056371e+00, -1.23847175e+00, -6.39386773e-01],\n",
      "       [ 2.38745785e+00, -3.37949172e-02, -2.01836848e+00],\n",
      "       [ 1.86682209e-01, -2.55223308e-02, -2.03500450e-01],\n",
      "       [-1.21764886e+00, -1.28098810e+00,  1.86869073e+00],\n",
      "       [-1.20203269e+00, -9.71667051e-01,  1.79081523e+00],\n",
      "       [-5.25286376e-01, -2.13007379e+00,  2.30739307e+00],\n",
      "       [ 2.37130910e-01,  2.34394819e-01, -4.80658799e-01],\n",
      "       [ 8.31352115e-01, -4.06963676e-01, -3.96808177e-01],\n",
      "       [-2.50569940e-01,  1.07558262e+00, -9.35995936e-01],\n",
      "       [-1.18134487e+00, -2.42018372e-01,  1.00899673e+00],\n",
      "       [ 2.66391420e+00, -9.20154870e-01, -1.36393142e+00],\n",
      "       [-9.38123047e-01, -7.55735874e-01,  1.30573368e+00],\n",
      "       [-1.21428184e-01, -1.45832717e+00,  1.25816250e+00],\n",
      "       [ 1.37644613e+00, -9.04483080e-01, -2.84869194e-01],\n",
      "       [-1.30475247e+00, -1.00611281e+00,  1.90075386e+00],\n",
      "       [ 2.24151182e+00, -7.20552087e-01, -1.15705276e+00],\n",
      "       [-3.39752078e-01, -6.11049950e-01,  6.67162061e-01],\n",
      "       [ 2.41541982e+00, -5.67125261e-01, -1.47610009e+00],\n",
      "       [ 3.32256603e+00, -7.29107022e-01, -1.93738222e+00],\n",
      "       [ 7.27632999e-01,  4.76632059e-01, -1.12664282e+00],\n",
      "       [ 2.80909848e+00, -7.96015501e-01, -1.59691036e+00],\n",
      "       [ 2.40999296e-01, -1.38649678e+00,  9.53705311e-01],\n",
      "       [ 2.59124589e+00, -1.84794754e-01, -2.10383296e+00],\n",
      "       [ 1.70198119e+00, -5.98791182e-01, -8.34584177e-01],\n",
      "       [-7.90355504e-01, -8.09609473e-01,  1.18913972e+00],\n",
      "       [ 1.59860551e+00, -3.95268857e-01, -1.02356088e+00],\n",
      "       [-3.76270980e-01, -1.47878480e+00,  1.50459743e+00],\n",
      "       [-3.67992133e-01,  1.67168367e+00, -1.28871369e+00],\n",
      "       [-1.31490731e+00, -1.20443225e+00,  2.09439588e+00],\n",
      "       [ 2.91113114e+00, -3.88583094e-01, -1.94069266e+00],\n",
      "       [ 1.74065375e+00, -2.70655125e-01, -1.25551033e+00],\n",
      "       [ 3.17921233e+00, -4.49155301e-01, -2.16501427e+00],\n",
      "       [-2.78075308e-01,  9.32835877e-01, -8.56386900e-01],\n",
      "       [ 2.39566541e+00, -4.34535414e-01, -1.54569972e+00],\n",
      "       [-1.44597769e+00, -1.64186925e-01,  1.15850568e+00],\n",
      "       [ 1.99813950e+00, -4.83651131e-01, -1.34232557e+00],\n",
      "       [ 2.32261229e+00, -9.79258776e-01, -9.78171825e-01],\n",
      "       [ 2.80722797e-01, -5.02199233e-01,  1.01685382e-01],\n",
      "       [-7.61268258e-01, -1.53090465e+00,  1.83968902e+00],\n",
      "       [ 2.84123731e+00, -3.60789388e-01, -1.84586430e+00],\n",
      "       [ 1.07615888e+00,  8.26444566e-01, -1.78037333e+00],\n",
      "       [ 2.87375855e+00, -5.65402567e-01, -1.77292037e+00],\n",
      "       [ 1.78761244e+00, -1.08601272e+00, -5.43618917e-01],\n",
      "       [ 8.55250359e-01, -7.78348982e-01, -4.20550965e-02],\n",
      "       [ 2.34598041e+00, -8.07410955e-01, -1.14723253e+00],\n",
      "       [ 2.96256495e+00, -2.95518905e-01, -2.10980892e+00],\n",
      "       [ 8.02126154e-03,  1.77182823e-01, -3.68423074e-01],\n",
      "       [ 1.87318254e+00, -4.27073330e-01, -1.21823359e+00],\n",
      "       [-1.31928539e+00, -1.03874969e+00,  1.94567442e+00],\n",
      "       [-3.66518885e-01, -5.63546792e-02,  2.25575984e-01],\n",
      "       [ 2.33717608e+00, -3.38551223e-01, -1.62876594e+00],\n",
      "       [ 2.20016718e+00, -2.69533694e-01, -1.54734707e+00],\n",
      "       [ 2.92851830e+00, -3.54157180e-01, -2.05314207e+00],\n",
      "       [ 1.68336713e+00,  1.21497825e-01, -1.52933538e+00],\n",
      "       [ 1.39420009e+00, -6.47519708e-01, -5.62683165e-01],\n",
      "       [ 8.68592262e-01, -7.81227708e-01, -4.79054339e-02],\n",
      "       [ 1.76147974e+00, -5.91800213e-01, -9.57812428e-01],\n",
      "       [ 7.45962024e-01,  1.59693766e+00, -2.08539557e+00],\n",
      "       [ 2.89662313e+00, -6.56133831e-01, -1.80062091e+00],\n",
      "       [-1.04635394e+00, -1.21263289e+00,  1.80207050e+00],\n",
      "       [-6.54765189e-01, -1.52935159e+00,  1.79573703e+00],\n",
      "       [ 1.73640072e+00, -1.02725697e+00, -4.77005333e-01],\n",
      "       [ 2.76594424e+00, -2.50976503e-01, -1.94349241e+00],\n",
      "       [ 1.91086113e+00, -5.83855629e-01, -9.73297477e-01],\n",
      "       [ 2.95905519e+00, -1.33018568e-01, -2.24479032e+00],\n",
      "       [ 2.57215810e+00, -3.40529174e-01, -1.72961831e+00],\n",
      "       [ 3.54714133e-02, -1.55037344e+00,  1.20889401e+00],\n",
      "       [ 2.45093727e+00, -1.08278298e+00, -9.28193152e-01],\n",
      "       [ 2.60003495e+00, -2.55257517e-01, -2.06420016e+00],\n",
      "       [ 2.72258997e+00,  1.36800790e-02, -2.22084618e+00],\n",
      "       [ 3.00278187e+00, -6.05254710e-01, -1.84018481e+00],\n",
      "       [-4.72062021e-01, -8.58448222e-02,  3.57188910e-01],\n",
      "       [ 9.14471865e-01, -6.95571601e-01, -1.61804765e-01],\n",
      "       [ 9.36166644e-01, -1.74661383e-01, -7.26549327e-01],\n",
      "       [ 2.70860410e+00, -4.19397920e-01, -1.78376365e+00],\n",
      "       [ 2.91488194e+00, -6.55998111e-01, -1.83822989e+00],\n",
      "       [ 2.64212918e+00, -5.48958838e-01, -1.66577506e+00],\n",
      "       [ 5.29540837e-01, -1.29381144e+00,  6.22595787e-01]], dtype=float32), array([[[ 0.12609881, -0.04237919, -0.03534053, ...,  0.04215578,\n",
      "          0.00570193,  0.06867582],\n",
      "        [ 0.25566745,  0.12796779, -0.03209544, ...,  0.0104533 ,\n",
      "         -0.08848447, -0.03530617],\n",
      "        [ 0.00353479,  0.00656884,  0.00710826, ...,  0.00471395,\n",
      "         -0.00589697, -0.01149635],\n",
      "        ...,\n",
      "        [ 0.35404044, -0.06219174, -0.23256545, ...,  0.28939   ,\n",
      "         -0.10379592,  0.07378723],\n",
      "        [ 0.35404044, -0.06219174, -0.23256545, ...,  0.28939   ,\n",
      "         -0.10379592,  0.07378723],\n",
      "        [ 0.35404044, -0.06219174, -0.23256545, ...,  0.28939   ,\n",
      "         -0.10379592,  0.07378723]],\n",
      "\n",
      "       [[-0.269285  ,  0.1287864 , -0.05664707, ...,  0.10287943,\n",
      "         -0.01444669,  0.01252361],\n",
      "        [-0.08408808, -0.1401898 , -0.06933572, ..., -0.01268138,\n",
      "         -0.00652837,  0.11001478],\n",
      "        [-0.19055451, -0.09807981,  0.08898807, ...,  0.14385068,\n",
      "         -0.05036617, -0.03783198],\n",
      "        ...,\n",
      "        [-0.08320266,  0.0949074 , -0.08343372, ...,  0.23468423,\n",
      "          0.04086227,  0.07791819],\n",
      "        [-0.08320266,  0.0949074 , -0.08343372, ...,  0.23468423,\n",
      "          0.04086227,  0.07791819],\n",
      "        [-0.08320266,  0.0949074 , -0.08343372, ...,  0.23468423,\n",
      "          0.04086227,  0.07791819]],\n",
      "\n",
      "       [[-0.2369441 , -0.01139409, -0.14115585, ...,  0.05248832,\n",
      "          0.01649333, -0.09390429],\n",
      "        [-0.09903114,  0.00496145, -0.09028682, ...,  0.12139256,\n",
      "          0.12561564,  0.0261984 ],\n",
      "        [-0.17822537, -0.10230809, -0.05499387, ...,  0.03324373,\n",
      "         -0.08918419, -0.01363208],\n",
      "        ...,\n",
      "        [-0.04906199,  0.031736  , -0.06442221, ...,  0.2946876 ,\n",
      "          0.0131632 , -0.02084983],\n",
      "        [-0.04906199,  0.031736  , -0.06442221, ...,  0.2946876 ,\n",
      "          0.0131632 , -0.02084983],\n",
      "        [-0.04906199,  0.031736  , -0.06442221, ...,  0.2946876 ,\n",
      "          0.0131632 , -0.02084983]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.05761909, -0.02940391, -0.13360505, ..., -0.05039419,\n",
      "          0.02985891, -0.02102086],\n",
      "        [ 0.11677796, -0.14420743,  0.1021767 , ...,  0.01451256,\n",
      "          0.06800219,  0.17667904],\n",
      "        [ 0.170792  , -0.29927123,  0.0955347 , ..., -0.03785854,\n",
      "         -0.12379131,  0.17510808],\n",
      "        ...,\n",
      "        [ 0.02768637, -0.01369006, -0.0306072 , ...,  0.23641597,\n",
      "         -0.17643122,  0.09544652],\n",
      "        [ 0.02768637, -0.01369006, -0.0306072 , ...,  0.23641597,\n",
      "         -0.17643122,  0.09544652],\n",
      "        [ 0.02768637, -0.01369006, -0.0306072 , ...,  0.23641597,\n",
      "         -0.17643122,  0.09544652]],\n",
      "\n",
      "       [[-0.14410327, -0.26357225,  0.254748  , ..., -0.01592459,\n",
      "         -0.11313711, -0.15489279],\n",
      "        [-0.13420446, -0.09779257,  0.08285081, ..., -0.11403484,\n",
      "         -0.19422355, -0.12962678],\n",
      "        [ 0.10455974, -0.13905406,  0.05123702, ..., -0.05933153,\n",
      "         -0.06068369, -0.13174607],\n",
      "        ...,\n",
      "        [-0.0052034 ,  0.0773989 , -0.09407668, ...,  0.10169178,\n",
      "         -0.10015633, -0.03038193],\n",
      "        [-0.0052034 ,  0.0773989 , -0.09407668, ...,  0.10169178,\n",
      "         -0.10015633, -0.03038193],\n",
      "        [-0.0052034 ,  0.0773989 , -0.09407668, ...,  0.10169178,\n",
      "         -0.10015633, -0.03038193]],\n",
      "\n",
      "       [[-0.05171071, -0.07570403, -0.14578174, ..., -0.01407065,\n",
      "          0.01855752, -0.1353372 ],\n",
      "        [ 0.13451959, -0.12506117, -0.22166309, ...,  0.10739798,\n",
      "         -0.14002515, -0.1266553 ],\n",
      "        [ 0.16938269,  0.01010129, -0.2668228 , ...,  0.08488925,\n",
      "         -0.24037297, -0.06581862],\n",
      "        ...,\n",
      "        [-0.00811596,  0.16274264, -0.1523616 , ...,  0.13414529,\n",
      "         -0.09319334,  0.11989058],\n",
      "        [-0.00811596,  0.16274264, -0.1523616 , ...,  0.13414529,\n",
      "         -0.09319334,  0.11989058],\n",
      "        [-0.00811596,  0.16274264, -0.1523616 , ...,  0.13414529,\n",
      "         -0.09319334,  0.11989058]]], dtype=float32)), label_ids=array([0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2,\n",
      "       0, 1, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 1, 0, 0,\n",
      "       0, 2, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 2,\n",
      "       0, 0, 2, 2, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       2, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 1, 0, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "       0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0,\n",
      "       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 0,\n",
      "       0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2]), metrics={'test_loss': 0.9753966927528381, 'test_accuracy': 0.6196581196581197, 'test_precision': 0.5722809185459787, 'test_recall': 0.6196581196581197, 'test_f1': 0.5906453297207288, 'test_runtime': 7.6502, 'test_samples_per_second': 30.587, 'test_steps_per_second': 3.921})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48fb2163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed6cfde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")  # Adjust xticklabels and yticklabels as needed\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "346c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.4.4_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4543aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad099d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           35\n",
       "Bone health              14\n",
       "Fitness                  12\n",
       "Cancer                   12\n",
       "Throat                    9\n",
       "Hair                      8\n",
       "Cardiovascular Health     7\n",
       "Diabetes                  7\n",
       "Skin                      7\n",
       "Ear                       6\n",
       "Neurological health       6\n",
       "Women' s Health           5\n",
       "Men's health              4\n",
       "Blood                     4\n",
       "Mental Health             3\n",
       "Eye                       3\n",
       "COVID                     3\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d45817c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skin                     17\n",
       "General Health           16\n",
       "Bone health               7\n",
       "Muscles                   6\n",
       "Eye                       6\n",
       "Diabetes                  5\n",
       "Cardiovascular Health     5\n",
       "Blood                     5\n",
       "Hair                      4\n",
       "Dental Health             3\n",
       "Fitness                   3\n",
       "Neurological health       3\n",
       "Vascular                  3\n",
       "COVID                     3\n",
       "Men's health              2\n",
       "Women' s Health           1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
