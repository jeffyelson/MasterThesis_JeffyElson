{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ed78e0185729396\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 230.42it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d8a73572c5526b09.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71c34de47dd068c7.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9e4fe1ebeeeb9fd4.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='dataset_semanticattribution_nerfeatures_split.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"claim\", \"premise\", \"label\", \"category\", \"count_bf\", \"count_ca\", \"count_dis\",\n",
    "        \"count_food\", \"count_lipid\", \"count_treat\", \"pres_bf\", \"pres_ca\", \"pres_dis\",\n",
    "        \"pres_food\", \"pres_lipid\", \"pres_treat\", \"counte_bf\", \"counte_ca\", \"counte_dis\",\n",
    "        \"counte_food\", \"counte_lipid\", \"counte_treat\", \"prese_bf\", \"prese_ca\", \"prese_dis\",\n",
    "        \"prese_food\", \"prese_lipid\", \"prese_treat\", \"url\", \"entities\", \"entity_map\",\n",
    "        \"gold_exp\", \"gemini_exp\", \"gemini_label\",\"entity_ev\",\"entity_map_ev\", \"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f1db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\", \"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-cd62fee432318997.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d93ed7a61272b334.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-edc4f9887fcc00ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 0,\n",
    "    \"entailment\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "        claim = item['claim'] \n",
    "        evidences = item['premise'].replace('[','').replace(']','').replace('\\n','')\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        additional_features = [\n",
    "            \"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "    \n",
    "        for feature in additional_features:\n",
    "            if feature in item:\n",
    "                claim += \"[SEP]\" + str(item[feature])\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences,claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "         # Construct the output item\n",
    "        output_item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),  # Remove batch dimension\n",
    "            'claim': claim,  # Include augmented claim text\n",
    "            'evidences': evidences  # Include original evidence text\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            output_item['label'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return output_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c5e0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    3,   117,   597,  9789,    23,     2,     6,   445,     5,   117,\n",
       "         18150,  2168,     2,     6,    27,     5,   117,  3049,     9,     2,\n",
       "          9789,    23,     2,     6,   276,     5,   117,   350, 31371,  9881,\n",
       "             2,     6,   283,     5,   117, 11469,    32,  1924,  2099,     2,\n",
       "             6,   411,     5,   117,   584,  1598,    32,  1924,  2099,     2,\n",
       "             6,   446,     5,  4937,    77,    75,  5167,    11,    82,    52,\n",
       "            52,   107,  1832, 10229,    11,  5958,    16,    75,  5167, 11349,\n",
       "            15,   581,  2179,    18,    77, 29884,     7,    13,  8802,  4900,\n",
       "         21128,     7,     5,   134,  1483,    15,    15,    51,     6,    27,\n",
       "             5,     3, 16977,   235, 14676,     3,   184, 12206,  1055,     7,\n",
       "            13,  8054,    52,   954,  8511,    23,    41,     5,   667,   226,\n",
       "            23,    26,  1528,  2189,    11, 17133, 14367,  1951,    13,   991,\n",
       "            11,    70, 28661,   257,    28,    82,    52,    52,   107,    41,\n",
       "         10205,    23, 19968,     9,     3,  4641,  4641,    61,     3,    15,\n",
       "          4115,  1938,     5,   427,     7,     7,  7220,  6067,     7,    10,\n",
       "          9222,   138, 23482,     7,    21, 13038,  2686,     5,   254, 15416,\n",
       "         11852,  1408,    63,     6,   445,     5,   117, 13329,  2152,   122,\n",
       "             6,   205,     5,   117,   901,  4331,   457,     6,   446,     5,\n",
       "           117, 21263,    40,     7,     6,   411,     5,   117, 11147,     6,\n",
       "           283,     5, 10060,  8009,    57,  6869,  7554,    10,   205,     5,\n",
       "            82,    52,    52,  1024,  6067, 13262,   302,  3068,  8527,     5,\n",
       "           566,     9,  6983, 13399,     6,   391,     5,   117,  1626,  6983,\n",
       "         13399,     6,   180,     5,   117,  1626,  6983, 13399,     6,   283,\n",
       "             5,   117, 19669,    40,  1665,     6,   283,     5,  4937,    77,\n",
       "            75,  5167,    41,     2,   391,     2,     3,     4,    23,     2,\n",
       "          1725,   117,     5,  7576,   725,    61,    10,  1029,     8,  1801,\n",
       "            13,  1435,  1564,    12,     8,  3714, 30512, 10896,    21,     8,\n",
       "          9793,    11,  1058,    13,  2261,  6716,     5,  3541, 12641,  8224,\n",
       "            11, 17133, 22763,  6546,  1756,    13,     3,  6296,  2917,    75,\n",
       "          5167,  1043,     5,  5890,  4718,     7,     3,  6443,  1491,     7,\n",
       "          2091,    23,     9,  5819,     7,     5,   117, 10078,     6,   262,\n",
       "             5,  4937,    77,    75,  5167,    11,    82,    52,    52,   107,\n",
       "            38, 20203,    16,   502,     5,     1,   499,    52,    52,   107,\n",
       "          1832,  1043,    19,  1664,   261,    16, 26309,   494,    12,   199,\n",
       "          1172,     8,  3179,    13,     8,  1133,     5,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'claim': 'Myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0',\n",
       " 'evidences': '; Unković, N.; Dimkić, I.; Janaćković, P.; Gavrilović, M.; Stanojević, O.; Vukojević, J. Frankincense and myrrh essential oils and burn incense fume against micro-inhabitants of sacral ambients.Shameem, I. Phytochemical & therapeutic potentials of Murr makki (.Oxidative stress and immunotoxic effects of lead and their amelioration with myrrh (Commiphora molmol) emulsion.Essential Oils: Magical Ingredients for Skin Care.Chakravarty, N.; Kellogg, C.; Alvarez, J.; Equils, O.; Morgan, M. UV Protection by Natural Products: C. myrrha Oil Versus Sunscreen.Hamidpour, R.; Hamidpour, S.; Hamidpour, M.; Shahlari, M. Frankincense (乳香 Rǔ Xiāng;.species): From the selection of traditional applications to the novel phytotherapy for the prevention and treatment of serious diseases.Chemistry and immunomodulatory activity of frankincense oil.Compositions containing Boswellia extracts.; Cooper, E. Frankincense and myrrh as remedies in children.',\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 42:11, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>0.791243</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.480356</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.564325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.728197</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.671613</td>\n",
       "      <td>0.686022</td>\n",
       "      <td>0.678603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558800</td>\n",
       "      <td>0.940772</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.676518</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.663278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>1.214587</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.691414</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.687471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>1.864098</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.693066</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.673170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>2.109404</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.689365</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.667972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>2.491406</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.708501</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.674163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>2.814705</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.707437</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.676206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>2.933511</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.694022</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.669653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>3.089622</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.707026</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.677040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>3.116209</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.694955</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.669983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>3.227178</td>\n",
       "      <td>0.647312</td>\n",
       "      <td>0.695417</td>\n",
       "      <td>0.647312</td>\n",
       "      <td>0.667779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>3.241853</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.693934</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.671282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>3.325545</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.696190</td>\n",
       "      <td>0.653763</td>\n",
       "      <td>0.672169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>3.365611</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.693546</td>\n",
       "      <td>0.649462</td>\n",
       "      <td>0.668606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-406/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-203] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-609/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.3_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.2.3_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.3_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.2.3_flant5/checkpoint-406 (score: 0.6860215053763441).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 05:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.2.3_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.2.3_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.2.3_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.2.3_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.2.3_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.2.3_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.2.3_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.2.3_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.2.3_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.2.3_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.2.3_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.2.3_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.2.3_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.2.3_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.2.3_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.2.3_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.2.3_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.2.3_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-0.52983487,  1.4064497 , -1.0700465 ],\n",
      "       [-0.41349682,  2.3907807 , -1.9759951 ],\n",
      "       [-0.30147812,  1.6960384 , -1.4242922 ],\n",
      "       [ 0.7122229 ,  0.02501809, -0.8084726 ],\n",
      "       [-0.08061912,  1.6995815 , -1.519418  ],\n",
      "       [-1.9592326 ,  4.3700233 , -2.4798322 ],\n",
      "       [ 0.35471514,  1.6813546 , -1.7787136 ],\n",
      "       [-1.2926892 ,  2.975718  , -1.8587772 ],\n",
      "       [-2.4297543 ,  3.833205  , -1.782249  ],\n",
      "       [ 0.07462247,  1.2034386 , -1.2830405 ],\n",
      "       [-1.584207  ,  3.0848942 , -1.816911  ],\n",
      "       [-0.08811323,  1.8563075 , -1.6564773 ],\n",
      "       [-0.59467655,  1.4240216 , -1.0650991 ],\n",
      "       [-1.7019129 ,  2.8838217 , -1.5302967 ],\n",
      "       [ 0.24149063,  0.9078586 , -1.1050913 ],\n",
      "       [ 0.88723546, -0.12783685, -0.7709628 ],\n",
      "       [-0.23609671,  0.7487022 , -0.55837095],\n",
      "       [ 0.24970518,  0.828155  , -1.0776731 ],\n",
      "       [-0.576694  ,  2.7632205 , -2.0163789 ],\n",
      "       [-0.7823055 ,  3.369941  , -2.4688106 ],\n",
      "       [-0.05339657,  1.0819149 , -1.0651755 ],\n",
      "       [-1.5563556 ,  3.2539065 , -1.7172391 ],\n",
      "       [-0.25636914,  1.9577423 , -1.6463646 ],\n",
      "       [ 0.9291867 , -0.20407842, -0.74511063],\n",
      "       [ 0.6545621 , -0.9071589 ,  0.1669682 ],\n",
      "       [ 0.54158115, -0.3257801 , -0.20416895],\n",
      "       [-0.3053754 ,  2.314262  , -1.95299   ],\n",
      "       [-0.6754585 ,  2.2736235 , -1.4370971 ],\n",
      "       [-0.16054566,  0.3406735 , -0.34955445],\n",
      "       [-0.8084755 ,  1.97848   , -1.3081739 ],\n",
      "       [-0.2488996 ,  1.1363256 , -1.0136168 ],\n",
      "       [-1.4812441 ,  3.2740312 , -1.902098  ],\n",
      "       [-0.2920248 ,  2.0274937 , -1.6876947 ],\n",
      "       [-0.7795569 ,  2.1299746 , -1.528996  ],\n",
      "       [-2.30683   ,  3.1823492 , -1.1389589 ],\n",
      "       [-0.30347893,  0.82998544, -0.6194617 ],\n",
      "       [ 0.16119248,  0.9903951 , -1.2700197 ],\n",
      "       [-0.96163684,  2.5739477 , -1.712744  ],\n",
      "       [ 0.8501374 , -0.1598853 , -0.744309  ],\n",
      "       [-0.318123  ,  1.1223953 , -0.8749302 ],\n",
      "       [-0.05929826,  1.0749907 , -1.1118532 ],\n",
      "       [-1.7933059 ,  3.4499426 , -1.9118418 ],\n",
      "       [ 0.21537557,  1.4407933 , -1.5155259 ],\n",
      "       [ 0.57295847,  0.49907452, -0.9835973 ],\n",
      "       [-2.1287625 ,  2.881247  , -1.0268198 ],\n",
      "       [-0.3980152 ,  2.0437446 , -1.6256956 ],\n",
      "       [-0.8118376 ,  1.5291425 , -0.81931514],\n",
      "       [ 0.15572256,  1.4987724 , -1.5231631 ],\n",
      "       [-0.28837833,  2.523259  , -2.1318164 ],\n",
      "       [ 1.0590228 , -0.9354275 , -0.18502033],\n",
      "       [ 0.62556654,  0.17594197, -0.82204276],\n",
      "       [ 0.22022459,  1.5098263 , -1.5951984 ],\n",
      "       [ 0.04846711,  0.8706301 , -1.0746417 ],\n",
      "       [-1.6498874 ,  2.9958324 , -1.4902738 ],\n",
      "       [ 0.69856215,  0.42169058, -1.0444647 ],\n",
      "       [-0.45544675,  2.5611303 , -2.0035717 ],\n",
      "       [-0.03678727,  1.083616  , -1.0957384 ],\n",
      "       [-1.0710572 ,  2.2981024 , -1.2980747 ],\n",
      "       [-0.45573214,  2.54921   , -1.9880809 ],\n",
      "       [-0.3343555 ,  2.193012  , -1.7302368 ],\n",
      "       [ 0.61853105,  0.59089684, -1.0953097 ],\n",
      "       [ 0.9114155 , -0.625241  , -0.36236247],\n",
      "       [-0.6129599 ,  1.4839271 , -0.9313349 ],\n",
      "       [ 0.04384142,  1.402243  , -1.444583  ],\n",
      "       [-0.3824632 ,  0.79361254, -0.49881932],\n",
      "       [-0.3300304 ,  2.4024131 , -1.9145122 ],\n",
      "       [-0.8766839 ,  3.1040022 , -2.1581056 ],\n",
      "       [-0.21218379,  2.9643016 , -2.450178  ],\n",
      "       [-1.815717  ,  3.268348  , -1.836285  ],\n",
      "       [ 0.6065255 , -0.5502298 , -0.18279386],\n",
      "       [-0.7068473 ,  1.785843  , -1.2976491 ],\n",
      "       [-0.87455875,  1.1606103 , -0.45454854],\n",
      "       [-0.4710916 ,  0.58699113, -0.2421415 ],\n",
      "       [-0.58792096,  1.5380285 , -1.1235994 ],\n",
      "       [ 0.32210428,  0.24144949, -0.6103447 ],\n",
      "       [ 0.8156061 , -0.39043507, -0.4980981 ],\n",
      "       [-0.5693888 ,  1.8139751 , -1.303318  ],\n",
      "       [ 0.93066806,  1.2911378 , -1.9685227 ],\n",
      "       [-0.7398671 ,  2.7115483 , -2.0767534 ],\n",
      "       [ 0.29122293,  0.4113241 , -0.61841863],\n",
      "       [-0.5707265 ,  3.09738   , -2.3973331 ],\n",
      "       [-0.6075901 ,  2.3620708 , -1.6972673 ],\n",
      "       [-1.2778219 ,  3.344689  , -2.1341686 ],\n",
      "       [-0.8025762 ,  2.2095158 , -1.4803118 ],\n",
      "       [-0.99813116,  2.4919696 , -1.7097938 ],\n",
      "       [ 0.02310332,  1.4444958 , -1.5817513 ],\n",
      "       [-0.30367735,  1.8394817 , -1.4795266 ],\n",
      "       [-0.23892254,  1.0266813 , -0.864     ],\n",
      "       [-0.8404779 ,  2.3167484 , -1.6021925 ],\n",
      "       [-0.73854077,  1.5511485 , -0.9400028 ],\n",
      "       [-0.07195387,  1.7636384 , -1.5802529 ],\n",
      "       [-1.4363537 ,  2.4414864 , -1.3054054 ],\n",
      "       [ 0.08265387,  1.3963485 , -1.3609518 ],\n",
      "       [ 0.06515228,  1.5420507 , -1.5085378 ],\n",
      "       [ 0.07684498,  1.676571  , -1.7015321 ],\n",
      "       [ 0.285777  ,  0.75434095, -1.0660999 ],\n",
      "       [ 0.5028154 ,  0.8934861 , -1.2897356 ],\n",
      "       [-0.04949076,  1.7560898 , -1.6435055 ],\n",
      "       [-1.5625242 ,  3.5069332 , -2.0822306 ],\n",
      "       [-0.08159686,  1.3321998 , -1.2653328 ],\n",
      "       [ 1.0301292 , -0.72408277, -0.3122094 ],\n",
      "       [-0.70098436,  2.7277036 , -1.9399992 ],\n",
      "       [-2.3842309 ,  3.536211  , -1.5168371 ],\n",
      "       [-0.38535482,  0.54083675, -0.3328226 ],\n",
      "       [ 0.28556144,  0.27791324, -0.70041513],\n",
      "       [-2.1209822 ,  3.2425416 , -1.5221864 ],\n",
      "       [ 0.1926272 ,  0.38247383, -0.71212894],\n",
      "       [ 0.18814903,  1.0707023 , -1.2529075 ],\n",
      "       [-1.7514738 ,  2.9554887 , -1.6037465 ],\n",
      "       [-1.6589732 ,  3.52167   , -2.0445116 ],\n",
      "       [-1.1186733 ,  1.6554663 , -0.7956639 ],\n",
      "       [-1.8536801 ,  3.2344246 , -1.7232382 ],\n",
      "       [-0.45964247,  2.7834613 , -2.1991239 ],\n",
      "       [-0.77973974,  1.6339945 , -1.0111732 ],\n",
      "       [-0.9929329 ,  2.8912458 , -1.9205021 ],\n",
      "       [-1.2361783 ,  2.3319104 , -1.423312  ],\n",
      "       [-0.86420476,  2.5825553 , -1.8016078 ],\n",
      "       [ 0.6883358 , -0.15352333, -0.50439966],\n",
      "       [-1.1582832 ,  2.5776453 , -1.6885602 ],\n",
      "       [ 0.66819763,  1.6948246 , -2.1464508 ],\n",
      "       [-1.1649305 ,  2.2188451 , -1.2255478 ],\n",
      "       [-0.8788141 ,  2.492089  , -1.8211981 ],\n",
      "       [-1.5999852 ,  3.1490905 , -1.5208156 ],\n",
      "       [-1.6031611 ,  3.1561701 , -1.6490264 ],\n",
      "       [-1.5066365 ,  3.1654284 , -1.7490642 ],\n",
      "       [-0.93839127,  2.7949965 , -1.8337747 ],\n",
      "       [-0.5050077 ,  0.90129614, -0.6086453 ],\n",
      "       [-0.27675882,  2.0018551 , -1.6554456 ],\n",
      "       [ 0.10211465,  1.0489595 , -1.1933517 ],\n",
      "       [-0.1975199 ,  1.6460327 , -1.4066864 ],\n",
      "       [ 0.5788749 ,  0.4078263 , -0.98464966],\n",
      "       [ 0.34888995,  0.7617203 , -1.1047848 ],\n",
      "       [-0.9423351 ,  2.3174107 , -1.36376   ],\n",
      "       [-0.21105433,  1.6843939 , -1.5694927 ],\n",
      "       [-1.6443719 ,  2.1339278 , -0.9536719 ],\n",
      "       [-0.11145796,  1.2468095 , -1.2043566 ],\n",
      "       [ 0.20110622,  0.38361105, -0.6730409 ],\n",
      "       [ 0.15105535,  1.6335341 , -1.7157761 ],\n",
      "       [ 0.8604731 , -0.03337868, -0.73699063],\n",
      "       [-2.095149  ,  3.3774202 , -1.5724165 ],\n",
      "       [-0.11717346,  2.0213375 , -1.8682957 ],\n",
      "       [-1.3652544 ,  2.29347   , -1.1897736 ],\n",
      "       [ 0.14500788,  1.2584943 , -1.3214681 ],\n",
      "       [-0.5358131 ,  3.2292793 , -2.4359632 ],\n",
      "       [ 0.88895404,  0.00621131, -0.9350332 ],\n",
      "       [ 0.7381401 , -0.3250251 , -0.44290677],\n",
      "       [-0.05258653,  1.8082253 , -1.6976695 ],\n",
      "       [ 0.62513787,  0.20868473, -0.8356007 ],\n",
      "       [-0.53764915,  2.58986   , -2.086328  ],\n",
      "       [ 0.38066068,  0.97731996, -1.2500908 ],\n",
      "       [-1.3789251 ,  1.9010683 , -0.74609905],\n",
      "       [ 0.11465777,  2.046517  , -1.9871901 ],\n",
      "       [-1.8992696 ,  3.3254428 , -1.6256018 ],\n",
      "       [-0.07922071,  1.6239858 , -1.5593206 ],\n",
      "       [-1.3770158 ,  3.0465066 , -1.8562796 ],\n",
      "       [-0.6703526 ,  1.2703037 , -0.905372  ],\n",
      "       [-1.5910076 ,  3.2773643 , -1.8270619 ],\n",
      "       [-0.25430766,  2.0180712 , -1.7248021 ],\n",
      "       [ 0.09994144,  0.90805894, -1.036637  ],\n",
      "       [ 0.1261142 ,  0.30589706, -0.58713734],\n",
      "       [ 0.86513096, -0.33430815, -0.5522151 ],\n",
      "       [ 0.81624335, -0.29745418, -0.59731334],\n",
      "       [-0.12395075,  1.6281823 , -1.4582443 ],\n",
      "       [ 0.9437673 ,  0.51126707, -1.3415782 ],\n",
      "       [ 0.69164366,  0.00840916, -0.6565816 ],\n",
      "       [ 0.5848312 ,  0.37995827, -0.93751866],\n",
      "       [-0.07253613,  1.21887   , -1.2618881 ],\n",
      "       [ 0.39928052,  0.31306082, -0.7279454 ],\n",
      "       [ 0.5552338 , -0.18824975, -0.41628847],\n",
      "       [ 0.6409308 ,  0.61484116, -1.2142036 ],\n",
      "       [ 0.9173169 , -0.4850611 , -0.41413784],\n",
      "       [-0.6687781 ,  2.3848333 , -1.7783815 ],\n",
      "       [ 0.6413073 ,  0.38041195, -0.87502736],\n",
      "       [-0.27021536,  2.8430061 , -2.3339596 ],\n",
      "       [-0.86061454,  2.5131166 , -1.8392203 ],\n",
      "       [-0.05267498,  0.79927266, -0.844026  ],\n",
      "       [-1.4123164 ,  2.9400666 , -1.7575986 ],\n",
      "       [ 0.65340894, -0.7591968 ,  0.03801724],\n",
      "       [ 0.05472374,  1.3185984 , -1.3696259 ],\n",
      "       [ 0.8876352 ,  0.06575993, -0.841742  ],\n",
      "       [ 0.7999658 ,  0.36279482, -1.0530925 ],\n",
      "       [ 0.50540864,  0.78008443, -1.2336794 ],\n",
      "       [ 0.8742289 , -0.2726861 , -0.6394262 ],\n",
      "       [-0.772104  ,  1.2739285 , -0.7914487 ],\n",
      "       [-2.3160336 ,  3.1583536 , -1.137785  ],\n",
      "       [-0.3026754 ,  2.2796824 , -1.9869668 ],\n",
      "       [-0.02823501,  0.7005858 , -0.75438106],\n",
      "       [-1.6439064 ,  2.9076622 , -1.4736242 ],\n",
      "       [ 0.59368443,  0.342716  , -0.9117219 ],\n",
      "       [-0.5437078 ,  2.3462453 , -1.7026788 ],\n",
      "       [-1.0724905 ,  2.0961936 , -1.2695296 ],\n",
      "       [-0.5503692 ,  1.9747344 , -1.4911705 ],\n",
      "       [ 0.35656664,  1.8936561 , -1.9889897 ],\n",
      "       [-0.15019888,  1.5503016 , -1.4872559 ],\n",
      "       [ 0.9102072 , -0.256342  , -0.69583976],\n",
      "       [-1.0302751 ,  3.0696213 , -2.2146118 ],\n",
      "       [-1.4731263 ,  3.0379128 , -1.8336533 ],\n",
      "       [-2.6425722 ,  3.7469728 , -1.6097299 ],\n",
      "       [-0.9908887 ,  1.5125287 , -0.71156687],\n",
      "       [-1.320103  ,  2.0518801 , -0.74312115],\n",
      "       [-0.5293642 ,  2.3126063 , -1.789614  ],\n",
      "       [-1.0462166 ,  3.6186588 , -2.6254535 ],\n",
      "       [-0.64587694,  1.2569144 , -0.7143265 ],\n",
      "       [-0.7986397 ,  3.0470855 , -2.1105328 ],\n",
      "       [-1.3139819 ,  2.2499528 , -1.2562134 ],\n",
      "       [ 0.37650216,  0.35519397, -0.78041005],\n",
      "       [ 0.0730767 ,  2.1733758 , -2.0094504 ],\n",
      "       [ 0.16439326,  0.70990735, -0.93105924],\n",
      "       [-1.6052259 ,  3.818785  , -2.4470792 ],\n",
      "       [ 0.00665073,  2.0677702 , -1.9727299 ],\n",
      "       [-1.8365166 ,  2.6047823 , -1.145136  ],\n",
      "       [-0.49904925,  0.70214266, -0.45011443],\n",
      "       [-0.3240778 ,  2.6321912 , -2.0644023 ],\n",
      "       [-1.7206756 ,  3.1863966 , -2.0052493 ],\n",
      "       [-0.730173  ,  2.4889855 , -1.9170046 ],\n",
      "       [-0.35456705,  1.3474718 , -1.054271  ],\n",
      "       [-0.07036251,  1.178791  , -1.172822  ],\n",
      "       [ 0.1519382 ,  1.0701241 , -1.3009291 ],\n",
      "       [-1.9830346 ,  3.8104296 , -2.187805  ],\n",
      "       [ 0.8971451 , -0.07381242, -0.8346353 ],\n",
      "       [-0.93940294,  3.1859758 , -2.0796814 ],\n",
      "       [-0.5417515 ,  2.0585105 , -1.5475289 ],\n",
      "       [ 0.8190226 , -0.08836981, -0.7961531 ],\n",
      "       [-1.5252489 ,  3.7279723 , -2.2413778 ],\n",
      "       [-1.0223067 ,  2.53841   , -1.629473  ],\n",
      "       [-1.3030642 ,  3.5716107 , -2.411537  ],\n",
      "       [-0.9182825 ,  2.788517  , -1.9014664 ],\n",
      "       [ 0.7951223 , -0.31232563, -0.49749082],\n",
      "       [ 0.5699734 ,  0.8898131 , -1.3341106 ],\n",
      "       [ 0.57631   , -0.32000506, -0.27542806],\n",
      "       [-0.912533  ,  2.6311014 , -1.7941655 ],\n",
      "       [ 0.59766304,  0.62150806, -1.173272  ],\n",
      "       [-0.6704665 ,  2.3318589 , -1.7501454 ],\n",
      "       [ 0.8212074 , -0.6622996 , -0.33568156]], dtype=float32), array([[[-6.99231029e-02,  2.41911292e-01,  6.58240216e-03, ...,\n",
      "          2.23375969e-02,  8.43623877e-02,  9.74094346e-02],\n",
      "        [-1.93023264e-01,  4.73057330e-02, -2.05024444e-02, ...,\n",
      "          1.69280231e-01,  5.65975606e-02,  6.16366304e-02],\n",
      "        [-2.60385543e-01,  2.60608811e-02,  3.00632473e-02, ...,\n",
      "         -1.03938200e-01, -5.01423553e-02,  1.49516016e-01],\n",
      "        ...,\n",
      "        [-1.48020312e-01,  1.38903812e-01,  5.03976643e-03, ...,\n",
      "          3.92605543e-01,  1.67294130e-01,  1.29666207e-02],\n",
      "        [-1.80529743e-01,  1.42412841e-01,  1.07080769e-03, ...,\n",
      "          3.78321290e-01,  1.75569817e-01,  1.60019174e-02],\n",
      "        [-1.01351574e-01,  1.17718592e-01,  4.99463081e-03, ...,\n",
      "          3.92632604e-01,  1.63089499e-01,  2.52826437e-02]],\n",
      "\n",
      "       [[-7.08215386e-02,  2.22140267e-01, -1.22578382e-01, ...,\n",
      "          5.60142882e-02,  4.26210724e-02,  1.35547832e-01],\n",
      "        [-5.63858189e-02,  1.89827457e-01, -1.07619449e-01, ...,\n",
      "         -2.14068234e-01,  4.71649691e-04,  8.58795121e-02],\n",
      "        [-5.40577881e-02,  1.25467882e-01, -9.05531868e-02, ...,\n",
      "         -8.43454227e-02, -9.16393101e-02,  1.54759869e-01],\n",
      "        ...,\n",
      "        [ 3.58980522e-03,  2.46250868e-01,  1.46014333e-01, ...,\n",
      "          1.62767962e-01,  2.29817167e-01,  4.43963930e-02],\n",
      "        [ 3.58980522e-03,  2.46250868e-01,  1.46014333e-01, ...,\n",
      "          1.62767962e-01,  2.29817167e-01,  4.43963930e-02],\n",
      "        [ 3.58980522e-03,  2.46250868e-01,  1.46014333e-01, ...,\n",
      "          1.62767962e-01,  2.29817167e-01,  4.43963930e-02]],\n",
      "\n",
      "       [[-1.27174715e-02,  1.22220688e-01,  4.09038644e-03, ...,\n",
      "          8.16079788e-03,  1.58652261e-01,  3.93025205e-02],\n",
      "        [ 2.28917450e-02,  2.94489175e-01, -1.70703769e-01, ...,\n",
      "          1.05938278e-01,  1.56275347e-01,  1.46865591e-01],\n",
      "        [ 3.98973562e-02,  2.09697232e-01, -1.17865875e-01, ...,\n",
      "         -1.35082915e-01,  7.45020956e-02,  8.63585025e-02],\n",
      "        ...,\n",
      "        [ 1.61722735e-01, -1.91591065e-02,  1.08761676e-01, ...,\n",
      "          2.78899193e-01,  3.14577557e-02,  1.60956398e-01],\n",
      "        [ 1.61722735e-01, -1.91591065e-02,  1.08761676e-01, ...,\n",
      "          2.78899193e-01,  3.14577557e-02,  1.60956398e-01],\n",
      "        [ 1.61722735e-01, -1.91591065e-02,  1.08761676e-01, ...,\n",
      "          2.78899193e-01,  3.14577557e-02,  1.60956398e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-2.38688722e-01,  1.52493045e-01, -1.50033459e-01, ...,\n",
      "          1.43548146e-01, -8.64821225e-02, -2.66936999e-02],\n",
      "        [-2.06772462e-01,  2.97951996e-01, -2.89291535e-02, ...,\n",
      "          1.57200739e-01, -4.25969549e-02,  2.71116972e-01],\n",
      "        [-1.47437409e-01,  2.97728360e-01, -5.04335528e-03, ...,\n",
      "          1.47439480e-01, -7.09271058e-02,  3.49432714e-02],\n",
      "        ...,\n",
      "        [-1.59786314e-01,  1.76855072e-01,  1.05636083e-01, ...,\n",
      "          2.39302099e-01,  1.48010731e-01,  5.60345128e-02],\n",
      "        [-1.59786314e-01,  1.76855072e-01,  1.05636083e-01, ...,\n",
      "          2.39302099e-01,  1.48010731e-01,  5.60345128e-02],\n",
      "        [-1.59786314e-01,  1.76855072e-01,  1.05636083e-01, ...,\n",
      "          2.39302099e-01,  1.48010731e-01,  5.60345128e-02]],\n",
      "\n",
      "       [[-2.05845743e-01,  2.26129200e-02, -9.37491804e-02, ...,\n",
      "          2.26059988e-01, -3.82937193e-02, -9.66918841e-02],\n",
      "        [ 1.52684813e-02,  3.82435601e-03, -7.07952902e-02, ...,\n",
      "          1.41920313e-01, -4.24435735e-02,  2.12254375e-02],\n",
      "        [ 7.30828121e-02, -1.13283768e-01, -2.59870365e-02, ...,\n",
      "          6.50563557e-03,  4.95926440e-02, -1.02051109e-01],\n",
      "        ...,\n",
      "        [ 4.97323424e-02,  2.91398913e-02,  2.43515875e-02, ...,\n",
      "          1.05479248e-01,  1.67732567e-01,  8.58502388e-02],\n",
      "        [ 4.97323424e-02,  2.91398913e-02,  2.43515875e-02, ...,\n",
      "          1.05479248e-01,  1.67732567e-01,  8.58502388e-02],\n",
      "        [ 4.97323424e-02,  2.91398913e-02,  2.43515875e-02, ...,\n",
      "          1.05479248e-01,  1.67732567e-01,  8.58502388e-02]],\n",
      "\n",
      "       [[-1.59663871e-01,  6.57169223e-02,  3.29822786e-02, ...,\n",
      "          2.70023830e-02,  2.65174508e-01,  1.37875751e-01],\n",
      "        [-1.22934185e-01, -3.83750943e-04,  7.19686821e-02, ...,\n",
      "          9.49299484e-02,  1.93582654e-01, -4.54670675e-02],\n",
      "        [-1.23724110e-01,  1.91324055e-01,  5.38646057e-02, ...,\n",
      "         -2.35823557e-01,  1.83037907e-01,  5.31415306e-02],\n",
      "        ...,\n",
      "        [-2.47457102e-01,  1.96388900e-01, -2.33254712e-02, ...,\n",
      "          1.83762804e-01,  9.96991768e-02,  1.71619311e-01],\n",
      "        [-2.47457102e-01,  1.96388900e-01, -2.33254712e-02, ...,\n",
      "          1.83762804e-01,  9.96991768e-02,  1.71619311e-01],\n",
      "        [-2.47457102e-01,  1.96388900e-01, -2.33254712e-02, ...,\n",
      "          1.83762804e-01,  9.96991768e-02,  1.71619311e-01]]],\n",
      "      dtype=float32)), label_ids=array([1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 0,\n",
      "       1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 1, 2, 0,\n",
      "       1, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
      "       2, 1, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2,\n",
      "       1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0]), metrics={'test_loss': 1.0039211511611938, 'test_accuracy': 0.6538461538461539, 'test_precision': 0.5314611314611314, 'test_recall': 0.6538461538461539, 'test_f1': 0.5858547921410295, 'test_runtime': 7.9294, 'test_samples_per_second': 29.51, 'test_steps_per_second': 3.783})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd75bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n",
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "766c4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.2.3_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e1cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afd884e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           31\n",
       "Fitness                  14\n",
       "Bone health              14\n",
       "Diabetes                 12\n",
       "Cancer                   12\n",
       "Throat                    9\n",
       "Eye                       8\n",
       "Cardiovascular Health     8\n",
       "Skin                      8\n",
       "Hair                      7\n",
       "Ear                       6\n",
       "Women' s Health           5\n",
       "Neurological health       5\n",
       "COVID                     4\n",
       "Blood                     3\n",
       "Men's health              3\n",
       "Mental Health             3\n",
       "Vascular                  1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6477f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           20\n",
       "Skin                     16\n",
       "Bone health               7\n",
       "Blood                     6\n",
       "Muscles                   6\n",
       "Hair                      5\n",
       "Cardiovascular Health     4\n",
       "Neurological health       4\n",
       "Men's health              3\n",
       "Dental Health             3\n",
       "Vascular                  2\n",
       "COVID                     2\n",
       "Eye                       1\n",
       "Women' s Health           1\n",
       "Fitness                   1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
