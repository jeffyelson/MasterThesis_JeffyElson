{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162b4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-6274d2e8b2aaf27b\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 358.79it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-11cf639ca5c8f9ee.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.7080103359173127, 'f1': 0.6783103361821334, 'precision': 0.6621122317069292, 'recall': 0.7080103359173127}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='datasethumanattribution.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"folder\",\"filename\",\"claim\",\"label\",\"url\",\"premise\",\"category\",\"gemini_label\",\"gemini_explanation\",\"gpt4_label\",\"gpt4_rationale\",\"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "# Keep only the required columns\n",
    "columns_to_keep = [\"claim\", \"premise\", \"label\"]\n",
    "all_columns = dataset['train'].column_names\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)\n",
    "\n",
    "# Define label mappings\n",
    "label2id = {\n",
    "    \"contradiction\": 0,\n",
    "    \"entailment\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping.get(example['label'], 'neutral')\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = \"pritamdeka/PubMedBERT-MNLI-MedNLI\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='pritamdeka/PubMedBERT-MNLI-MedNLI'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Extracting claim and premise texts\n",
    "        claim = str(item['claim']) if item['claim'] is not None else \"\"\n",
    "        premise = str(item['premise']) if item['premise'] is not None else \"\"\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            premise, claim,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and add to item\n",
    "        item['input_ids'] = inputs['input_ids'].squeeze(0)\n",
    "        item['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Convert entire dataset to MediClaimDataset\n",
    "full_dataset = MediClaimDataset(dataset['train'], tokenizer_name=model_name)\n",
    "\n",
    "# Create DataLoader for the full dataset\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=8, collate_fn=lambda x: x)\n",
    "\n",
    "# Perform inference\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        # Ensure no None values in batch\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        input_ids = torch.stack([b['input_ids'] for b in batch]).to(device)\n",
    "        attention_mask = torch.stack([b['attention_mask'] for b in batch]).to(device)\n",
    "        labels = torch.stack([b['labels'] for b in batch]).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "# Map predicted labels back to their original labels\n",
    "predicted_labels = [id2label[label] for label in all_predictions]\n",
    "\n",
    "# Print the predictions and metrics\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4bcbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6274d2e8b2aaf27b\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 293.06it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-c5e1f9bf51ce625f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.5917312661498708, 'f1': 0.6445585498615652, 'precision': 0.736051296774811, 'recall': 0.5917312661498708}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='datasethumanattribution.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"folder\",\"filename\",\"claim\",\"label\",\"url\",\"premise\",\"category\",\"gemini_label\",\"gemini_explanation\",\"gpt4_label\",\"gpt4_rationale\",\"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "# Keep only the required columns\n",
    "columns_to_keep = [\"claim\", \"premise\", \"label\"]\n",
    "all_columns = dataset['train'].column_names\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)\n",
    "\n",
    "# Define label mappings\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping.get(example['label'], 'neutral')\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Extracting claim and premise texts\n",
    "        claim = str(item['claim']) if item['claim'] is not None else \"\"\n",
    "        premise = str(item['premise']) if item['premise'] is not None else \"\"\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            premise, claim,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and add to item\n",
    "        item['input_ids'] = inputs['input_ids'].squeeze(0)\n",
    "        item['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Convert entire dataset to MediClaimDataset\n",
    "full_dataset = MediClaimDataset(dataset['train'], tokenizer_name=model_name)\n",
    "\n",
    "# Create DataLoader for the full dataset\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=8, collate_fn=lambda x: x)\n",
    "\n",
    "# Perform inference\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        # Ensure no None values in batch\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        input_ids = torch.stack([b['input_ids'] for b in batch]).to(device)\n",
    "        attention_mask = torch.stack([b['attention_mask'] for b in batch]).to(device)\n",
    "        labels = torch.stack([b['labels'] for b in batch]).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "# Map predicted labels back to their original labels\n",
    "predicted_labels = [id2label[label] for label in all_predictions]\n",
    "\n",
    "# Print the predictions and metrics\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c24b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6274d2e8b2aaf27b\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 303.63it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-605dfeae61287f0b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.6834625322997416, 'f1': 0.6904926312050433, 'precision': 0.7498243660156471, 'recall': 0.6834625322997416}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='datasethumanattribution.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"folder\",\"filename\",\"claim\",\"label\",\"url\",\"premise\",\"category\",\"gemini_label\",\"gemini_explanation\",\"gpt4_label\",\"gpt4_rationale\",\"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "# Keep only the required columns\n",
    "columns_to_keep = [\"claim\", \"premise\", \"label\"]\n",
    "all_columns = dataset['train'].column_names\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)\n",
    "\n",
    "# Define label mappings\n",
    "label2id = {\n",
    "    \"contradiction\": 1,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping.get(example['label'], 'neutral')\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Extracting claim and premise texts\n",
    "        claim = str(item['claim']) if item['claim'] is not None else \"\"\n",
    "        premise = str(item['premise']) if item['premise'] is not None else \"\"\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            premise, claim,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and add to item\n",
    "        item['input_ids'] = inputs['input_ids'].squeeze(0)\n",
    "        item['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Convert entire dataset to MediClaimDataset\n",
    "full_dataset = MediClaimDataset(dataset['train'], tokenizer_name=model_name)\n",
    "\n",
    "# Create DataLoader for the full dataset\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=8, collate_fn=lambda x: x)\n",
    "\n",
    "# Perform inference\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        # Ensure no None values in batch\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        input_ids = torch.stack([b['input_ids'] for b in batch]).to(device)\n",
    "        attention_mask = torch.stack([b['attention_mask'] for b in batch]).to(device)\n",
    "        labels = torch.stack([b['labels'] for b in batch]).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall}\n",
    "\n",
    "# Map predicted labels back to their original labels\n",
    "predicted_labels = [id2label[label] for label in all_predictions]\n",
    "\n",
    "# Print the predictions and metrics\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1868ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6274d2e8b2aaf27b\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 336.19it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-c5e1f9bf51ce625f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.5813953488372093, 'f1': 0.6081182747392223, 'precision': 0.6847642957212506, 'recall': 0.5813953488372093}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='datasethumanattribution.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"folder\",\"filename\",\"claim\",\"label\",\"url\",\"premise\",\"category\",\"gemini_label\",\"gemini_explanation\",\"gpt4_label\",\"gpt4_rationale\",\"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "# Keep only the required columns\n",
    "columns_to_keep = [\"claim\", \"premise\", \"label\"]\n",
    "all_columns = dataset['train'].column_names\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)\n",
    "\n",
    "# Define label mappings\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping.get(example['label'], 'neutral')\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = 'bioformers/bioformer-8L-mnli'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Extracting claim and premise texts\n",
    "        claim = str(item['claim']) if item['claim'] is not None else \"\"\n",
    "        premise = str(item['premise']) if item['premise'] is not None else \"\"\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            premise, claim,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and add to item\n",
    "        item['input_ids'] = inputs['input_ids'].squeeze(0)\n",
    "        item['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Convert entire dataset to MediClaimDataset\n",
    "full_dataset = MediClaimDataset(dataset['train'], tokenizer_name=model_name)\n",
    "\n",
    "# Create DataLoader for the full dataset\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=8, collate_fn=lambda x: x)\n",
    "\n",
    "# Perform inference\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        # Ensure no None values in batch\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        input_ids = torch.stack([b['input_ids'] for b in batch]).to(device)\n",
    "        attention_mask = torch.stack([b['attention_mask'] for b in batch]).to(device)\n",
    "        labels = torch.stack([b['labels'] for b in batch]).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall}\n",
    "\n",
    "# Map predicted labels back to their original labels\n",
    "predicted_labels = [id2label[label] for label in all_predictions]\n",
    "\n",
    "# Print the predictions and metrics\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c41ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6274d2e8b2aaf27b\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 312.84it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-6274d2e8b2aaf27b/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-c5e1f9bf51ce625f.arrow\n",
      "Downloading: 100%|██████████| 302/302 [00:00<00:00, 665kB/s]\n",
      "Downloading: 100%|██████████| 968/968 [00:00<00:00, 2.24MB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.12MB/s]\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.52MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 253kB/s]\n",
      "Downloading: 100%|██████████| 1.25G/1.25G [02:32<00:00, 8.77MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.5387596899224806, 'f1': 0.6097898063384216, 'precision': 0.7969209362741386, 'recall': 0.5387596899224806}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set the environment variable for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='datasethumanattribution.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"folder\",\"filename\",\"claim\",\"label\",\"url\",\"premise\",\"category\",\"gemini_label\",\"gemini_explanation\",\"gpt4_label\",\"gpt4_rationale\",\"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "# Keep only the required columns\n",
    "columns_to_keep = [\"claim\", \"premise\", \"label\"]\n",
    "all_columns = dataset['train'].column_names\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)\n",
    "\n",
    "# Define label mappings\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping.get(example['label'], 'neutral')\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = 'howey/electra-large-mnli'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Extracting claim and premise texts\n",
    "        claim = str(item['claim']) if item['claim'] is not None else \"\"\n",
    "        premise = str(item['premise']) if item['premise'] is not None else \"\"\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            premise, claim,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension and add to item\n",
    "        item['input_ids'] = inputs['input_ids'].squeeze(0)\n",
    "        item['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Convert entire dataset to MediClaimDataset\n",
    "full_dataset = MediClaimDataset(dataset['train'], tokenizer_name=model_name)\n",
    "\n",
    "# Create DataLoader for the full dataset\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=8, collate_fn=lambda x: x)\n",
    "\n",
    "# Perform inference\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        # Ensure no None values in batch\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        input_ids = torch.stack([b['input_ids'] for b in batch]).to(device)\n",
    "        attention_mask = torch.stack([b['attention_mask'] for b in batch]).to(device)\n",
    "        labels = torch.stack([b['labels'] for b in batch]).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "balanced_accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "# Map predicted labels back to their original labels\n",
    "predicted_labels = [id2label[label] for label in all_predictions]\n",
    "\n",
    "\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
