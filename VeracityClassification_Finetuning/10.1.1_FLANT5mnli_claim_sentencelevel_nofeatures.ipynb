{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cfbf7584a6bb7c13\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 213.07it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv',data_files='dataset_sentenceattribution_nerfeatures_split.csv',delimiter=',',column_names=[\"claim\",\"premise\",\"label\",\"category\",\"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\",\"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\",\"url\", \"entities\",\"entity_map\",\"gem_exp\",\"gem_label\",\"gpt_label\",\"gpt_exp\",\"gold_exp\",\"entity_map_ev\",\"entity_ev\",\"synonym\",\"voice\",\"split\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133f9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-aa1fd4d2889f655e.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9cd628f4430f2ace.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-df968f39da4ad12a.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01252e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\",\"category\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-ab3c8fa265c0f20f.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-f395a7570b738078.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-cfbf7584a6bb7c13/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-90ab1fd7f46a781a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 2, 'entailment': 0, 'neutral': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 2,\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15010ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "\n",
    "        claim = item['claim']\n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences, claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "        item['input_ids'] = inputs['input_ids'].squeeze()  # Remove batch dimension\n",
    "        item['attention_mask']= inputs['attention_mask'].squeeze() # Remove batch dimension\n",
    "        \n",
    "        if 'label' in item:\n",
    "            item['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5cf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.',\n",
       " 'premise': 'Additionally, laser doppler blood flow measurement showed that the frankincense and myrrh essential oil compound could promote the elimination of capillaries from skin epidermis to dermis by increasing skin blood flow.',\n",
       " 'label': 1,\n",
       " 'category': 'General Health',\n",
       " 'input_ids': tensor([ 5433,     6,  6124,   103,  1572,  1171,  1717,  2537,  9753,  3217,\n",
       "            24,     8,     3,  6296,  2917,    75,  5167,    11,    82,    52,\n",
       "            52,   107,  1832,  1043, 12771,   228,  2519,     8, 23458,    13,\n",
       "           212, 12851,   725,    45,  1133,  9241,   588,    51,   159,    12,\n",
       "            74,    51,   159,    57,  3094,  1133,  1717,  2537,     5,     1,\n",
       "           499,    52,    52,   107,  1832,  1043,    19,  1664,   261,    16,\n",
       "         26309,   494,    12,   199,  1172,     8,  3179,    13,     8,  1133,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 42:01, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.566100</td>\n",
       "      <td>0.951321</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.621722</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.587990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>1.147301</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.642071</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.620874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.362700</td>\n",
       "      <td>1.353842</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.656274</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.643513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>2.362528</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.632819</td>\n",
       "      <td>0.638710</td>\n",
       "      <td>0.635477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>1.874204</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.633176</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.637277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>2.721908</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.653425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>2.564944</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.646222</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.638913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>2.894123</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>0.616979</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>0.615703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>3.050231</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.621505</td>\n",
       "      <td>0.624777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3.228750</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.632781</td>\n",
       "      <td>0.643011</td>\n",
       "      <td>0.636824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.351698</td>\n",
       "      <td>0.615054</td>\n",
       "      <td>0.615545</td>\n",
       "      <td>0.615054</td>\n",
       "      <td>0.614797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.476220</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.628932</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>0.628329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>3.653412</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>0.625516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.682353</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.621350</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.620240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.719499</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.621943</td>\n",
       "      <td>0.619355</td>\n",
       "      <td>0.620449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-203/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-3045] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-203] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-812/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-609] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-1421/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-1624/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-1421] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.1.1_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.1.1_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.1.1_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.1.1_flant5/checkpoint-1218 (score: 0.6580645161290323).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 14:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.1.1_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.1.1_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.1.1_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.1.1_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.1.1_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.1.1_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.1.1_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.1.1_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.1.1_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.1.1_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.1.1_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.1.1_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.1.1_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.1.1_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.1.1_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.1.1_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.1.1_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.1.1_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-2.5419655 ,  6.0125155 , -3.3343048 ],\n",
      "       [ 7.695624  , -1.9590325 , -4.326606  ],\n",
      "       [ 5.7288394 ,  0.5549578 , -5.227865  ],\n",
      "       [-2.177685  ,  1.6803061 ,  0.27634266],\n",
      "       [ 7.003108  , -2.6457677 , -3.3871539 ],\n",
      "       [ 7.906324  , -2.544994  , -4.1062846 ],\n",
      "       [ 7.465103  , -2.4745228 , -3.85182   ],\n",
      "       [ 7.824973  , -2.952928  , -3.6991918 ],\n",
      "       [ 7.508679  , -1.887596  , -4.4049892 ],\n",
      "       [-3.569597  , -1.906554  ,  4.492086  ],\n",
      "       [ 6.30013   , -3.0175009 , -2.429001  ],\n",
      "       [ 7.2325816 , -2.6862187 , -3.4397383 ],\n",
      "       [-5.2254806 ,  1.3745565 ,  2.9631495 ],\n",
      "       [ 7.951588  , -3.1051395 , -3.6000834 ],\n",
      "       [-2.1389718 ,  4.455536  , -2.3041291 ],\n",
      "       [-4.0812497 , -0.86820537,  4.167485  ],\n",
      "       [ 6.1452947 , -0.92648566, -4.2905145 ],\n",
      "       [ 7.164433  , -2.6076276 , -3.3911273 ],\n",
      "       [ 7.6353383 , -2.8018568 , -3.7042654 ],\n",
      "       [ 7.149371  , -1.7657919 , -4.171374  ],\n",
      "       [ 7.207489  , -2.1878529 , -3.8589613 ],\n",
      "       [ 4.803986  , -0.76433355, -3.362686  ],\n",
      "       [-3.2562277 ,  5.9242973 , -2.9414928 ],\n",
      "       [ 1.3467094 ,  1.002319  , -2.147452  ],\n",
      "       [-1.655207  , -1.9297216 ,  3.1997309 ],\n",
      "       [-5.1063    , -1.6024027 ,  5.607412  ],\n",
      "       [ 7.6622844 , -2.4527442 , -3.981246  ],\n",
      "       [ 1.1124731 , -0.227179  , -0.66436553],\n",
      "       [ 7.802769  , -2.8206017 , -3.7325034 ],\n",
      "       [ 3.0900679 , -2.8837748 , -0.079134  ],\n",
      "       [-3.281811  ,  5.2766995 , -2.1324873 ],\n",
      "       [ 8.166492  , -2.5835717 , -4.1788306 ],\n",
      "       [ 7.1375766 , -2.084756  , -3.8666327 ],\n",
      "       [ 2.5293117 , -2.1161397 , -0.10863274],\n",
      "       [-4.083125  ,  5.956929  , -2.046248  ],\n",
      "       [ 6.6904874 , -1.1008438 , -4.4601235 ],\n",
      "       [ 5.119073  , -0.00900299, -4.3294    ],\n",
      "       [ 7.130816  , -1.9997125 , -4.051307  ],\n",
      "       [-3.8875685 , -2.2735176 ,  5.20256   ],\n",
      "       [ 7.679207  , -2.518143  , -3.852481  ],\n",
      "       [-2.356422  ,  5.6436787 , -3.40228   ],\n",
      "       [ 7.1488338 , -2.230138  , -3.8100185 ],\n",
      "       [ 6.913542  , -2.2799542 , -3.5476782 ],\n",
      "       [-4.846629  ,  2.707333  ,  1.4816173 ],\n",
      "       [-3.8389862 ,  5.6537313 , -2.0623322 ],\n",
      "       [ 3.7125108 ,  1.7865409 , -4.5129647 ],\n",
      "       [-1.2960384 ,  4.7940254 , -3.4958525 ],\n",
      "       [ 5.892023  , -1.0418149 , -3.91992   ],\n",
      "       [ 7.449348  , -2.2698429 , -3.9678376 ],\n",
      "       [-2.7957525 , -4.6670012 ,  6.3905325 ],\n",
      "       [ 6.7657657 , -1.7191967 , -3.9895372 ],\n",
      "       [ 4.3358135 , -2.1142764 , -1.6759267 ],\n",
      "       [-1.4352106 ,  5.0681267 , -3.6361039 ],\n",
      "       [-5.818625  ,  4.1605153 ,  0.98129004],\n",
      "       [-4.5999193 ,  3.728928  ,  0.3204229 ],\n",
      "       [ 8.142626  , -2.7906168 , -4.01716   ],\n",
      "       [-3.561972  ,  3.2201657 ,  0.05326837],\n",
      "       [ 6.086261  , -0.26913145, -4.8516703 ],\n",
      "       [ 2.7542639 ,  2.344324  , -4.612034  ],\n",
      "       [ 7.665922  , -2.2932558 , -4.1887    ],\n",
      "       [ 7.197444  , -2.9933734 , -3.1894393 ],\n",
      "       [-2.7537513 , -0.61720663,  2.7585497 ],\n",
      "       [ 4.863925  ,  1.0141773 , -4.801484  ],\n",
      "       [-3.265072  ,  6.3994274 , -3.2093987 ],\n",
      "       [-1.769168  ,  3.6001208 , -1.8517272 ],\n",
      "       [ 7.91293   , -2.4966087 , -4.1195083 ],\n",
      "       [ 2.0821328 ,  2.9262054 , -4.633635  ],\n",
      "       [ 7.663128  , -2.6612566 , -3.8652365 ],\n",
      "       [ 6.497892  , -2.7723458 , -2.7958376 ],\n",
      "       [-2.009986  , -3.9423487 ,  5.131339  ],\n",
      "       [ 7.789462  , -2.617414  , -3.8570714 ],\n",
      "       [-5.9713397 ,  1.0557938 ,  3.8287475 ],\n",
      "       [ 6.450423  , -2.1300645 , -3.2308593 ],\n",
      "       [-5.274697  ,  4.284399  ,  0.3448597 ],\n",
      "       [ 5.581497  , -1.7695781 , -2.94102   ],\n",
      "       [ 6.9642615 , -2.2451088 , -3.6624775 ],\n",
      "       [ 4.5540123 , -0.96040994, -2.863961  ],\n",
      "       [ 7.336041  , -2.7103865 , -3.5090754 ],\n",
      "       [ 8.312938  , -2.9171505 , -4.0658875 ],\n",
      "       [ 7.388621  , -2.1003263 , -4.0851135 ],\n",
      "       [ 7.744009  , -3.0638041 , -3.569824  ],\n",
      "       [ 7.7479258 , -2.9401217 , -3.595862  ],\n",
      "       [ 8.01311   , -2.765094  , -3.886211  ],\n",
      "       [ 6.9832687 , -2.6293263 , -3.3628743 ],\n",
      "       [ 7.455515  , -2.4359498 , -3.962615  ],\n",
      "       [ 0.09432639,  2.9070284 , -2.6537776 ],\n",
      "       [ 6.278401  , -2.277009  , -3.0739658 ],\n",
      "       [-2.974111  ,  5.1026325 , -2.4898005 ],\n",
      "       [ 0.27970153,  3.8923266 , -3.9777498 ],\n",
      "       [ 4.279655  ,  1.1210248 , -4.4819803 ],\n",
      "       [ 6.7359776 , -1.924734  , -3.7861059 ],\n",
      "       [ 7.7102733 , -2.7986844 , -3.7109628 ],\n",
      "       [-3.5455985 ,  0.95341754,  2.0427277 ],\n",
      "       [ 6.1633186 , -1.4657655 , -3.8102932 ],\n",
      "       [ 5.6806893 ,  0.83066446, -5.4381113 ],\n",
      "       [ 7.056182  , -2.283579  , -3.6035922 ],\n",
      "       [-3.3482678 ,  4.571092  , -1.5112692 ],\n",
      "       [ 1.351747  ,  3.7365534 , -4.5893717 ],\n",
      "       [ 8.059078  , -2.4274106 , -4.39052   ],\n",
      "       [ 7.6919894 , -2.8694208 , -3.6060693 ],\n",
      "       [-2.9662993 , -5.58552   ,  7.391088  ],\n",
      "       [ 6.8832173 , -2.0896986 , -3.699852  ],\n",
      "       [ 7.5254264 , -2.8233473 , -3.6298904 ],\n",
      "       [-1.8997805 ,  4.8785224 , -3.0667598 ],\n",
      "       [ 7.517605  , -2.543726  , -3.7950513 ],\n",
      "       [ 6.8812747 , -2.41137   , -3.515893  ],\n",
      "       [ 5.6893425 , -0.42326766, -4.6263976 ],\n",
      "       [-4.4839015 ,  6.3918395 , -2.1425574 ],\n",
      "       [ 7.5315695 , -2.569983  , -3.8135533 ],\n",
      "       [-2.7777572 ,  5.4168677 , -2.7190907 ],\n",
      "       [-4.910199  ,  5.448569  , -1.0499417 ],\n",
      "       [ 8.02671   , -3.0672634 , -3.7206926 ],\n",
      "       [ 7.771005  , -2.6871333 , -3.870388  ],\n",
      "       [ 1.9226013 ,  1.6635278 , -3.16693   ],\n",
      "       [ 7.982905  , -2.7856832 , -3.9350371 ],\n",
      "       [ 7.179273  , -1.6461776 , -4.3840504 ],\n",
      "       [ 7.939583  , -2.6429985 , -3.9878724 ],\n",
      "       [ 7.213175  , -2.520821  , -3.5036893 ],\n",
      "       [ 7.635329  , -2.6971657 , -3.597809  ],\n",
      "       [ 7.6224484 , -2.380181  , -3.9955375 ],\n",
      "       [-2.4621933 ,  5.977952  , -3.5231307 ],\n",
      "       [-3.531544  ,  5.494074  , -2.180983  ],\n",
      "       [ 6.547521  , -0.11881674, -5.287008  ],\n",
      "       [-2.10467   ,  6.083446  , -3.8459601 ],\n",
      "       [-3.632182  ,  6.083078  , -2.6380804 ],\n",
      "       [ 6.305432  , -1.3033199 , -4.000369  ],\n",
      "       [-2.2941883 ,  5.2218957 , -2.9135258 ],\n",
      "       [ 7.696125  , -2.4913101 , -4.006826  ],\n",
      "       [ 7.707156  , -2.8362458 , -3.6330438 ],\n",
      "       [ 5.8331347 , -2.3548527 , -2.6554058 ],\n",
      "       [ 5.3220744 , -1.6917214 , -2.8691204 ],\n",
      "       [-2.8027165 ,  4.363887  , -1.6405253 ],\n",
      "       [ 6.862039  , -0.922513  , -4.767672  ],\n",
      "       [-4.0109787 ,  4.498811  , -0.8852072 ],\n",
      "       [ 6.8539968 , -2.5436492 , -3.2363555 ],\n",
      "       [-0.7370931 ,  4.9964232 , -4.0016894 ],\n",
      "       [ 7.5817103 , -2.698513  , -3.6980903 ],\n",
      "       [-4.291258  ,  3.6373765 ,  0.13714111],\n",
      "       [-3.1729963 ,  4.737574  , -1.7641823 ],\n",
      "       [ 7.916784  , -3.0441628 , -3.6986508 ],\n",
      "       [ 6.3123813 , -1.3585056 , -3.8259995 ],\n",
      "       [ 5.9562182 , -1.3554494 , -3.7109888 ],\n",
      "       [ 6.098397  , -1.8769891 , -3.299052  ],\n",
      "       [ 7.6991706 , -2.7264595 , -3.72893   ],\n",
      "       [ 5.872873  , -1.0525893 , -4.0148206 ],\n",
      "       [ 7.294996  , -2.3028393 , -3.7924843 ],\n",
      "       [ 2.3400352 ,  3.5671031 , -5.3489394 ],\n",
      "       [-4.2127013 , -0.7352875 ,  4.215726  ],\n",
      "       [ 5.444569  , -0.58506083, -3.9396884 ],\n",
      "       [-3.843356  ,  5.268727  , -1.7977874 ],\n",
      "       [-3.7825377 ,  5.8567023 , -2.288528  ],\n",
      "       [ 5.789444  , -0.17109863, -4.638056  ],\n",
      "       [ 7.9406304 , -2.8233993 , -3.9069788 ],\n",
      "       [ 7.868549  , -2.348203  , -4.2174706 ],\n",
      "       [ 7.7222557 , -2.1975055 , -4.2710533 ],\n",
      "       [ 7.365091  , -2.7839088 , -3.5529132 ],\n",
      "       [ 6.4550934 , -2.1760492 , -3.175058  ],\n",
      "       [-1.4901972 ,  5.3111053 , -3.7499814 ],\n",
      "       [-0.37326878,  4.212304  , -3.502563  ],\n",
      "       [-4.436684  , -2.8507998 ,  5.92846   ],\n",
      "       [-4.7771235 , -0.6485608 ,  4.5787663 ],\n",
      "       [-4.156411  , -4.4675345 ,  7.3590055 ],\n",
      "       [ 7.4277534 , -3.0158489 , -3.289316  ],\n",
      "       [-3.8570175 , -2.0461352 ,  4.971561  ],\n",
      "       [ 6.25775   , -2.6703253 , -2.6700058 ],\n",
      "       [ 7.4471903 , -2.2918975 , -3.9861948 ],\n",
      "       [ 7.536556  , -2.8718    , -3.3947704 ],\n",
      "       [-4.0847983 ,  5.3695235 , -1.4695907 ],\n",
      "       [-3.9563463 , -4.156155  ,  6.9901395 ],\n",
      "       [ 1.1236042 , -3.5054376 ,  2.1262696 ],\n",
      "       [-4.810888  , -1.2865604 ,  5.221472  ],\n",
      "       [ 6.158036  , -1.6844399 , -3.5577183 ],\n",
      "       [-4.4583282 ,  6.5761127 , -2.395208  ],\n",
      "       [ 7.746015  , -2.796955  , -3.7851624 ],\n",
      "       [ 3.4777644 ,  2.4792967 , -5.250372  ],\n",
      "       [-2.9996002 ,  5.4776692 , -2.5122287 ],\n",
      "       [ 7.1093125 , -2.2730002 , -3.6690161 ],\n",
      "       [-2.5648694 , -1.807709  ,  3.846081  ],\n",
      "       [ 5.9310975 , -1.0602497 , -4.0534835 ],\n",
      "       [-2.4027798 , -0.35279098,  2.1911142 ],\n",
      "       [-1.5833576 ,  1.3650498 ,  0.06954069],\n",
      "       [ 2.8270156 , -1.9771451 , -0.494194  ],\n",
      "       [-2.998325  ,  0.4255539 ,  2.2076607 ],\n",
      "       [-2.636117  ,  6.444814  , -3.7981915 ],\n",
      "       [-3.0901744 ,  6.2637105 , -3.1792216 ],\n",
      "       [ 7.349895  , -2.4883804 , -3.6652308 ],\n",
      "       [ 7.052622  , -1.5992966 , -4.267953  ],\n",
      "       [ 4.685745  ,  2.353277  , -6.07903   ],\n",
      "       [ 6.0639496 ,  0.34083772, -5.3983207 ],\n",
      "       [ 7.1782203 , -2.4687676 , -3.559005  ],\n",
      "       [-3.8828263 ,  5.759092  , -1.9703006 ],\n",
      "       [-3.1652453 ,  6.0244665 , -3.071782  ],\n",
      "       [ 6.7258987 , -2.1954908 , -3.575879  ],\n",
      "       [ 7.2282596 , -2.289091  , -3.8562522 ],\n",
      "       [-2.9401124 , -3.3336418 ,  5.4445863 ],\n",
      "       [ 7.8510695 , -2.8460119 , -3.784157  ],\n",
      "       [ 7.6588645 , -2.2297027 , -4.212002  ],\n",
      "       [ 7.888099  , -2.8793046 , -3.8162282 ],\n",
      "       [ 7.0693164 , -2.308531  , -3.6534336 ],\n",
      "       [ 2.1028628 , -0.6353934 , -1.1851306 ],\n",
      "       [ 7.5281854 , -2.8038857 , -3.6166027 ],\n",
      "       [ 7.1100793 , -1.4409078 , -4.391591  ],\n",
      "       [-0.77163744,  5.263943  , -4.288515  ],\n",
      "       [ 7.778148  , -2.6940215 , -3.880215  ],\n",
      "       [-3.733155  ,  4.9448524 , -1.3787673 ],\n",
      "       [-2.1331    ,  4.638356  , -2.4686859 ],\n",
      "       [ 6.5505214 ,  0.11764122, -5.6520877 ],\n",
      "       [ 7.099289  , -2.2975028 , -3.6173635 ],\n",
      "       [ 7.9841313 , -2.4791195 , -4.2702456 ],\n",
      "       [ 6.056671  ,  0.32482183, -5.1900253 ],\n",
      "       [-3.5216663 ,  4.6239114 , -1.2880514 ],\n",
      "       [-5.5847945 , -0.9010442 ,  5.240262  ],\n",
      "       [ 7.55211   , -2.9891696 , -3.3722315 ],\n",
      "       [-0.9968295 ,  5.2513795 , -4.095285  ],\n",
      "       [ 7.4848056 , -2.500224  , -3.906805  ],\n",
      "       [-2.5842583 ,  5.237095  , -2.609639  ],\n",
      "       [-3.5059228 ,  5.142713  , -1.8182772 ],\n",
      "       [-3.2925794 , -3.7937806 ,  5.885954  ],\n",
      "       [ 8.07032   , -2.905089  , -3.8450403 ],\n",
      "       [-3.9025671 ,  3.5930803 , -0.33973762],\n",
      "       [ 7.5422378 , -2.4723303 , -3.8618658 ],\n",
      "       [ 7.2685847 , -1.6037315 , -4.379795  ],\n",
      "       [ 4.020235  , -1.3186508 , -2.0938578 ],\n",
      "       [ 7.3425074 , -1.7462553 , -4.555576  ],\n",
      "       [ 6.8312144 , -2.286676  , -3.6363246 ],\n",
      "       [ 7.759266  , -2.3174    , -4.1521206 ],\n",
      "       [ 7.808124  , -2.9137459 , -3.7666695 ],\n",
      "       [-3.5593731 ,  3.3653471 , -0.23882662],\n",
      "       [ 1.4585127 ,  0.5753616 , -1.8232349 ],\n",
      "       [ 7.1794696 , -2.451605  , -3.5276058 ],\n",
      "       [ 5.485953  , -0.92264515, -3.6433685 ],\n",
      "       [ 6.9680243 , -1.829578  , -4.1209435 ],\n",
      "       [ 6.715305  , -1.5377983 , -4.1776757 ],\n",
      "       [-1.4525219 , -2.9318995 ,  3.6516762 ]], dtype=float32), array([[[-0.1521593 , -0.00888492, -0.00303369, ...,  0.0619979 ,\n",
      "         -0.0360668 , -0.00063758],\n",
      "        [-0.10569567,  0.01655   , -0.15172517, ...,  0.01599978,\n",
      "         -0.05720916, -0.02308388],\n",
      "        [-0.0277071 , -0.03208988, -0.0459178 , ..., -0.02870351,\n",
      "         -0.08941094, -0.00741504],\n",
      "        ...,\n",
      "        [ 0.05253404, -0.00060807, -0.17236044, ...,  0.28625095,\n",
      "         -0.17357838,  0.17982389],\n",
      "        [ 0.05253404, -0.00060807, -0.17236044, ...,  0.28625095,\n",
      "         -0.17357838,  0.17982389],\n",
      "        [ 0.05253404, -0.00060807, -0.17236044, ...,  0.28625095,\n",
      "         -0.17357838,  0.17982389]],\n",
      "\n",
      "       [[-0.20635217,  0.0177674 , -0.13084489, ...,  0.02111667,\n",
      "         -0.02885477,  0.04855198],\n",
      "        [ 0.04213654, -0.3036839 , -0.13833052, ..., -0.08673292,\n",
      "         -0.03233166,  0.00347919],\n",
      "        [-0.02930567, -0.12642136, -0.1412548 , ...,  0.09227686,\n",
      "         -0.01410064, -0.00372098],\n",
      "        ...,\n",
      "        [ 0.15022206,  0.10222755, -0.15589547, ...,  0.31009734,\n",
      "          0.00477684,  0.19390391],\n",
      "        [ 0.15022206,  0.10222755, -0.15589547, ...,  0.31009734,\n",
      "          0.00477684,  0.19390391],\n",
      "        [ 0.15022206,  0.10222755, -0.15589547, ...,  0.31009734,\n",
      "          0.00477684,  0.19390391]],\n",
      "\n",
      "       [[-0.19780526,  0.03254921, -0.14963482, ...,  0.11412721,\n",
      "          0.04214284, -0.12614697],\n",
      "        [-0.090771  ,  0.05222242, -0.04591619, ...,  0.0783342 ,\n",
      "          0.04069944, -0.02109665],\n",
      "        [-0.09518142, -0.04678869, -0.12290432, ...,  0.02391798,\n",
      "         -0.12248024, -0.06156239],\n",
      "        ...,\n",
      "        [-0.00600847, -0.00644734, -0.12152407, ...,  0.23619401,\n",
      "          0.05584596, -0.02378733],\n",
      "        [-0.00600847, -0.00644734, -0.12152407, ...,  0.23619401,\n",
      "          0.05584596, -0.02378733],\n",
      "        [-0.00600847, -0.00644734, -0.12152407, ...,  0.23619401,\n",
      "          0.05584596, -0.02378733]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.0378831 ,  0.1750008 , -0.18631336, ..., -0.06886917,\n",
      "          0.07554659,  0.00601916],\n",
      "        [ 0.01444317,  0.03181558,  0.03767212, ...,  0.0295022 ,\n",
      "          0.08790568,  0.25774744],\n",
      "        [ 0.17276041, -0.23049234,  0.05870172, ..., -0.09814267,\n",
      "         -0.08865507,  0.16347298],\n",
      "        ...,\n",
      "        [-0.23221399, -0.13448149, -0.09323928, ...,  0.36048463,\n",
      "         -0.09553278,  0.26029637],\n",
      "        [-0.23221399, -0.13448149, -0.09323928, ...,  0.36048463,\n",
      "         -0.09553278,  0.26029637],\n",
      "        [-0.23221399, -0.13448149, -0.09323928, ...,  0.36048463,\n",
      "         -0.09553278,  0.26029637]],\n",
      "\n",
      "       [[-0.06901141,  0.01330735, -0.13548632, ...,  0.20596595,\n",
      "         -0.15419783, -0.04764859],\n",
      "        [ 0.01326698, -0.10462813, -0.1238654 , ...,  0.14106648,\n",
      "         -0.07826935,  0.22682405],\n",
      "        [ 0.05450113, -0.13697563, -0.09257205, ...,  0.02472442,\n",
      "         -0.02994603,  0.07756609],\n",
      "        ...,\n",
      "        [ 0.0347812 , -0.0571679 , -0.0139498 , ...,  0.14570478,\n",
      "         -0.2851314 ,  0.03160767],\n",
      "        [ 0.0347812 , -0.0571679 , -0.0139498 , ...,  0.14570478,\n",
      "         -0.2851314 ,  0.03160767],\n",
      "        [ 0.0347812 , -0.0571679 , -0.0139498 , ...,  0.14570478,\n",
      "         -0.2851314 ,  0.03160767]],\n",
      "\n",
      "       [[-0.11377117, -0.19522195, -0.1778752 , ...,  0.0374837 ,\n",
      "          0.18687427, -0.3119136 ],\n",
      "        [ 0.00862061,  0.08067537, -0.21065098, ..., -0.06463715,\n",
      "          0.07741741, -0.01826631],\n",
      "        [-0.0521838 ,  0.0269403 , -0.17502326, ..., -0.2115099 ,\n",
      "          0.18977185, -0.12804545],\n",
      "        ...,\n",
      "        [ 0.07894313,  0.170087  , -0.08570307, ..., -0.01438102,\n",
      "          0.03045656, -0.00880672],\n",
      "        [ 0.07894313,  0.170087  , -0.08570307, ..., -0.01438102,\n",
      "          0.03045656, -0.00880672],\n",
      "        [ 0.07894313,  0.170087  , -0.08570307, ..., -0.01438102,\n",
      "          0.03045656, -0.00880672]]], dtype=float32)), label_ids=array([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 0,\n",
      "       1, 0, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2]), metrics={'test_loss': 2.443057060241699, 'test_accuracy': 0.6752136752136753, 'test_precision': 0.6862444800297907, 'test_recall': 0.6752136752136753, 'test_f1': 0.6665692854011658, 'test_runtime': 7.6349, 'test_samples_per_second': 30.649, 'test_steps_per_second': 3.929})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "808dd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48fb2163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed6cfde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")  # Adjust xticklabels and yticklabels as needed\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "346c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = test_data.__getitem__(idx)\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.1.1_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4543aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad099d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           41\n",
       "Bone health              13\n",
       "Fitness                  11\n",
       "Diabetes                 10\n",
       "Cancer                   10\n",
       "Skin                     10\n",
       "Hair                      9\n",
       "Cardiovascular Health     7\n",
       "Throat                    6\n",
       "Neurological health       6\n",
       "Ear                       6\n",
       "Eye                       6\n",
       "Blood                     5\n",
       "Muscles                   4\n",
       "COVID                     3\n",
       "Mental Health             3\n",
       "Women' s Health           3\n",
       "Men's health              3\n",
       "Vascular                  2\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d45817c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skin                     14\n",
       "General Health           10\n",
       "Bone health               8\n",
       "Cardiovascular Health     5\n",
       "Blood                     4\n",
       "Fitness                   4\n",
       "Men's health              3\n",
       "Dental Health             3\n",
       "Throat                    3\n",
       "COVID                     3\n",
       "Hair                      3\n",
       "Neurological health       3\n",
       "Eye                       3\n",
       "Women' s Health           3\n",
       "Muscles                   2\n",
       "Cancer                    2\n",
       "Diabetes                  2\n",
       "Vascular                  1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e54a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
