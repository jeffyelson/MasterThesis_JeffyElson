{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,balanced_accuracy_score,precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65ca0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ed78e0185729396\n",
      "Reusing dataset csv (/home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "100%|██████████| 1/1 [00:00<00:00, 245.74it/s]\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-d8a73572c5526b09.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-71c34de47dd068c7.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-9e4fe1ebeeeb9fd4.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files='dataset_semanticattribution_nerfeatures_split.csv',\n",
    "    delimiter=',',\n",
    "    column_names=[\n",
    "        \"claim\", \"premise\", \"label\", \"category\", \"count_bf\", \"count_ca\", \"count_dis\",\n",
    "        \"count_food\", \"count_lipid\", \"count_treat\", \"pres_bf\", \"pres_ca\", \"pres_dis\",\n",
    "        \"pres_food\", \"pres_lipid\", \"pres_treat\", \"counte_bf\", \"counte_ca\", \"counte_dis\",\n",
    "        \"counte_food\", \"counte_lipid\", \"counte_treat\", \"prese_bf\", \"prese_ca\", \"prese_dis\",\n",
    "        \"prese_food\", \"prese_lipid\", \"prese_treat\", \"url\", \"entities\", \"entity_map\",\n",
    "        \"gold_exp\", \"gemini_exp\", \"gemini_label\",\"entity_ev\",\"entity_map_ev\", \"split\"\n",
    "    ],\n",
    "    skiprows=1\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
    "validation_dataset = dataset['train'].filter(lambda example: example['split'] == 'validation')\n",
    "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d368dccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 1623\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 465\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'premise', 'label', 'category', 'count_bf', 'count_ca', 'count_dis', 'count_food', 'count_lipid', 'count_treat', 'pres_bf', 'pres_ca', 'pres_dis', 'pres_food', 'pres_lipid', 'pres_treat', 'counte_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat', 'url', 'entities', 'entity_map', 'gold_exp', 'gemini_exp', 'gemini_label', 'entity_ev', 'entity_map_ev', 'split'],\n",
       "        num_rows: 234\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f1db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"claim\", \"premise\", \"label\", \"category\",\"count_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "all_columns = dataset[\"train\"].column_names\n",
    "\n",
    "columns_to_drop = [col for col in all_columns if col not in columns_to_keep]\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae50f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-412796ab84426da4.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-b8902013ceb6a5e1.arrow\n",
      "Loading cached processed dataset at /home/elson/.cache/huggingface/datasets/csv/default-9ed78e0185729396/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-84282dd8005661f7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping: {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "label2id = {\n",
    "    \"contradiction\": 0,\n",
    "    \"entailment\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_mapping = {\n",
    "    'SUPPORTED': 'entailment',\n",
    "    'REFUTED': 'contradiction',\n",
    "    'NOT ENOUGH INFORMATION': 'neutral'\n",
    "}\n",
    "\n",
    "def map_and_encode_labels(example):\n",
    "    # Map original dataset labels to new labels ('entailment', 'contradiction', 'neutral')\n",
    "    mapped_label = label_mapping[example['label']]\n",
    "    # Encode mapped labels using label2id\n",
    "    example['label'] = label2id[mapped_label]\n",
    "    return example\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_and_encode_labels)\n",
    "\n",
    "\n",
    "# Show the label encoding mapping\n",
    "print(\"Label Encoding Mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7095b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b93ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.utils.data\n",
    "\n",
    "class MediClaimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer_name='sjrhuschlee/flan-t5-base-mnli'):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # Ensure idx is an integer\n",
    "        item = self.dataset[idx]  # Access the dataset item at idx\n",
    "        \n",
    "        # Extracting claim and evidence texts\n",
    "        claim = item['claim'].lower() \n",
    "        evidences = item['premise']\n",
    "        item['premise']=evidences\n",
    "        item['claim']=claim\n",
    "        additional_features_evidence = [\n",
    "            \"counte_bf\",\"counte_ca\",\"counte_dis\",\"counte_food\",\"counte_lipid\",\"counte_treat\",\"prese_bf\",\"prese_ca\",\"prese_dis\",\"prese_food\",\"prese_lipid\",\"prese_treat\"]\n",
    "    \n",
    "        for feature in additional_features_evidence:\n",
    "            if feature in item:\n",
    "                evidences += \"[SEP]\" + str(item[feature])\n",
    "        additional_features = [\n",
    "            \"count_bf\",\"count_ca\",\"count_dis\",\"count_food\",\"count_lipid\",\"count_treat\",\"pres_bf\",\"pres_ca\",\"pres_dis\",\"pres_food\",\"pres_lipid\",\"pres_treat\"]\n",
    "    \n",
    "        for feature_ev in additional_features:\n",
    "            if feature_ev in item:\n",
    "                claim += \"[SEP]\" + str(item[feature_ev])\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            evidences,claim,\n",
    "            return_tensors=\"pt\",  # Ensure PyTorch tensors are returned\n",
    "            padding='max_length',  # Apply padding to the maximum length\n",
    "            truncation='longest_first',  # Truncate to the maximum length if necessary\n",
    "            max_length=512,  # Specify the maximum length\n",
    "            add_special_tokens=True  # Add special tokens like [CLS], [SEP]\n",
    "        )\n",
    "        \n",
    "         # Construct the output item\n",
    "        output_item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),  # Remove batch dimension\n",
    "            'claim': claim,  # Include augmented claim text\n",
    "            'evidences': evidences  # Include original evidence text\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            output_item['label'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return output_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Available GPUs:\n",
      "GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62421eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (gelu_act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sjrhuschlee/flan-t5-base-mnli\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,ignore_mismatched_sizes=True,config=config, trust_remote_code=True)\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377ee0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(\n",
    "        logits, tuple\n",
    "    ):  # if the model also returns hidden_states or attentions\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd17ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim', 'premise', 'label', 'category', 'count_bf', 'counte_ca', 'counte_dis', 'counte_food', 'counte_lipid', 'counte_treat', 'prese_bf', 'prese_ca', 'prese_dis', 'prese_food', 'prese_lipid', 'prese_treat'],\n",
       "    num_rows: 1623\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c2b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Checking GPU memory, making sure to reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6efe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CUDA device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: GPU {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "614620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['val']\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f97ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = MediClaimDataset(train_data)\n",
    "vdata = MediClaimDataset(eval_data)\n",
    "test_data = MediClaimDataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c5e0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    3,   117,   597,  9789,    23,     2,     6,   445,     5,   117,\n",
       "         18150,  2168,     2,     6,    27,     5,   117,  3049,     9,     2,\n",
       "          9789,    23,     2,     6,   276,     5,   117,   350, 31371,  9881,\n",
       "             2,     6,   283,     5,   117, 11469,    32,  1924,  2099,     2,\n",
       "             6,   411,     5,   117,   584,  1598,    32,  1924,  2099,     2,\n",
       "             6,   446,     5,  4937,    77,    75,  5167,    11,    82,    52,\n",
       "            52,   107,  1832, 10229,    11,  5958,    16,    75,  5167, 11349,\n",
       "            15,   581,  2179,    18,    77, 29884,     7,    13,  8802,  4900,\n",
       "         21128,     7,     5, 23518,    15,    15,    51,     6,    27,     5,\n",
       "             3, 16977,   235, 14676,     3,   184, 12206,  1055,     7,    13,\n",
       "          8054,    52,   954,  8511,    23,    41,     5, 20919,    23,    26,\n",
       "          1528,  2189,    11, 17133, 14367,  1951,    13,   991,    11,    70,\n",
       "         28661,   257,    28,    82,    52,    52,   107,    41, 10205,    23,\n",
       "         19968,     9,     3,  4641,  4641,    61,     3,    15,  4115,  1938,\n",
       "             5, 17135,  6067,     7,    10,  9222,   138, 23482,     7,    21,\n",
       "         13038,  2686,     5,  3643,  9669,  4331,    17,    63,     6,   445,\n",
       "             5,   117, 13329,  2152,   122,     6,   205,     5,   117,   901,\n",
       "          4331,   457,     6,   446,     5,   117, 21263,    40,     7,     6,\n",
       "           411,     5,   117, 11147,     6,   283,     5, 10060,  8009,    57,\n",
       "          6869,  7554,    10,   205,     5,    82,    52,    52,  1024,  6067,\n",
       "         13262,   302,  3068,  8527,     5,  1626,  6983, 13399,     6,   391,\n",
       "             5,   117,  1626,  6983, 13399,     6,   180,     5,   117,  1626,\n",
       "          6983, 13399,     6,   283,     5,   117, 19669,    40,  1665,     6,\n",
       "           283,     5,  4937,    77,    75,  5167,    41,     2,   391,     2,\n",
       "             3,     4,    23,     2,  1725,   117,     5,  3244,    61,    10,\n",
       "          1029,     8,  1801,    13,  1435,  1564,    12,     8,  3714, 30512,\n",
       "         10896,    21,     8,  9793,    11,  1058,    13,  2261,  6716,     5,\n",
       "         21299,    11, 17133, 22763,  6546,  1756,    13,     3,  6296,  2917,\n",
       "            75,  5167,  1043,     5,  2570,  4718,     7,     3,  6443,  1491,\n",
       "             7,  2091,    23,     9,  5819,     7,     5,     3,   117, 10078,\n",
       "             6,   262,     5,  4937,    77,    75,  5167,    11,    82,    52,\n",
       "            52,   107,    38, 20203,    16,   502,     5,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,  6306,   134,  8569,   908,   632,  6306,   134,  8569,\n",
       "           908,   632,     1,    82,    52,    52,   107,  1832,  1043,    19,\n",
       "          1664,   261,    16, 26309,   494,    12,   199,  1172,     8,  3179,\n",
       "            13,     8,  1133,     5,  6306,   134,  8569,   908,   632,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'claim': 'myrrh essential oil is sometimes used in skincare products to help improve the appearance of the skin.[SEP]0',\n",
       " 'evidences': '; Unković, N.; Dimkić, I.; Janaćković, P.; Gavrilović, M.; Stanojević, O.; Vukojević, J. Frankincense and myrrh essential oils and burn incense fume against micro-inhabitants of sacral ambients.\\nShameem, I. Phytochemical & therapeutic potentials of Murr makki (.\\nOxidative stress and immunotoxic effects of lead and their amelioration with myrrh (Commiphora molmol) emulsion.\\nEssential Oils: Magical Ingredients for Skin Care.\\nChakravarty, N.; Kellogg, C.; Alvarez, J.; Equils, O.; Morgan, M. UV Protection by Natural Products: C. myrrha Oil Versus Sunscreen.\\nHamidpour, R.; Hamidpour, S.; Hamidpour, M.; Shahlari, M. Frankincense (乳香 Rǔ Xiāng;.\\nspecies): From the selection of traditional applications to the novel phytotherapy for the prevention and treatment of serious diseases.\\nChemistry and immunomodulatory activity of frankincense oil.\\nCompositions containing Boswellia extracts.\\n; Cooper, E. Frankincense and myrrh as remedies in children.[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0[SEP]0',\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4cd4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1623\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 41:55, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.733342</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.603492</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.571522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.736291</td>\n",
       "      <td>0.668817</td>\n",
       "      <td>0.670687</td>\n",
       "      <td>0.668817</td>\n",
       "      <td>0.669705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401400</td>\n",
       "      <td>1.023154</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.682048</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.669022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>1.796531</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.666829</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.643557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>2.153953</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.681150</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.655266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.223039</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.680351</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>0.657198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>2.391180</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.683534</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.690585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>2.697554</td>\n",
       "      <td>0.660215</td>\n",
       "      <td>0.696341</td>\n",
       "      <td>0.660215</td>\n",
       "      <td>0.673373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>2.714968</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.697473</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.679086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>2.661857</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.684602</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.684209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>2.816989</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.684551</td>\n",
       "      <td>0.673118</td>\n",
       "      <td>0.676268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>2.961219</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.692750</td>\n",
       "      <td>0.670968</td>\n",
       "      <td>0.679450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>3.111374</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.680699</td>\n",
       "      <td>0.664516</td>\n",
       "      <td>0.670209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>3.131031</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.678712</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.661938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>3.069640</td>\n",
       "      <td>0.668817</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.668817</td>\n",
       "      <td>0.675794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-203\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-203/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-203/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-406\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-406/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-406/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-609\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-609/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-609/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-406] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-812\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-812/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-812/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-609] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-1015\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-1015/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-1218\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-1218/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-1218/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-1015] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-1421\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-1421/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-1421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-203] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-1218] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-1624\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-1624/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-1624/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-1827\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-1827/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-1827/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-1624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-2030\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-2030/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-2030/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-1827] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-2233\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-2233/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-2233/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-2030] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-2436\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-2436/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-2436/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-2233] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-2639\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-2639/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-2639/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-2436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-2842\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-2842/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-2842/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-2639] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /home/elson/10.2.5_flant5/checkpoint-3045\n",
      "Configuration saved in /home/elson/10.2.5_flant5/checkpoint-3045/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/checkpoint-3045/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/elson/10.2.5_flant5/checkpoint-2842] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/elson/10.2.5_flant5/checkpoint-1421 (score: 0.6989247311827957).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 465\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 05:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/elson/10.2.5_flant5/best_model/config.json\n",
      "Model weights saved in /home/elson/10.2.5_flant5/best_model/pytorch_model.bin\n",
      "tokenizer config file saved in /home/elson/10.2.5_flant5/best_model/tokenizer_config.json\n",
      "Special tokens file saved in /home/elson/10.2.5_flant5/best_model/special_tokens_map.json\n",
      "Copy vocab file to /home/elson/10.2.5_flant5/best_model/spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/elson/10.2.5_flant5/best_model/tokenizer_config.json',\n",
       " '/home/elson/10.2.5_flant5/best_model/special_tokens_map.json',\n",
       " '/home/elson/10.2.5_flant5/best_model/spiece.model',\n",
       " '/home/elson/10.2.5_flant5/best_model/added_tokens.json',\n",
       " '/home/elson/10.2.5_flant5/best_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'/home/elson/10.2.5_flant5/',\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tdata,\n",
    "    eval_dataset=vdata,\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")\n",
    "\n",
    "# Training and Evaluation\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate(vdata)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained(f'/home/elson/10.2.5_flant5/best_model')\n",
    "tokenizer.save_pretrained(f'/home/elson/10.2.5_flant5/best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63deaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elson/10.2.5_flant5/best_model/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/elson/10.2.5_flant5/best_model/\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForSequenceClassification\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_t5seq.T5ForSequenceClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "loading weights file /home/elson/10.2.5_flant5/best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForSequenceClassification.\n",
      "\n",
      "All the weights of T5ForSequenceClassification were initialized from the model checkpoint at /home/elson/10.2.5_flant5/best_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 234\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/elson/10.2.5_flant5/best_model/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a7b8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-2.225087  ,  7.2111335 , -4.7911916 ],\n",
      "       [-3.072118  ,  6.112519  , -2.9862366 ],\n",
      "       [-2.8658624 ,  7.6425295 , -4.7454376 ],\n",
      "       [ 6.479827  ,  0.65985084, -5.2721534 ],\n",
      "       [-3.0693095 ,  7.1308146 , -4.364548  ],\n",
      "       [-3.3565924 ,  7.954085  , -4.3748612 ],\n",
      "       [-2.9976988 ,  8.013835  , -4.7382474 ],\n",
      "       [-4.536298  ,  7.4050794 , -3.0482183 ],\n",
      "       [-3.8363643 ,  7.329     , -3.690856  ],\n",
      "       [-3.5613792 ,  7.218176  , -3.7632065 ],\n",
      "       [-2.8647573 ,  7.3219876 , -4.2366595 ],\n",
      "       [-4.2478957 ,  5.5297093 , -1.5628564 ],\n",
      "       [-2.384366  ,  6.110741  , -3.7123528 ],\n",
      "       [-4.617278  ,  7.537024  , -3.0258024 ],\n",
      "       [-5.0349846 ,  3.3692048 ,  0.8137714 ],\n",
      "       [-0.1719101 ,  6.0903788 , -5.273113  ],\n",
      "       [-3.9397738 ,  7.157637  , -3.3161452 ],\n",
      "       [-3.387957  , -0.5461362 ,  3.3909726 ],\n",
      "       [-3.210748  ,  7.3331656 , -3.920751  ],\n",
      "       [-2.7440355 ,  6.843337  , -3.9788487 ],\n",
      "       [-2.9481747 ,  7.127564  , -4.3733563 ],\n",
      "       [-2.6659007 ,  7.816551  , -5.192703  ],\n",
      "       [-1.5216931 ,  4.4592605 , -2.980759  ],\n",
      "       [ 3.5758705 , -2.0043743 , -1.1321276 ],\n",
      "       [ 7.5815787 , -3.7220984 , -3.1709073 ],\n",
      "       [ 7.8150125 , -3.2212458 , -3.428906  ],\n",
      "       [-3.449049  ,  7.673078  , -4.1161027 ],\n",
      "       [-3.8830738 ,  6.5193357 , -2.9662008 ],\n",
      "       [-3.1294932 ,  7.4648705 , -3.9125955 ],\n",
      "       [-3.318357  ,  7.802818  , -4.2918043 ],\n",
      "       [-4.0001955 ,  3.217861  ,  0.41760477],\n",
      "       [-4.595611  ,  7.4274845 , -2.8731556 ],\n",
      "       [-2.4667807 ,  7.3573513 , -4.876251  ],\n",
      "       [-4.3534436 ,  6.6244116 , -2.6240616 ],\n",
      "       [-3.4033995 ,  7.656825  , -4.351151  ],\n",
      "       [-3.4313767 ,  7.6375523 , -4.173022  ],\n",
      "       [-2.6495032 ,  6.6630154 , -3.9796367 ],\n",
      "       [-3.0442784 ,  7.6491337 , -4.3542647 ],\n",
      "       [ 7.3381977 , -1.3531691 , -4.6542664 ],\n",
      "       [-3.6585605 ,  6.7985497 , -3.1348689 ],\n",
      "       [-2.4788425 ,  7.1301126 , -4.561802  ],\n",
      "       [-3.5075161 ,  7.315177  , -3.7534575 ],\n",
      "       [-2.6583633 ,  5.949993  , -3.3033948 ],\n",
      "       [-3.538611  ,  6.5735464 , -3.081312  ],\n",
      "       [-3.6522975 ,  7.831021  , -4.331074  ],\n",
      "       [-3.1585271 ,  7.8983364 , -4.364909  ],\n",
      "       [-2.5573804 ,  6.017776  , -3.503727  ],\n",
      "       [-3.2376385 ,  6.3252387 , -3.0545006 ],\n",
      "       [-3.0158377 ,  7.7007036 , -4.7125    ],\n",
      "       [ 7.7384634 , -1.5693542 , -4.6903214 ],\n",
      "       [-2.4805493 ,  7.1714597 , -4.899934  ],\n",
      "       [-4.4027123 ,  6.3312564 , -2.3432353 ],\n",
      "       [-3.063738  ,  6.785189  , -3.844867  ],\n",
      "       [-3.0443785 ,  7.3796654 , -4.214336  ],\n",
      "       [ 1.5392717 ,  3.520225  , -4.2181892 ],\n",
      "       [-2.332777  ,  6.9098234 , -4.1089525 ],\n",
      "       [-3.17975   ,  6.449873  , -3.5150757 ],\n",
      "       [-4.207608  ,  5.871892  , -2.0688314 ],\n",
      "       [-2.812554  ,  6.921279  , -3.8911111 ],\n",
      "       [-3.4305284 ,  7.474607  , -4.0036387 ],\n",
      "       [-3.0426967 ,  7.8084927 , -4.5331054 ],\n",
      "       [ 7.4165096 , -1.4764388 , -4.4072084 ],\n",
      "       [-4.708889  ,  6.2365046 , -1.9681036 ],\n",
      "       [-3.0421014 ,  7.2735977 , -4.5030313 ],\n",
      "       [-5.136888  ,  5.0113745 , -0.38194424],\n",
      "       [-2.7453704 ,  7.948428  , -5.1296506 ],\n",
      "       [-2.510302  ,  6.6725593 , -3.288142  ],\n",
      "       [-3.5371037 ,  7.4217267 , -3.6587272 ],\n",
      "       [-2.811174  ,  7.433079  , -4.305347  ],\n",
      "       [ 7.6011267 , -2.1935644 , -4.1616354 ],\n",
      "       [-2.6424067 ,  6.479665  , -4.1165113 ],\n",
      "       [-3.522345  ,  6.3073044 , -2.980934  ],\n",
      "       [-5.216053  ,  5.554197  , -0.99201274],\n",
      "       [-3.78278   ,  6.395523  , -2.9774804 ],\n",
      "       [ 4.0231376 ,  3.643174  , -6.6891103 ],\n",
      "       [ 3.9036798 ,  0.7489586 , -3.9035573 ],\n",
      "       [-3.339105  ,  7.5623593 , -4.329183  ],\n",
      "       [-3.144959  ,  8.041228  , -4.5397763 ],\n",
      "       [-2.1869066 ,  6.6945524 , -4.282214  ],\n",
      "       [ 6.236085  ,  0.6184306 , -5.00411   ],\n",
      "       [-3.2939565 ,  7.6186132 , -4.2229624 ],\n",
      "       [-3.3900661 ,  7.4909697 , -3.9004576 ],\n",
      "       [-3.3395505 ,  7.572619  , -4.102299  ],\n",
      "       [-3.3281288 ,  7.305083  , -3.7055125 ],\n",
      "       [-3.178438  ,  7.237997  , -3.9014711 ],\n",
      "       [-1.908089  ,  1.4011364 ,  0.4405728 ],\n",
      "       [-2.980116  ,  7.4748917 , -4.2656193 ],\n",
      "       [-3.5204163 ,  6.33695   , -3.1816208 ],\n",
      "       [-2.8343055 ,  7.7928267 , -4.8595057 ],\n",
      "       [-4.0135593 ,  5.777143  , -1.9673663 ],\n",
      "       [-3.020781  ,  7.80805   , -4.6146617 ],\n",
      "       [-4.465285  ,  4.926116  , -0.95974505],\n",
      "       [-2.5138097 ,  7.7736015 , -5.065663  ],\n",
      "       [-3.4435825 ,  6.8840094 , -3.5370889 ],\n",
      "       [-2.4129436 ,  6.6176867 , -3.9794276 ],\n",
      "       [-2.3337276 ,  5.332248  , -3.0433083 ],\n",
      "       [-1.259847  ,  6.480757  , -5.1913033 ],\n",
      "       [-2.8632343 ,  7.1140833 , -4.0741262 ],\n",
      "       [-2.4754472 ,  6.6188374 , -4.166083  ],\n",
      "       [-2.9171722 ,  4.0316653 , -1.313423  ],\n",
      "       [ 7.8602953 , -1.3064861 , -4.9575973 ],\n",
      "       [-3.3686852 ,  4.8039045 , -1.5140088 ],\n",
      "       [-3.644469  ,  7.8408604 , -4.107244  ],\n",
      "       [-4.324081  ,  6.976438  , -2.8550553 ],\n",
      "       [-2.936409  ,  7.4597535 , -4.1221657 ],\n",
      "       [-4.37171   ,  6.5100365 , -2.7140524 ],\n",
      "       [-2.7803948 ,  6.667203  , -3.9213305 ],\n",
      "       [ 0.21538493,  2.7267911 , -2.5700233 ],\n",
      "       [-4.152522  ,  6.8377175 , -2.9260507 ],\n",
      "       [-3.1938176 ,  7.737617  , -4.2451425 ],\n",
      "       [-4.304081  ,  4.102236  , -0.33483732],\n",
      "       [-4.697483  ,  7.3192573 , -2.8977191 ],\n",
      "       [-3.6338427 ,  7.209236  , -3.465346  ],\n",
      "       [-3.4336035 ,  7.634745  , -4.399431  ],\n",
      "       [-3.3061364 ,  6.97714   , -3.753212  ],\n",
      "       [-2.7000346 ,  7.446257  , -4.159251  ],\n",
      "       [-3.0078347 ,  7.502096  , -4.228398  ],\n",
      "       [ 4.472891  , -2.6687284 , -1.336032  ],\n",
      "       [-2.5158842 ,  6.8595347 , -3.974251  ],\n",
      "       [-2.5199795 ,  7.663132  , -4.948105  ],\n",
      "       [-3.9390554 ,  7.3673697 , -3.4034255 ],\n",
      "       [-3.0573037 ,  7.7336044 , -4.5164533 ],\n",
      "       [-4.272524  ,  6.416851  , -2.7662468 ],\n",
      "       [-3.3048062 ,  7.586393  , -3.9372523 ],\n",
      "       [-3.221074  ,  7.802675  , -4.2417326 ],\n",
      "       [-3.7092063 ,  7.7595263 , -3.8425095 ],\n",
      "       [-3.074968  ,  6.6483088 , -3.3807895 ],\n",
      "       [-3.0639064 ,  6.7254505 , -3.7303686 ],\n",
      "       [-3.5968723 ,  6.9778695 , -3.5699503 ],\n",
      "       [-3.4328594 ,  6.8807497 , -3.593717  ],\n",
      "       [-1.7567637 ,  5.3574834 , -3.4110558 ],\n",
      "       [ 7.6979547 , -1.0096956 , -4.928512  ],\n",
      "       [-3.1574533 ,  7.784594  , -4.2089367 ],\n",
      "       [-3.0337348 ,  7.239161  , -4.155943  ],\n",
      "       [-3.58569   ,  5.9756126 , -2.556907  ],\n",
      "       [-2.9912264 ,  6.9570723 , -3.9002934 ],\n",
      "       [-2.3246791 ,  6.8371987 , -4.433007  ],\n",
      "       [ 7.4596505 , -1.4740279 , -4.3506374 ],\n",
      "       [ 1.532504  ,  2.2114096 , -3.1423116 ],\n",
      "       [-3.7744114 ,  7.847323  , -4.0628953 ],\n",
      "       [-3.1685631 ,  6.1406794 , -2.770831  ],\n",
      "       [-3.5121455 ,  4.38314   , -1.2870934 ],\n",
      "       [-4.440413  ,  6.20412   , -2.1083689 ],\n",
      "       [-3.5945458 ,  7.484239  , -4.046507  ],\n",
      "       [-1.6707265 ,  5.5955763 , -3.7416658 ],\n",
      "       [-4.1560097 ,  3.20516   ,  0.3485886 ],\n",
      "       [-2.7601373 ,  7.36806   , -4.51143   ],\n",
      "       [ 7.675565  , -1.0877515 , -4.9502363 ],\n",
      "       [-2.9591262 ,  7.6190515 , -4.4955025 ],\n",
      "       [-2.6336215 ,  2.2979739 ,  0.20577767],\n",
      "       [-3.164444  ,  6.963225  , -3.7157838 ],\n",
      "       [-2.9647856 ,  7.797848  , -4.658712  ],\n",
      "       [-2.9525492 ,  7.632357  , -4.3823333 ],\n",
      "       [-2.7835088 ,  6.9069815 , -4.1793265 ],\n",
      "       [-3.6787472 ,  7.8433337 , -4.2593966 ],\n",
      "       [-3.9182818 ,  7.276685  , -3.4821782 ],\n",
      "       [-3.4859746 ,  7.813087  , -4.1138177 ],\n",
      "       [-3.0835106 ,  7.612527  , -4.4462957 ],\n",
      "       [ 4.594496  , -2.5548038 , -1.6108375 ],\n",
      "       [-2.831445  ,  6.2209296 , -3.2789686 ],\n",
      "       [ 7.804182  , -1.9088182 , -4.216734  ],\n",
      "       [ 6.244181  , -0.13620976, -4.547167  ],\n",
      "       [-2.9011657 ,  6.9679546 , -3.995471  ],\n",
      "       [ 2.2756267 , -1.8517272 , -0.2023136 ],\n",
      "       [-2.559084  ,  5.2956467 , -2.8232434 ],\n",
      "       [-2.2394238 ,  7.0475655 , -4.911572  ],\n",
      "       [-4.7243204 ,  5.3475323 , -1.4300402 ],\n",
      "       [-4.4975724 ,  4.7451243 , -0.8077594 ],\n",
      "       [ 7.604478  , -0.9832123 , -4.9943323 ],\n",
      "       [ 7.311051  , -2.224467  , -3.7149749 ],\n",
      "       [ 7.471235  , -1.4012424 , -4.4640727 ],\n",
      "       [-2.646733  ,  7.526805  , -4.6131177 ],\n",
      "       [ 3.362194  ,  0.38579732, -3.103664  ],\n",
      "       [-3.0725622 ,  7.90991   , -4.6024423 ],\n",
      "       [-3.7095122 ,  7.7033453 , -3.9413037 ],\n",
      "       [-4.290727  , -0.9694631 ,  4.642666  ],\n",
      "       [-3.1218176 ,  7.7673016 , -4.5635796 ],\n",
      "       [ 7.781432  , -3.269396  , -3.445252  ],\n",
      "       [-2.7990367 ,  7.7614613 , -4.5934343 ],\n",
      "       [ 7.1331725 , -1.5996377 , -4.1453843 ],\n",
      "       [-2.715365  ,  6.73103   , -3.8751104 ],\n",
      "       [-2.0234776 ,  2.479298  , -0.47959659],\n",
      "       [ 7.4007373 , -1.0573688 , -4.7189226 ],\n",
      "       [-2.443451  ,  7.067622  , -4.335841  ],\n",
      "       [-3.6710172 ,  7.7047215 , -3.9065704 ],\n",
      "       [-3.171919  ,  6.6918044 , -3.3677928 ],\n",
      "       [-2.7133157 ,  5.387899  , -2.634037  ],\n",
      "       [-3.7298667 ,  8.170999  , -4.159096  ],\n",
      "       [-2.6027532 ,  6.840793  , -4.514917  ],\n",
      "       [-3.0795557 ,  7.274455  , -3.878408  ],\n",
      "       [-4.2181354 ,  6.923187  , -2.6610973 ],\n",
      "       [-2.1846113 ,  5.369892  , -3.1472518 ],\n",
      "       [-2.5268571 ,  7.454192  , -4.363085  ],\n",
      "       [-4.674812  ,  5.383362  , -1.2887644 ],\n",
      "       [ 6.966373  , -0.5387585 , -5.1319456 ],\n",
      "       [-4.1449084 ,  7.496176  , -3.2983656 ],\n",
      "       [-2.8467002 ,  7.807461  , -5.0341945 ],\n",
      "       [-3.5250843 ,  7.7422442 , -4.1278315 ],\n",
      "       [-3.8368924 ,  7.907402  , -3.9876256 ],\n",
      "       [-4.449545  ,  2.870847  ,  0.8632227 ],\n",
      "       [-2.807864  ,  7.7883797 , -4.5561376 ],\n",
      "       [-3.0098763 ,  7.970167  , -4.7055106 ],\n",
      "       [-2.4036782 ,  7.2273135 , -4.5157905 ],\n",
      "       [-3.4110293 ,  7.942191  , -4.1721387 ],\n",
      "       [-3.3685305 ,  8.003408  , -4.4393044 ],\n",
      "       [ 6.8391795 , -2.8918478 , -3.1391597 ],\n",
      "       [-3.3959646 ,  7.3161883 , -3.8356128 ],\n",
      "       [-4.1317515 ,  3.949069  , -0.16498876],\n",
      "       [-3.5544355 ,  7.8644004 , -4.4042144 ],\n",
      "       [-2.608392  ,  7.0654182 , -4.0159245 ],\n",
      "       [-3.268151  ,  7.40652   , -4.108168  ],\n",
      "       [-3.9059079 ,  5.1083    , -1.7027874 ],\n",
      "       [-3.0488882 ,  7.313634  , -4.041773  ],\n",
      "       [-3.459204  ,  6.8862896 , -3.6888669 ],\n",
      "       [-3.40525   ,  7.2708483 , -3.5562558 ],\n",
      "       [-3.0822892 ,  7.4818897 , -4.605487  ],\n",
      "       [-2.2016983 ,  6.419081  , -4.203603  ],\n",
      "       [-2.9994729 ,  7.0694375 , -3.880354  ],\n",
      "       [-4.9341955 ,  6.748291  , -1.9349811 ],\n",
      "       [ 0.1567657 , -4.234945  ,  3.7982783 ],\n",
      "       [-3.3392665 ,  7.5803647 , -4.089688  ],\n",
      "       [-2.986844  ,  7.4231434 , -4.162013  ],\n",
      "       [ 6.6411214 , -1.0918653 , -4.66877   ],\n",
      "       [-3.0125625 ,  7.234503  , -4.0120916 ],\n",
      "       [-3.0011437 ,  8.072826  , -4.7369504 ],\n",
      "       [-3.2031703 ,  7.7151337 , -4.364399  ],\n",
      "       [-3.156331  ,  7.3406205 , -4.0609293 ],\n",
      "       [ 2.803172  , -4.464451  ,  1.5520914 ],\n",
      "       [-0.57699007,  3.069131  , -2.267723  ],\n",
      "       [ 5.847031  , -3.1142173 , -2.1251223 ],\n",
      "       [-3.2363212 ,  7.352231  , -3.8608582 ],\n",
      "       [-3.0130796 ,  7.5113792 , -4.4994345 ],\n",
      "       [-3.1241772 ,  7.678487  , -4.31677   ],\n",
      "       [-2.220034  ,  5.0062904 , -2.976989  ]], dtype=float32), array([[[-0.15244152,  0.23546533,  0.04003485, ...,  0.06315079,\n",
      "          0.09822161,  0.08496984],\n",
      "        [-0.32228774, -0.0181652 ,  0.03867197, ...,  0.11751855,\n",
      "          0.13804571,  0.04924075],\n",
      "        [-0.3394234 , -0.01159036,  0.05190767, ..., -0.03056039,\n",
      "          0.07337127,  0.15068953],\n",
      "        ...,\n",
      "        [-0.24716327,  0.11472959,  0.03122723, ...,  0.08808124,\n",
      "          0.12938304, -0.02076795],\n",
      "        [-0.24716327,  0.11472959,  0.03122723, ...,  0.08808124,\n",
      "          0.12938304, -0.02076795],\n",
      "        [-0.24716327,  0.11472959,  0.03122723, ...,  0.08808124,\n",
      "          0.12938304, -0.02076795]],\n",
      "\n",
      "       [[-0.28311816,  0.15849411, -0.07003821, ...,  0.08537476,\n",
      "          0.16047671,  0.145295  ],\n",
      "        [-0.16465205,  0.08337966, -0.00338702, ..., -0.1187128 ,\n",
      "          0.12201409,  0.06288393],\n",
      "        [-0.21604927,  0.20745252, -0.07159287, ..., -0.04256418,\n",
      "          0.05403805,  0.17987208],\n",
      "        ...,\n",
      "        [-0.19776231,  0.10909063,  0.06509758, ...,  0.13161023,\n",
      "          0.1351224 , -0.12668963],\n",
      "        [-0.19776231,  0.10909063,  0.06509758, ...,  0.13161023,\n",
      "          0.1351224 , -0.12668963],\n",
      "        [-0.19776231,  0.10909063,  0.06509758, ...,  0.13161023,\n",
      "          0.1351224 , -0.12668963]],\n",
      "\n",
      "       [[-0.11792081,  0.13075359,  0.02720255, ..., -0.00119739,\n",
      "          0.23338681,  0.03258134],\n",
      "        [-0.02823544,  0.2907826 , -0.16474074, ...,  0.13051674,\n",
      "          0.21148053,  0.12395668],\n",
      "        [ 0.01203273,  0.17968929, -0.08177064, ..., -0.08861291,\n",
      "          0.16022116,  0.0557813 ],\n",
      "        ...,\n",
      "        [-0.110972  ,  0.0805831 ,  0.00604976, ...,  0.18288222,\n",
      "          0.1577368 , -0.01405044],\n",
      "        [-0.110972  ,  0.0805831 ,  0.00604976, ...,  0.18288222,\n",
      "          0.1577368 , -0.01405044],\n",
      "        [-0.110972  ,  0.0805831 ,  0.00604976, ...,  0.18288222,\n",
      "          0.1577368 , -0.01405044]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.22539574,  0.18337326, -0.1530322 , ...,  0.1684587 ,\n",
      "         -0.00751059,  0.09139789],\n",
      "        [-0.36198294,  0.15013121,  0.00481575, ...,  0.10524706,\n",
      "          0.07678457,  0.19218837],\n",
      "        [-0.31335402,  0.17379244, -0.00145518, ...,  0.09452183,\n",
      "          0.05234036,  0.05818735],\n",
      "        ...,\n",
      "        [-0.10758622,  0.11312251,  0.11641763, ...,  0.1270026 ,\n",
      "          0.05267856, -0.02849184],\n",
      "        [-0.10758622,  0.11312251,  0.11641763, ...,  0.1270026 ,\n",
      "          0.05267856, -0.02849184],\n",
      "        [-0.10758622,  0.11312251,  0.11641763, ...,  0.1270026 ,\n",
      "          0.05267856, -0.02849184]],\n",
      "\n",
      "       [[-0.23647058, -0.0040111 , -0.05636298, ...,  0.2128864 ,\n",
      "         -0.07837196, -0.06956334],\n",
      "        [ 0.08158281, -0.0262775 , -0.04831147, ...,  0.13188285,\n",
      "         -0.05297193,  0.05124553],\n",
      "        [ 0.08559359, -0.16313322, -0.05055959, ..., -0.02367348,\n",
      "          0.02814612, -0.04839727],\n",
      "        ...,\n",
      "        [-0.16668072, -0.09072346,  0.19619583, ...,  0.04664416,\n",
      "          0.15499108, -0.03979161],\n",
      "        [-0.16668072, -0.09072346,  0.19619583, ...,  0.04664416,\n",
      "          0.15499108, -0.03979161],\n",
      "        [-0.16668072, -0.09072346,  0.19619583, ...,  0.04664416,\n",
      "          0.15499108, -0.03979161]],\n",
      "\n",
      "       [[-0.24370977, -0.01911692,  0.10501261, ...,  0.02517793,\n",
      "          0.20116852,  0.12199835],\n",
      "        [-0.24759242, -0.00260328,  0.10254641, ...,  0.13954845,\n",
      "          0.17586635, -0.04800244],\n",
      "        [-0.24438672,  0.19677502,  0.07243104, ..., -0.18885525,\n",
      "          0.19393536,  0.05152117],\n",
      "        ...,\n",
      "        [-0.26274803,  0.10944304,  0.04167241, ...,  0.15120527,\n",
      "          0.00848429,  0.00836481],\n",
      "        [-0.26274803,  0.10944304,  0.04167241, ...,  0.15120527,\n",
      "          0.00848429,  0.00836481],\n",
      "        [-0.26274803,  0.10944304,  0.04167241, ...,  0.15120527,\n",
      "          0.00848429,  0.00836481]]], dtype=float32)), label_ids=array([1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 0,\n",
      "       1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 1, 2, 0,\n",
      "       1, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "       0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
      "       2, 1, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2,\n",
      "       1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0]), metrics={'test_loss': 2.8654801845550537, 'test_accuracy': 0.6794871794871795, 'test_precision': 0.6141511510544174, 'test_recall': 0.6794871794871795, 'test_f1': 0.6107627133415099, 'test_runtime': 7.8209, 'test_samples_per_second': 29.92, 'test_steps_per_second': 3.836})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cd75bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "predictions_array = np.array(test_results.predictions[0])\n",
    "\n",
    "predictions_tensor = torch.tensor(predictions_array).to(torch.float32)\n",
    "probabilities = torch.softmax(predictions_tensor, dim=-1)\n",
    "\n",
    "predictions = np.argmax(probabilities.numpy(), axis=1)\n",
    "true_labels = test_results.label_ids\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "766c4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_to_save = []\n",
    "for idx in range(len(test_data)):\n",
    "    item = dataset['test'][idx]\n",
    "    actual_label = item['label']\n",
    "    predicted_label = predictions[idx]\n",
    "    claim = item['claim'] \n",
    "    premise = item['premise'] \n",
    "    category = item['category']\n",
    "    \n",
    "    # Append the information as a dictionary to the list\n",
    "    data_to_save.append({\n",
    "        'Claim': claim,\n",
    "        'Premise': premise,\n",
    "        'Actual Label': actual_label,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Category' : category\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('/home/elson/results/10.2.5_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3e1cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly classified instances\n",
    "correctly_classified = df[df['Actual Label'] == df['Predicted Label']]\n",
    "\n",
    "# Calculate misclassified instances\n",
    "misclassified = df[df['Actual Label'] != df['Predicted Label']]\n",
    "\n",
    "# Count the number of correctly classified and misclassified by category\n",
    "correct_classification_counts = correctly_classified['Category'].value_counts()\n",
    "misclassification_counts = misclassified['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afd884e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "General Health           36\n",
       "Fitness                  15\n",
       "Bone health              14\n",
       "Cancer                   12\n",
       "Diabetes                 11\n",
       "Throat                    9\n",
       "Eye                       9\n",
       "Skin                      8\n",
       "Hair                      6\n",
       "Cardiovascular Health     6\n",
       "Neurological health       6\n",
       "Ear                       6\n",
       "Women' s Health           5\n",
       "Blood                     3\n",
       "Mental Health             3\n",
       "Muscles                   3\n",
       "Men's health              3\n",
       "COVID                     3\n",
       "Dental Health             1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6477f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skin                     16\n",
       "General Health           15\n",
       "Bone health               7\n",
       "Hair                      6\n",
       "Blood                     6\n",
       "Cardiovascular Health     6\n",
       "COVID                     3\n",
       "Muscles                   3\n",
       "Vascular                  3\n",
       "Men's health              3\n",
       "Neurological health       3\n",
       "Dental Health             2\n",
       "Women' s Health           1\n",
       "Diabetes                  1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
