{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "6832fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "4b7e7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    id : int\n",
    "    num_labels: int\n",
    "    type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "52154c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/home/elson/topk3_minilm.xlsx',engine='openpyxl')\n",
    "df= data.dropna(subset=['label','CATEGORY'])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder2 = LabelEncoder()\n",
    "claims = df.claim.tolist()\n",
    "labels = df.label.tolist()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "evidence_1 = df.top_1_minilm_ce.to_list()\n",
    "evidence_2 = df.top_2_minilm_ce.to_list()\n",
    "evidence_3 = df.top_3_minilm_ce.to_list()\n",
    "category = df.CATEGORY.tolist()\n",
    "category = label_encoder2.fit_transform(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f3480e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blood' 'Bone health' 'COVID' 'COVID-19' 'Cancer' 'Cardiovascular Health'\n",
      " 'Dental Health' 'Diabetes' 'Ear' 'Eye' 'Fitness'\n",
      " 'Gastrointestinal Health' 'General Health' 'Hair' 'Kidney' \"Men's health\"\n",
      " 'Mental Health' 'Muscles' 'Neurological health' 'Skin' 'Throat'\n",
      " 'Vascular' \"Women' s Health\"]\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder2.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "3de11b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform the split\n",
    "train_premises, test_premises, train_hypothesis1, test_hypothesis1,train_hypothesis2, test_hypothesis2, train_hypothesis3, test_hypothesis3, train_labels, test_labels = train_test_split(\n",
    "    claims, evidence_1,evidence_2,evidence_3, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e6b38317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MediClaimVeracityDataset(Dataset):\n",
    "    def __init__(self, premises, hypothesis1, hypothesis2, hypothesis3, labels, tokenizer_name='allenai/scibert_scivocab_uncased'):\n",
    "        self.premises = premises\n",
    "        self.hypothesis1 = hypothesis1\n",
    "        self.hypothesis2 = hypothesis2\n",
    "        self.hypothesis3 = hypothesis3\n",
    "        self.labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Corrected: Use `.join` properly to concatenate the hypotheses with the premises.\n",
    "        separator = self.tokenizer.sep_token\n",
    "        grouped = self.premises[idx] + separator + separator.join([self.hypothesis1[idx], self.hypothesis2[idx], self.hypothesis3[idx]])\n",
    "        \n",
    "        tokenized_input = self.tokenizer(\n",
    "            text=grouped,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt')\n",
    "        tokenized_input = {key: val.squeeze(0) for key, val in tokenized_input.items()}\n",
    "        \n",
    "        # Adding labels and task_id to the returned dictionary.\n",
    "        tokenized_input['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        tokenized_input['task_ids'] = torch.tensor(0)  # Adding task_id as 0 for all instances\n",
    "        \n",
    "        return tokenized_input\n",
    "\n",
    "def load_entailment_dataset(id=0):\n",
    "    trainV_dataset = MediClaimVeracityDataset(train_premises, train_hypothesis1, train_hypothesis2, train_hypothesis3, train_labels)\n",
    "    valV_dataset = MediClaimVeracityDataset(test_premises, test_hypothesis1, test_hypothesis2, test_hypothesis3, test_labels)\n",
    "    logging_steps = len(test_premises) // 16\n",
    "\n",
    "        # Wrap the subsets with DataLoader\n",
    "    #trainV_loader = DataLoader(trainV_dataset, batch_size=16, shuffle=True)\n",
    "    #valV_loader = DataLoader(valV_dataset, batch_size=16, shuffle=False)\n",
    "    num_labels = len(train_premises)\n",
    "    task_info = Task(\n",
    "            id, num_labels=num_labels, type=\"ver_classification\"\n",
    "        )\n",
    "\n",
    "    return trainV_dataset, valV_dataset, task_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1ede52a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:17,724 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:17,902 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:17,905 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:20,324 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:20,327 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:20,328 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:20,330 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:20,331 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:20,518 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:20,521 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:20,741 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:20,744 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:20,947 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:21,345 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:21,348 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:23,321 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:23,323 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:23,325 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:23,327 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:23,330 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:24,063 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:24,066 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:24,275 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:24,278 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.MediClaimVeracityDataset at 0x7fbae33f4b70>,\n",
       " <__main__.MediClaimVeracityDataset at 0x7fba39b4d828>,\n",
       " Task(id=0, num_labels=619, type='ver_classification'))"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_entailment_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d9172162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stat, test_stat, train_categories, test_categories = train_test_split(\n",
    "    claims, category, test_size=0.2, random_state=42)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MediClaimCategoryDataset(Dataset):\n",
    "    def __init__(self, claims, category, tokenizer_name='allenai/scibert_scivocab_uncased'):\n",
    "        self.claims = claims\n",
    "        self.category = category\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.category)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_input = self.tokenizer(\n",
    "            text=self.claims[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt')\n",
    "        tokenized_input = {key: val.squeeze(0) for key, val in tokenized_input.items()}\n",
    "        \n",
    "        # Adding 'labels' to align with the format in the `forward` method and MultiTaskModel expectations\n",
    "        tokenized_input['labels'] = torch.tensor(self.category[idx], dtype=torch.long)\n",
    "        # Adding task_id as 1 for all instances to identify this as category classification\n",
    "        tokenized_input['task_ids'] = torch.tensor(1)\n",
    "        \n",
    "        return tokenized_input\n",
    "\n",
    "def load_category_dataset(id=1):\n",
    "    trainC_dataset = MediClaimCategoryDataset(train_stat,train_categories)\n",
    "    valC_dataset = MediClaimCategoryDataset(test_stat,test_categories)\n",
    "    logging_steps = len(train_stat) // 16\n",
    "\n",
    "        # Wrap the subsets with DataLoader\n",
    "#     trainC_loader = DataLoader(trainC_dataset, batch_size=16, shuffle=True)\n",
    "#     valC_loader = DataLoader(valC_dataset, batch_size=16, shuffle=False)\n",
    "    num_labels = len(train_stat)\n",
    "    task_info = Task(\n",
    "            id, num_labels=num_labels, type=\"cat_classification\"\n",
    "        )\n",
    "\n",
    "    return trainC_dataset, valC_dataset, task_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c1d296b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:24,732 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:25,115 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:25,119 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:27,166 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:27,167 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:27,169 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:27,170 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:27,173 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:27,345 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:27,348 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:27,785 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:27,788 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:28,190 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:28,573 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:28,576 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:30,224 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:30,226 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:30,227 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:30,228 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:30,230 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:30,603 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:30,606 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:30,821 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:30,825 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.MediClaimCategoryDataset at 0x7fb81c2a71d0>,\n",
       " <__main__.MediClaimCategoryDataset at 0x7fb859a03400>,\n",
       " Task(id=1, num_labels=619, type='cat_classification'))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_category_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f8d7d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, dataset1, dataset2):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.total_size = len(dataset1) + len(dataset2)\n",
    "        \n",
    "        # Create an array that indicates which dataset each index should fetch from\n",
    "        self.dataset_labels = np.array([0]*len(dataset1) + [1]*len(dataset2))\n",
    "        # Shuffle to mix dataset indices\n",
    "        np.random.shuffle(self.dataset_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset_labels[idx] == 0:\n",
    "            # Fetch from dataset1\n",
    "            return self.dataset1[idx % len(self.dataset1)]\n",
    "        else:\n",
    "            # Fetch from dataset2\n",
    "            return self.dataset2[idx % len(self.dataset2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "eeea0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    (\n",
    "        veracity_classification_train_dataset,\n",
    "        veracity_classification_validation_dataset,\n",
    "        veracity_classification_task,\n",
    "    ) = load_entailment_dataset(0)\n",
    "    (\n",
    "        cat_classification_train_dataset,\n",
    "        cat_classification_validation_dataset,\n",
    "        cat_classification_task,\n",
    "    ) = load_category_dataset(1)\n",
    "\n",
    "    # Merge train datasets\n",
    "    train_combined_dataset = CombinedDataset(veracity_classification_train_dataset, cat_classification_train_dataset)\n",
    "    val_combined_dataset = CombinedDataset(veracity_classification_validation_dataset, cat_classification_validation_dataset)\n",
    "\n",
    "    \n",
    "    tasks = [veracity_classification_task, cat_classification_task]\n",
    "    return tasks, train_combined_dataset,val_combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "39b4a953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:31,263 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:31,647 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:31,650 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:33,714 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:33,718 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:33,719 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:33,720 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:33,721 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:33,895 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:33,898 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:34,324 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:34,328 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:34,754 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:34,925 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:34,928 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:37,055 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:37,059 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:37,060 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:37,061 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:37,063 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:37,446 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:37,450 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:37,863 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:37,866 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:38,064 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:38,236 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:38,239 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:40,326 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:40,327 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:40,328 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:40,330 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:40,332 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:40,513 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:40,517 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:40,938 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:40,942 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:41,342 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:41,725 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:41,729 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:43,614 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:43,616 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:43,618 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:43,619 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:43,620 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:43,791 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:43,794 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:44,014 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:44,017 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([Task(id=0, num_labels=619, type='ver_classification'),\n",
       "  Task(id=1, num_labels=619, type='cat_classification')],\n",
       " <__main__.CombinedDataset at 0x7fb81c2a7f28>,\n",
       " <__main__.CombinedDataset at 0x7fb81c2a7710>)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "061afec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "from typing import List, Dict\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder_name_or_path, tasks: List):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name_or_path)\n",
    "        self.output_heads = nn.ModuleDict()\n",
    "\n",
    "        for task in tasks:\n",
    "            decoder = self._create_output_head(self.encoder.config.hidden_size, task)\n",
    "            self.output_heads[str(task.id)] = decoder\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_output_head(encoder_hidden_size: int, task):\n",
    "        # Placeholder for actual implementation\n",
    "        if task.type == \"ver_classification\":\n",
    "            return VeracityClassificationHead(encoder_hidden_size, task.num_labels)\n",
    "        elif task.type == \"cat_classification\":\n",
    "            return CategoryClassificationHead(encoder_hidden_size, task.num_labels)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        task_ids=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "\n",
    "        unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "\n",
    "        loss_list = []\n",
    "        logits = None\n",
    "        for unique_task_id in unique_task_ids_list:\n",
    "\n",
    "            task_id_filter = task_ids == unique_task_id\n",
    "            logits, task_loss = self.output_heads[str(unique_task_id)].forward(\n",
    "                sequence_output[task_id_filter],\n",
    "                pooled_output[task_id_filter],\n",
    "                labels=None if labels is None else labels[task_id_filter],\n",
    "                attention_mask=attention_mask[task_id_filter],\n",
    "            )\n",
    "\n",
    "            if labels is not None:\n",
    "                loss_list.append(task_loss)\n",
    "\n",
    "        # logits are only used for eval. and in case of eval the batch is not multi task\n",
    "        # For training only the loss is used\n",
    "        outputs = (logits, outputs[2:])\n",
    "\n",
    "        if loss_list:\n",
    "            loss = torch.stack(loss_list)\n",
    "            outputs = (loss.mean(),) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "dbd36fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeracityClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, **kwargs):\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.dim() != 1:\n",
    "                # Remove padding\n",
    "                labels = labels[:, 0]\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.num_labels), labels.long().view(-1)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4817af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, **kwargs):\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.dim() != 1:\n",
    "                # Remove padding\n",
    "                labels = labels[:, 0]\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.num_labels), labels.long().view(-1)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "9fda1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    # Extract predictions and true labels\n",
    "    preds = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions\n",
    "    preds = np.argmax(preds, axis=1) if preds.ndim > 1 else preds.squeeze()\n",
    "    \n",
    "    true_labels = pred.label_ids\n",
    "    \n",
    "    # Ensure predictions and labels are aligned\n",
    "    # This is a simplistic fix; for real scenarios, you might need more complex handling\n",
    "    min_len = min(len(preds), len(true_labels))\n",
    "    preds, true_labels = preds[:min_len], true_labels[:min_len]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds, average='weighted', labels=np.unique(preds))\n",
    "    precision = precision_score(true_labels, preds, average='weighted', labels=np.unique(preds))\n",
    "    recall = recall_score(true_labels, preds, average='weighted', labels=np.unique(preds))\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "75954618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",datefmt=\"%m/%d/%Y %H:%M:%S\",handlers=[logging.StreamHandler(sys.stdout)],)\n",
    "import transformers\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(log_level)\n",
    "transformers.logging.set_verbosity_info()\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "636dd69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:44,498 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:44,852 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:44,855 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:46,833 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:46,835 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:46,837 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:46,838 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:46,841 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:47,213 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:47,216 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:47,654 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:47,657 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f4a404a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:48,070 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:48,442 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:48,445 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:51,312 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:51,315 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:51,317 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:51,319 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:51,320 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:51,694 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:51,697 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:52,121 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:52,124 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:52,537 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:52,715 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:52,718 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:54,696 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:54,698 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:54,699 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:54,701 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:54,702 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:54,876 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:54,879 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:55,292 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:55,297 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:55,702 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:56,081 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:56,084 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:57,526 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:57,528 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:57,529 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:57,532 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:31:57,533 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:57,932 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:57,935 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:58,150 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:58,153 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2024-03-25 17:31:58,353 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:31:58,526 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:31:58,529 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:32:00,565 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/vocab.txt from cache at /home/elson/.cache/huggingface/transformers/33593020f507d72099bd84ea6cd2296feb424fecd62d4a8edcc2a02899af6e29.38339d84e6e392addd730fd85fae32652c4cc7c5423633d6fa73e5f7937bbc38\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:32:00,567 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:32:00,568 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:32:00,570 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2024-03-25 17:32:00,571 >> loading file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:32:00,943 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:32:00,945 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2024-03-25 17:32:01,359 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:32:01,362 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks, train_dataset,val_dataset=load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b08dd76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2024-03-25 17:32:01,569 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/config.json from cache at /home/elson/.cache/huggingface/transformers/858852fd2471ce39075378592ddc87f5a6551e64c6825d1b92c8dab9318e0fc3.03ff9e9f998b9a9d40647a2148a202e3fb3d568dc0f170dda9dda194bab4d5dd\n",
      "[INFO|configuration_utils.py:690] 2024-03-25 17:32:01,572 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1772] 2024-03-25 17:32:01,956 >> loading weights file https://huggingface.co/allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin from cache at /home/elson/.cache/huggingface/transformers/de14937a851e8180a2bc5660c0041d385f8a0c62b1b2ccafa46df31043a2390c.74830bb01a0ffcdeaed8be9916312726d0c4cd364ac6fc15b375f789eaff4cbb\n",
      "[WARNING|modeling_utils.py:2049] 2024-03-25 17:32:03,487 >> Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO|modeling_utils.py:2066] 2024-03-25 17:32:03,489 >> All the weights of BertModel were initialized from the model checkpoint at allenai/scibert_scivocab_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskModel(model_name, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "49639028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:453] 2024-03-25 17:32:03,616 >> Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8d5e48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1023] 2024-03-25 17:32:03,666 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:886] 2024-03-25 17:32:03,669 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[INFO|trainer.py:453] 2024-03-25 17:32:03,676 >> Using amp half precision backend\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1290] 2024-03-25 17:32:03,687 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2024-03-25 17:32:03,688 >>   Num examples = 1238\n",
      "[INFO|trainer.py:1292] 2024-03-25 17:32:03,689 >>   Num Epochs = 40\n",
      "[INFO|trainer.py:1293] 2024-03-25 17:32:03,689 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1294] 2024-03-25 17:32:03,690 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1295] 2024-03-25 17:32:03,691 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1296] 2024-03-25 17:32:03,692 >>   Total optimization steps = 400\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 04:58, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.546908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.114854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.755576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.321622</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.947538</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.656958</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.084634</td>\n",
       "      <td>0.117060</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.409511</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.123598</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>4.198694</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.163310</td>\n",
       "      <td>0.094545</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>4.013943</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.151579</td>\n",
       "      <td>0.084706</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>3.851131</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>3.709910</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>3.579114</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>3.464793</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>3.365044</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>3.276612</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>3.195443</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>3.120298</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>3.055988</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>2.999133</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>2.948289</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>2.905069</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>2.858301</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.785900</td>\n",
       "      <td>2.816666</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.778215</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.164760</td>\n",
       "      <td>0.092072</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.741717</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.153373</td>\n",
       "      <td>0.085828</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.708693</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.153373</td>\n",
       "      <td>0.085828</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.676118</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.155210</td>\n",
       "      <td>0.086980</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.649329</td>\n",
       "      <td>0.109677</td>\n",
       "      <td>0.147470</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.622649</td>\n",
       "      <td>0.109677</td>\n",
       "      <td>0.147470</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>2.599471</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.139636</td>\n",
       "      <td>0.078367</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.578343</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.094924</td>\n",
       "      <td>0.053314</td>\n",
       "      <td>0.432432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.560320</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.094924</td>\n",
       "      <td>0.053314</td>\n",
       "      <td>0.432432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.545203</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.074298</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.326087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.531914</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.080268</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.519901</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.077848</td>\n",
       "      <td>0.044192</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.508831</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.074462</td>\n",
       "      <td>0.042386</td>\n",
       "      <td>0.306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.501564</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.078893</td>\n",
       "      <td>0.044867</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.712600</td>\n",
       "      <td>2.495927</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.078893</td>\n",
       "      <td>0.044867</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.509100</td>\n",
       "      <td>2.492142</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.074462</td>\n",
       "      <td>0.042386</td>\n",
       "      <td>0.306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.509100</td>\n",
       "      <td>2.490854</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.063457</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2416] 2024-03-25 17:32:10,049 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:10,050 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:10,051 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:32:17,691 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:17,692 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:17,693 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:32:25,280 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:25,282 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:25,283 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:32:32,540 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:32,542 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:32,544 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:32:39,899 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:39,900 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:39,902 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:32:47,469 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:47,471 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:47,473 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:32:55,132 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:32:55,133 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:32:55,135 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:02,531 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:02,533 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:02,534 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:10,161 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:10,163 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:10,164 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:17,629 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:17,631 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:17,632 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:25,227 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:25,229 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:25,230 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:32,514 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:32,516 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:32,516 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2416] 2024-03-25 17:33:39,782 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:39,785 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:39,786 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:47,303 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:47,305 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:47,307 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:33:54,910 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:33:54,911 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:33:54,912 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:02,361 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:02,363 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:02,364 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:09,418 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:09,420 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:09,421 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:17,027 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:17,029 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:17,030 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:24,385 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:24,388 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:24,389 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:32,072 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:32,073 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:32,074 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:39,772 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:39,774 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:39,775 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:47,669 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:47,670 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:47,671 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:34:54,887 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:34:54,888 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:34:54,889 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:02,175 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:02,177 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:02,179 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:09,797 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:09,798 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:09,799 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:17,486 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:17,487 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:17,488 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:25,115 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:25,117 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:25,119 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:32,497 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:32,498 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:32,499 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:39,638 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:39,640 >>   Num examples = 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2421] 2024-03-25 17:35:39,641 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:47,014 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:47,016 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:47,017 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:35:54,372 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:35:54,373 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:35:54,374 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:01,702 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:01,703 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:01,703 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:09,537 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:09,539 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:09,540 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:16,969 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:16,970 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:16,972 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:24,347 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:24,349 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:24,351 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:31,829 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:31,830 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:31,832 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:39,535 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:39,536 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:39,538 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:47,059 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:47,061 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:47,063 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:36:54,600 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:36:54,600 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:36:54,601 >>   Batch size = 16\n",
      "/home/elson/factcheck/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:2416] 2024-03-25 17:37:02,099 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2024-03-25 17:37:02,101 >>   Num examples = 310\n",
      "[INFO|trainer.py:2421] 2024-03-25 17:37:02,101 >>   Batch size = 16\n",
      "[INFO|trainer.py:1530] 2024-03-25 17:37:03,353 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=3.4750553798675536, metrics={'train_runtime': 299.6624, 'train_samples_per_second': 165.253, 'train_steps_per_second': 1.335, 'total_flos': 0.0, 'train_loss': 3.4750553798675536, 'epoch': 40.0})"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_steps = len(train_dataset)//16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/elson/multi-task/scibert\",  # Specify your desired output directory\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    num_train_epochs=40,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,  # Make sure logging_steps is defined, e.g., 50\n",
    "    push_to_hub=False,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # Make sure your model is specified here\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    )\n",
    "    # Start the training process\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
