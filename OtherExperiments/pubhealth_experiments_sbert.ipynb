{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:27:16.971877700Z",
     "start_time": "2024-03-07T15:27:14.022064900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./factcheck/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: rank_bm25 in ./factcheck/lib/python3.6/site-packages (0.2.2)\n",
      "Requirement already satisfied: tqdm in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: numpy in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: torchvision in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (0.11.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (4.18.0)\n",
      "Requirement already satisfied: sentencepiece in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (3.6.7)\n",
      "Requirement already satisfied: scipy in ./factcheck/lib/python3.6/site-packages (from sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: packaging>=20.9 in ./factcheck/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in ./factcheck/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in ./factcheck/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: pyyaml in ./factcheck/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: importlib-metadata in ./factcheck/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./factcheck/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in ./factcheck/lib/python3.6/site-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./factcheck/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in ./factcheck/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: sacremoses in ./factcheck/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.53)\n",
      "Requirement already satisfied: importlib-resources in ./factcheck/lib/python3.6/site-packages (from tqdm->sentence-transformers) (5.4.0)\n",
      "Requirement already satisfied: click in ./factcheck/lib/python3.6/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in ./factcheck/lib/python3.6/site-packages (from nltk->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./factcheck/lib/python3.6/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in ./factcheck/lib/python3.6/site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./factcheck/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./factcheck/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./factcheck/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./factcheck/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./factcheck/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./factcheck/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: six in ./factcheck/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a392b3d4cef5df2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T15:27:14.021063800Z",
     "start_time": "2024-03-07T15:26:27.273913Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38348837c73d474a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:54:11.844258500Z",
     "start_time": "2024-01-09T13:54:11.830229200Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)  # Convert non-strings to strings\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2459baa4845d691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:54:11.869642100Z",
     "start_time": "2024-01-09T13:54:11.847390800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Module \n",
    "import os \n",
    "\n",
    "# Folder Path \n",
    "path_to_zip_file = \"/home/elson/corpus.zip\"\n",
    "directory_to_extract_to = \"/home/elson/\"\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "def read_text_file(file_path): \n",
    "    with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "        doc = f.read()\n",
    "    return doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62adbbbf5f603a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:54:55.182422700Z",
     "start_time": "2024-01-09T13:54:11.864367400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc394.txt\n",
      "doc227.txt\n",
      "doc462.txt\n",
      "doc586.txt\n",
      "doc810.txt\n",
      "doc803.txt\n",
      "doc653.txt\n",
      "doc743.txt\n",
      "doc108.txt\n",
      "doc292.txt\n",
      "doc36.txt\n",
      "doc483.txt\n",
      "doc710.txt\n",
      "doc333.txt\n",
      "doc786.txt\n",
      "doc795.txt\n",
      "doc471.txt\n",
      "doc851.txt\n",
      "doc645.txt\n",
      "doc428.txt\n",
      "doc47.txt\n",
      "doc607.txt\n",
      "doc775.txt\n",
      "doc111.txt\n",
      "doc530.txt\n",
      "doc286.txt\n",
      "doc250.txt\n",
      "doc417.txt\n",
      "doc633.txt\n",
      "doc593.txt\n",
      "doc764.txt\n",
      "doc615.txt\n",
      "doc748.txt\n",
      "doc81.txt\n",
      "doc666.txt\n",
      "doc700.txt\n",
      "doc797.txt\n",
      "doc489.txt\n",
      "doc425.txt\n",
      "doc295.txt\n",
      "doc598.txt\n",
      "doc162.txt\n",
      "doc113.txt\n",
      "doc466.txt\n",
      "doc97.txt\n",
      "doc552.txt\n",
      "doc207.txt\n",
      "doc468.txt\n",
      "doc19.txt\n",
      "doc16.txt\n",
      "doc711.txt\n",
      "doc304.txt\n",
      "doc750.txt\n",
      "doc852.txt\n",
      "doc33.txt\n",
      "doc848.txt\n",
      "doc737.txt\n",
      "doc727.txt\n",
      "doc551.txt\n",
      "doc395.txt\n",
      "doc385.txt\n",
      "doc323.txt\n",
      "doc229.txt\n",
      "doc777.txt\n",
      "doc778.txt\n",
      "doc267.txt\n",
      "doc729.txt\n",
      "doc414.txt\n",
      "doc218.txt\n",
      "doc836.txt\n",
      "doc649.txt\n",
      "doc224.txt\n",
      "doc453.txt\n",
      "doc704.txt\n",
      "doc345.txt\n",
      "doc278.txt\n",
      "doc828.txt\n",
      "doc571.txt\n",
      "doc628.txt\n",
      "doc656.txt\n",
      "doc354.txt\n",
      "doc211.txt\n",
      "doc435.txt\n",
      "doc568.txt\n",
      "doc173.txt\n",
      "doc415.txt\n",
      "doc685.txt\n",
      "doc487.txt\n",
      "doc759.txt\n",
      "doc506.txt\n",
      "doc112.txt\n",
      "doc51.txt\n",
      "doc400.txt\n",
      "doc238.txt\n",
      "doc606.txt\n",
      "doc470.txt\n",
      "doc577.txt\n",
      "doc254.txt\n",
      "doc472.txt\n",
      "doc843.txt\n",
      "doc860.txt\n",
      "doc787.txt\n",
      "doc281.txt\n",
      "doc691.txt\n",
      "doc142.txt\n",
      "doc119.txt\n",
      "doc542.txt\n",
      "doc67.txt\n",
      "doc463.txt\n",
      "doc169.txt\n",
      "doc121.txt\n",
      "doc714.txt\n",
      "doc650.txt\n",
      "doc769.txt\n",
      "doc859.txt\n",
      "doc566.txt\n",
      "doc716.txt\n",
      "doc202.txt\n",
      "doc738.txt\n",
      "doc26.txt\n",
      "doc853.txt\n",
      "doc25.txt\n",
      "doc380.txt\n",
      "doc723.txt\n",
      "doc657.txt\n",
      "doc482.txt\n",
      "doc699.txt\n",
      "doc614.txt\n",
      "doc497.txt\n",
      "doc290.txt\n",
      "doc241.txt\n",
      "doc120.txt\n",
      "doc518.txt\n",
      "doc404.txt\n",
      "doc863.txt\n",
      "doc247.txt\n",
      "doc492.txt\n",
      "doc364.txt\n",
      "doc192.txt\n",
      "doc422.txt\n",
      "doc636.txt\n",
      "doc255.txt\n",
      "doc401.txt\n",
      "doc163.txt\n",
      "doc722.txt\n",
      "doc280.txt\n",
      "doc667.txt\n",
      "doc98.txt\n",
      "doc643.txt\n",
      "doc436.txt\n",
      "doc377.txt\n",
      "doc249.txt\n",
      "doc585.txt\n",
      "doc724.txt\n",
      "doc830.txt\n",
      "doc374.txt\n",
      "doc623.txt\n",
      "doc205.txt\n",
      "doc578.txt\n",
      "doc533.txt\n",
      "doc589.txt\n",
      "doc84.txt\n",
      "doc532.txt\n",
      "doc846.txt\n",
      "doc277.txt\n",
      "doc196.txt\n",
      "doc686.txt\n",
      "doc43.txt\n",
      "doc12.txt\n",
      "doc672.txt\n",
      "doc461.txt\n",
      "doc780.txt\n",
      "doc768.txt\n",
      "doc534.txt\n",
      "doc55.txt\n",
      "doc595.txt\n",
      "doc441.txt\n",
      "doc305.txt\n",
      "doc140.txt\n",
      "doc424.txt\n",
      "doc547.txt\n",
      "doc669.txt\n",
      "doc27.txt\n",
      "doc440.txt\n",
      "doc689.txt\n",
      "doc529.txt\n",
      "doc805.txt\n",
      "doc230.txt\n",
      "doc439.txt\n",
      "doc618.txt\n",
      "doc426.txt\n",
      "doc35.txt\n",
      "doc824.txt\n",
      "doc408.txt\n",
      "doc60.txt\n",
      "doc752.txt\n",
      "doc101.txt\n",
      "doc545.txt\n",
      "doc350.txt\n",
      "doc739.txt\n",
      "doc528.txt\n",
      "doc850.txt\n",
      "doc11.txt\n",
      "doc213.txt\n",
      "doc86.txt\n",
      "doc624.txt\n",
      "doc613.txt\n",
      "doc210.txt\n",
      "doc605.txt\n",
      "doc447.txt\n",
      "doc203.txt\n",
      "doc208.txt\n",
      "doc677.txt\n",
      "doc451.txt\n",
      "doc570.txt\n",
      "doc449.txt\n",
      "doc651.txt\n",
      "doc257.txt\n",
      "doc766.txt\n",
      "doc507.txt\n",
      "doc671.txt\n",
      "doc328.txt\n",
      "doc130.txt\n",
      "doc687.txt\n",
      "doc392.txt\n",
      "doc548.txt\n",
      "doc825.txt\n",
      "doc276.txt\n",
      "doc204.txt\n",
      "doc490.txt\n",
      "doc145.txt\n",
      "doc579.txt\n",
      "doc494.txt\n",
      "doc849.txt\n",
      "doc397.txt\n",
      "doc755.txt\n",
      "doc553.txt\n",
      "doc630.txt\n",
      "doc72.txt\n",
      "doc87.txt\n",
      "doc299.txt\n",
      "doc198.txt\n",
      "doc217.txt\n",
      "doc30.txt\n",
      "doc74.txt\n",
      "doc668.txt\n",
      "doc635.txt\n",
      "doc430.txt\n",
      "doc274.txt\n",
      "doc783.txt\n",
      "doc564.txt\n",
      "doc731.txt\n",
      "doc45.txt\n",
      "doc260.txt\n",
      "doc383.txt\n",
      "doc315.txt\n",
      "doc177.txt\n",
      "doc114.txt\n",
      "doc256.txt\n",
      "doc405.txt\n",
      "doc474.txt\n",
      "doc680.txt\n",
      "doc495.txt\n",
      "doc735.txt\n",
      "doc57.txt\n",
      "doc799.txt\n",
      "doc406.txt\n",
      "doc284.txt\n",
      "doc707.txt\n",
      "doc619.txt\n",
      "doc330.txt\n",
      "doc17.txt\n",
      "doc129.txt\n",
      "doc611.txt\n",
      "doc556.txt\n",
      "doc637.txt\n",
      "doc826.txt\n",
      "doc676.txt\n",
      "doc774.txt\n",
      "doc412.txt\n",
      "doc640.txt\n",
      "doc195.txt\n",
      "doc282.txt\n",
      "doc335.txt\n",
      "doc88.txt\n",
      "doc509.txt\n",
      "doc272.txt\n",
      "doc708.txt\n",
      "doc560.txt\n",
      "doc340.txt\n",
      "doc705.txt\n",
      "doc582.txt\n",
      "doc171.txt\n",
      "doc840.txt\n",
      "doc308.txt\n",
      "doc5.txt\n",
      "doc234.txt\n",
      "doc678.txt\n",
      "doc391.txt\n",
      "doc251.txt\n",
      "doc390.txt\n",
      "doc403.txt\n",
      "doc457.txt\n",
      "doc21.txt\n",
      "doc368.txt\n",
      "doc372.txt\n",
      "doc808.txt\n",
      "doc2.txt\n",
      "doc814.txt\n",
      "doc813.txt\n",
      "doc225.txt\n",
      "doc527.txt\n",
      "doc137.txt\n",
      "doc378.txt\n",
      "doc706.txt\n",
      "doc4.txt\n",
      "doc596.txt\n",
      "doc273.txt\n",
      "doc674.txt\n",
      "doc456.txt\n",
      "doc178.txt\n",
      "doc252.txt\n",
      "doc675.txt\n",
      "doc827.txt\n",
      "doc174.txt\n",
      "doc216.txt\n",
      "doc209.txt\n",
      "doc442.txt\n",
      "doc351.txt\n",
      "doc839.txt\n",
      "doc37.txt\n",
      "doc398.txt\n",
      "doc355.txt\n",
      "doc32.txt\n",
      "doc501.txt\n",
      "doc546.txt\n",
      "doc834.txt\n",
      "doc555.txt\n",
      "doc662.txt\n",
      "doc283.txt\n",
      "doc363.txt\n",
      "doc697.txt\n",
      "doc301.txt\n",
      "doc550.txt\n",
      "doc15.txt\n",
      "doc779.txt\n",
      "doc418.txt\n",
      "doc410.txt\n",
      "doc608.txt\n",
      "doc40.txt\n",
      "doc609.txt\n",
      "doc709.txt\n",
      "doc271.txt\n",
      "doc179.txt\n",
      "doc538.txt\n",
      "doc337.txt\n",
      "doc1.txt\n",
      "doc819.txt\n",
      "doc782.txt\n",
      "doc42.txt\n",
      "doc448.txt\n",
      "doc427.txt\n",
      "doc703.txt\n",
      "doc170.txt\n",
      "doc366.txt\n",
      "doc696.txt\n",
      "doc23.txt\n",
      "doc44.txt\n",
      "doc407.txt\n",
      "doc503.txt\n",
      "doc817.txt\n",
      "doc327.txt\n",
      "doc641.txt\n",
      "doc647.txt\n",
      "doc118.txt\n",
      "doc694.txt\n",
      "doc833.txt\n",
      "doc148.txt\n",
      "doc182.txt\n",
      "doc753.txt\n",
      "doc594.txt\n",
      "doc92.txt\n",
      "doc504.txt\n",
      "doc331.txt\n",
      "doc673.txt\n",
      "doc310.txt\n",
      "doc244.txt\n",
      "doc798.txt\n",
      "doc557.txt\n",
      "doc288.txt\n",
      "doc287.txt\n",
      "doc22.txt\n",
      "doc726.txt\n",
      "doc48.txt\n",
      "doc357.txt\n",
      "doc796.txt\n",
      "doc804.txt\n",
      "doc76.txt\n",
      "doc38.txt\n",
      "doc347.txt\n",
      "doc352.txt\n",
      "doc423.txt\n",
      "doc359.txt\n",
      "doc770.txt\n",
      "doc303.txt\n",
      "doc434.txt\n",
      "doc184.txt\n",
      "doc172.txt\n",
      "doc189.txt\n",
      "doc512.txt\n",
      "doc800.txt\n",
      "doc559.txt\n",
      "doc188.txt\n",
      "doc432.txt\n",
      "doc85.txt\n",
      "doc807.txt\n",
      "doc683.txt\n",
      "doc756.txt\n",
      "doc14.txt\n",
      "doc131.txt\n",
      "doc781.txt\n",
      "doc365.txt\n",
      "doc505.txt\n",
      "doc318.txt\n",
      "doc692.txt\n",
      "doc80.txt\n",
      "doc536.txt\n",
      "doc66.txt\n",
      "doc517.txt\n",
      "doc480.txt\n",
      "doc815.txt\n",
      "doc655.txt\n",
      "doc751.txt\n",
      "doc510.txt\n",
      "doc100.txt\n",
      "doc346.txt\n",
      "doc690.txt\n",
      "doc599.txt\n",
      "doc298.txt\n",
      "doc732.txt\n",
      "doc502.txt\n",
      "doc572.txt\n",
      "doc149.txt\n",
      "doc396.txt\n",
      "doc261.txt\n",
      "doc242.txt\n",
      "doc125.txt\n",
      "doc29.txt\n",
      "doc194.txt\n",
      "doc665.txt\n",
      "doc702.txt\n",
      "doc127.txt\n",
      "doc420.txt\n",
      "doc627.txt\n",
      "doc46.txt\n",
      "doc393.txt\n",
      "doc500.txt\n",
      "doc296.txt\n",
      "doc513.txt\n",
      "doc373.txt\n",
      "doc688.txt\n",
      "doc465.txt\n",
      "doc758.txt\n",
      "doc258.txt\n",
      "doc154.txt\n",
      "doc105.txt\n",
      "doc78.txt\n",
      "doc41.txt\n",
      "doc158.txt\n",
      "doc622.txt\n",
      "doc567.txt\n",
      "doc818.txt\n",
      "doc219.txt\n",
      "doc317.txt\n",
      "doc713.txt\n",
      "doc626.txt\n",
      "doc789.txt\n",
      "doc221.txt\n",
      "doc858.txt\n",
      "doc79.txt\n",
      "doc790.txt\n",
      "doc122.txt\n",
      "doc263.txt\n",
      "doc89.txt\n",
      "doc681.txt\n",
      "doc399.txt\n",
      "doc658.txt\n",
      "doc754.txt\n",
      "doc747.txt\n",
      "doc652.txt\n",
      "doc543.txt\n",
      "doc520.txt\n",
      "doc231.txt\n",
      "doc498.txt\n",
      "doc222.txt\n",
      "doc715.txt\n",
      "doc648.txt\n",
      "doc855.txt\n",
      "doc847.txt\n",
      "doc583.txt\n",
      "doc388.txt\n",
      "doc265.txt\n",
      "doc212.txt\n",
      "doc741.txt\n",
      "doc176.txt\n",
      "doc592.txt\n",
      "doc107.txt\n",
      "doc39.txt\n",
      "doc106.txt\n",
      "doc429.txt\n",
      "doc812.txt\n",
      "doc701.txt\n",
      "doc360.txt\n",
      "doc152.txt\n",
      "doc664.txt\n",
      "doc835.txt\n",
      "doc389.txt\n",
      "doc591.txt\n",
      "doc269.txt\n",
      "doc379.txt\n",
      "doc376.txt\n",
      "doc576.txt\n",
      "doc841.txt\n",
      "doc638.txt\n",
      "doc9.txt\n",
      "doc151.txt\n",
      "doc525.txt\n",
      "doc526.txt\n",
      "doc185.txt\n",
      "doc776.txt\n",
      "doc70.txt\n",
      "doc223.txt\n",
      "doc7.txt\n",
      "doc467.txt\n",
      "doc857.txt\n",
      "doc115.txt\n",
      "doc760.txt\n",
      "doc311.txt\n",
      "doc375.txt\n",
      "doc275.txt\n",
      "doc659.txt\n",
      "doc481.txt\n",
      "doc338.txt\n",
      "doc161.txt\n",
      "doc772.txt\n",
      "doc49.txt\n",
      "doc493.txt\n",
      "doc226.txt\n",
      "doc361.txt\n",
      "doc785.txt\n",
      "doc531.txt\n",
      "doc110.txt\n",
      "doc63.txt\n",
      "doc736.txt\n",
      "doc71.txt\n",
      "doc52.txt\n",
      "doc370.txt\n",
      "doc693.txt\n",
      "doc771.txt\n",
      "doc168.txt\n",
      "doc197.txt\n",
      "doc236.txt\n",
      "doc109.txt\n",
      "doc602.txt\n",
      "doc353.txt\n",
      "doc102.txt\n",
      "doc712.txt\n",
      "doc166.txt\n",
      "doc248.txt\n",
      "doc646.txt\n",
      "doc300.txt\n",
      "doc821.txt\n",
      "doc336.txt\n",
      "doc362.txt\n",
      "doc28.txt\n",
      "doc473.txt\n",
      "doc65.txt\n",
      "doc574.txt\n",
      "doc325.txt\n",
      "doc610.txt\n",
      "doc239.txt\n",
      "doc794.txt\n",
      "doc763.txt\n",
      "doc725.txt\n",
      "doc62.txt\n",
      "doc822.txt\n",
      "doc806.txt\n",
      "doc339.txt\n",
      "doc823.txt\n",
      "doc187.txt\n",
      "doc600.txt\n",
      "doc438.txt\n",
      "doc165.txt\n",
      "doc266.txt\n",
      "doc809.txt\n",
      "doc382.txt\n",
      "doc167.txt\n",
      "doc720.txt\n",
      "doc156.txt\n",
      "doc332.txt\n",
      "doc639.txt\n",
      "doc387.txt\n",
      "doc590.txt\n",
      "doc549.txt\n",
      "doc324.txt\n",
      "doc854.txt\n",
      "doc514.txt\n",
      "doc540.txt\n",
      "doc660.txt\n",
      "doc742.txt\n",
      "doc285.txt\n",
      "doc838.txt\n",
      "doc459.txt\n",
      "doc791.txt\n",
      "doc235.txt\n",
      "doc523.txt\n",
      "doc18.txt\n",
      "doc144.txt\n",
      "doc134.txt\n",
      "doc175.txt\n",
      "doc141.txt\n",
      "doc10.txt\n",
      "doc73.txt\n",
      "doc861.txt\n",
      "doc431.txt\n",
      "doc164.txt\n",
      "doc294.txt\n",
      "doc243.txt\n",
      "doc220.txt\n",
      "doc50.txt\n",
      "doc193.txt\n",
      "doc160.txt\n",
      "doc245.txt\n",
      "doc416.txt\n",
      "doc69.txt\n",
      "doc143.txt\n",
      "doc96.txt\n",
      "doc460.txt\n",
      "doc183.txt\n",
      "doc484.txt\n",
      "doc443.txt\n",
      "doc20.txt\n",
      "doc524.txt\n",
      "doc654.txt\n",
      "doc215.txt\n",
      "doc486.txt\n",
      "doc262.txt\n",
      "doc214.txt\n",
      "doc642.txt\n",
      "doc128.txt\n",
      "doc634.txt\n",
      "doc793.txt\n",
      "doc6.txt\n",
      "doc329.txt\n",
      "doc159.txt\n",
      "doc488.txt\n",
      "doc679.txt\n",
      "doc745.txt\n",
      "doc306.txt\n",
      "doc126.txt\n",
      "doc450.txt\n",
      "doc670.txt\n",
      "doc569.txt\n",
      "doc259.txt\n",
      "doc625.txt\n",
      "doc150.txt\n",
      "doc837.txt\n",
      "doc717.txt\n",
      "doc138.txt\n",
      "doc477.txt\n",
      "doc319.txt\n",
      "doc761.txt\n",
      "doc718.txt\n",
      "doc157.txt\n",
      "doc831.txt\n",
      "doc95.txt\n",
      "doc617.txt\n",
      "doc765.txt\n",
      "doc117.txt\n",
      "doc621.txt\n",
      "doc75.txt\n",
      "doc264.txt\n",
      "doc381.txt\n",
      "doc240.txt\n",
      "doc206.txt\n",
      "doc13.txt\n",
      "doc384.txt\n",
      "doc232.txt\n",
      "doc730.txt\n",
      "doc413.txt\n",
      "doc371.txt\n",
      "doc64.txt\n",
      "doc862.txt\n",
      "doc476.txt\n",
      "doc291.txt\n",
      "doc698.txt\n",
      "doc757.txt\n",
      "doc511.txt\n",
      "doc788.txt\n",
      "doc53.txt\n",
      "doc147.txt\n",
      "doc601.txt\n",
      "doc0.txt\n",
      "doc581.txt\n",
      "doc832.txt\n",
      "doc411.txt\n",
      "doc684.txt\n",
      "doc190.txt\n",
      "doc356.txt\n",
      "doc233.txt\n",
      "doc587.txt\n",
      "doc24.txt\n",
      "doc237.txt\n",
      "doc61.txt\n",
      "doc135.txt\n",
      "doc402.txt\n",
      "doc358.txt\n",
      "doc445.txt\n",
      "doc297.txt\n",
      "doc349.txt\n",
      "doc844.txt\n",
      "doc612.txt\n",
      "doc663.txt\n",
      "doc475.txt\n",
      "doc312.txt\n",
      "doc341.txt\n",
      "doc740.txt\n",
      "doc544.txt\n",
      "doc386.txt\n",
      "doc469.txt\n",
      "doc801.txt\n",
      "doc588.txt\n",
      "doc99.txt\n",
      "doc103.txt\n",
      "doc616.txt\n",
      "doc479.txt\n",
      "doc348.txt\n",
      "doc322.txt\n",
      "doc82.txt\n",
      "doc334.txt\n",
      "doc116.txt\n",
      "doc132.txt\n",
      "doc478.txt\n",
      "doc199.txt\n",
      "doc437.txt\n",
      "doc104.txt\n",
      "doc632.txt\n",
      "doc563.txt\n",
      "doc845.txt\n",
      "doc792.txt\n",
      "doc293.txt\n",
      "doc499.txt\n",
      "doc521.txt\n",
      "doc270.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc565.txt\n",
      "doc539.txt\n",
      "doc508.txt\n",
      "doc816.txt\n",
      "doc91.txt\n",
      "doc719.txt\n",
      "doc522.txt\n",
      "doc802.txt\n",
      "doc584.txt\n",
      "doc34.txt\n",
      "doc603.txt\n",
      "doc343.txt\n",
      "doc153.txt\n",
      "doc56.txt\n",
      "doc316.txt\n",
      "doc773.txt\n",
      "doc253.txt\n",
      "doc604.txt\n",
      "doc200.txt\n",
      "doc419.txt\n",
      "doc629.txt\n",
      "doc762.txt\n",
      "doc321.txt\n",
      "doc58.txt\n",
      "doc191.txt\n",
      "doc344.txt\n",
      "doc516.txt\n",
      "doc721.txt\n",
      "doc444.txt\n",
      "doc59.txt\n",
      "doc314.txt\n",
      "doc369.txt\n",
      "doc537.txt\n",
      "doc458.txt\n",
      "doc309.txt\n",
      "doc519.txt\n",
      "doc856.txt\n",
      "doc811.txt\n",
      "doc491.txt\n",
      "doc631.txt\n",
      "doc54.txt\n",
      "doc228.txt\n",
      "doc124.txt\n",
      "doc746.txt\n",
      "doc541.txt\n",
      "doc575.txt\n",
      "doc289.txt\n",
      "doc842.txt\n",
      "doc68.txt\n",
      "doc744.txt\n",
      "doc246.txt\n",
      "doc620.txt\n",
      "doc561.txt\n",
      "doc734.txt\n",
      "doc695.txt\n",
      "doc597.txt\n",
      "doc485.txt\n",
      "doc749.txt\n",
      "doc433.txt\n",
      "doc733.txt\n",
      "doc728.txt\n",
      "doc155.txt\n",
      "doc515.txt\n",
      "doc201.txt\n",
      "doc326.txt\n",
      "doc3.txt\n",
      "doc454.txt\n",
      "doc90.txt\n",
      "doc573.txt\n",
      "doc77.txt\n",
      "doc661.txt\n",
      "doc820.txt\n",
      "doc767.txt\n",
      "doc455.txt\n",
      "doc644.txt\n",
      "doc146.txt\n",
      "doc83.txt\n",
      "doc446.txt\n",
      "doc139.txt\n",
      "doc313.txt\n",
      "doc133.txt\n",
      "doc421.txt\n",
      "doc496.txt\n",
      "doc409.txt\n",
      "doc558.txt\n",
      "doc342.txt\n",
      "doc31.txt\n",
      "doc268.txt\n",
      "doc186.txt\n",
      "doc8.txt\n",
      "doc562.txt\n",
      "doc279.txt\n",
      "doc123.txt\n",
      "doc554.txt\n",
      "doc464.txt\n",
      "doc580.txt\n",
      "doc682.txt\n",
      "doc307.txt\n",
      "doc535.txt\n",
      "doc320.txt\n",
      "doc452.txt\n",
      "doc367.txt\n",
      "doc94.txt\n",
      "doc302.txt\n",
      "doc829.txt\n",
      "doc93.txt\n",
      "doc136.txt\n",
      "doc784.txt\n",
      "doc180.txt\n",
      "doc181.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK models\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Prepare to handle newline characters in sentences\n",
    "def handle_newlines_in_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if '\\n' in sentence:\n",
    "            # Replace newline characters with a period and space, then re-tokenize\n",
    "            sub_sentences = sent_tokenize(sentence.replace('\\n', '. '))\n",
    "            processed_sentences.extend(sub_sentences)\n",
    "        else:\n",
    "            processed_sentences.append(sentence)\n",
    "    return processed_sentences\n",
    "\n",
    "# Path to the corpus directory\n",
    "path = \"/home/elson/corpus/\"\n",
    "evidence_documents = []\n",
    "\n",
    "# Iterate through each text file in the directory\n",
    "for efile in os.listdir(path):\n",
    "    if efile.endswith(\".txt\"):\n",
    "        print(efile)  # Print the file name\n",
    "        file_path = os.path.join(path, efile)  # Construct full file path\n",
    "        evidence = read_text_file(file_path)  # Read text from the file\n",
    "        \n",
    "        # Tokenize the document into sentences\n",
    "        sentences = sent_tokenize(evidence)\n",
    "        \n",
    "        # Handle newline characters within sentences\n",
    "        sentences = handle_newlines_in_sentences(sentences)\n",
    "        \n",
    "        # Filter and add unique sentences of adequate length\n",
    "        for sentence in sentences:\n",
    "            if (sentence not in evidence_documents) and len(sentence) >= 40:\n",
    "                evidence_documents.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8c95703eb06d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:54:55.203607700Z",
     "start_time": "2024-01-09T13:54:55.188760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While many studies have shown a connection between stress and autoimmune disease,  most of the evidence for stress contributing to the onset and course of  autoimmune disease is circumstantial and the mechanisms by which stress affects  autoimmune disease are not fully understood.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb1709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/elson/corpus.txt', 'w') as f:\n",
    "    for line in evidence_documents:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77c2705fdea44427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:54:58.603643600Z",
     "start_time": "2024-01-09T13:54:55.201168200Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_documents = [remove_punctuation(doc) for doc in evidence_documents]\n",
    "preprocessed_documents = [doc.lower() for doc in preprocessed_documents]\n",
    "preprocessed_documents = [doc.replace('\\n', ' ') for doc in preprocessed_documents]\n",
    "preprocessed_documents = [doc.replace('\\t', ' ') for doc in preprocessed_documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce0b87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51fc30dd31b783ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:00.281073Z",
     "start_time": "2024-01-09T13:54:58.606208400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/elson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8ab3339e32a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Remove stopwords from each document in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed_documents = [' '.join([word for word in doc.split() if word not in stop_words]) \n\u001b[0;32m----> 8\u001b[0;31m                           for doc in preprocessed_documents]\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_documents' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document in the list\n",
    "preprocessed_documents = [' '.join([word for word in doc.split() if word not in stop_words]) \n",
    "                          for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b1f7f1834de011",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-09T13:55:00.286423300Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4810/4810 [01:09<00:00, 69.38it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = bi_encoder.encode(evidence_documents, convert_to_tensor=True, show_progress_bar=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda0ff628f800bf6",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153910/153910 [00:02<00:00, 66316.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for doc in tqdm(evidence_documents):\n",
    "    tokenized_corpus.append(bm25_tokenizer(doc))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acefea721a404a17",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:10]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], evidence_documents[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, evidence_documents[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:10]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], evidence_documents[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:10]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], evidence_documents[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef9b23789ddca289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T15:21:25.062102300Z",
     "start_time": "2023-12-20T15:21:21.034048200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: You can't get thyroid disease if you're young.\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t17.651\tBut a TSH test can't show what is causing a thyroid problem.\n",
      "\t14.024\tSide effects are more likely if you're:.\n",
      "\t13.225\tIf you're using both topical corticosteroids and.\n",
      "\t13.225\tThis is also true even if you're already feeling better.\n",
      "\t13.225\tIf you're looking to donate for the first time, find out more about.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.797\thence proving there is no relation of age to thyroid diseases.\n",
      "\t0.747\tThyroid Disorders in Childhood and Adolescence.\n",
      "\t0.740\tAnd TSH levels may be higher in people over age 80, even though they don't have any thyroid problems.\n",
      "\t0.727\tIf you have a history of thyroid disease, be sure to talk with your provider if you are pregnant or are thinking of becoming pregnant.\n",
      "\t0.723\tThyroid-related medical problems are exceedingly common.\n",
      "\t0.722\tThe unique challenge to the provider of  adolescent health care is that thyroid problems can adversely affect growth and  development during puberty, a crucial period of hormonal interaction.\n",
      "\t0.711\tThyroid disorders are common in adolescence.\n",
      "\t0.709\tThyroid disorders include autoimmune thyroid diseases (AITD), thyroid goiter, nodule and cancer.\n",
      "\t0.699\tPrevious diagnosis with thyroid disease.\n",
      "\t0.694\tThyroid disorders are one of the most underdiagnosed and neglected medical problems, and the lack of general patient knowledge may be of considerable concern [.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t1.116\tThyroid Disorders in Childhood and Adolescence.\n",
      "\t1.038\tThyroid disorders are common in adolescence.\n",
      "\t-0.132\tAnd TSH levels may be higher in people over age 80, even though they don't have any thyroid problems.\n",
      "\t-0.202\thence proving there is no relation of age to thyroid diseases.\n",
      "\t-0.255\tThe unique challenge to the provider of  adolescent health care is that thyroid problems can adversely affect growth and  development during puberty, a crucial period of hormonal interaction.\n",
      "\t-0.670\tIf you have a history of thyroid disease, be sure to talk with your provider if you are pregnant or are thinking of becoming pregnant.\n",
      "\t-1.576\tDo you think females are more at risk of having thyroid diseases?\n",
      "\t-1.634\tThis  chapter addresses the diagnosis, treatment alternatives, and prognosis for a  variety of common and uncommon thyroid abnormalities in adolescents.\n",
      "\t-2.072\tThyroid disorders include autoimmune thyroid diseases (AITD), thyroid goiter, nodule and cancer.\n",
      "\t-2.124\tThyroid disease requires special care in pregnant women or those desiring pregnancy.\n"
     ]
    }
   ],
   "source": [
    "search(\"You can't get thyroid disease if you're young.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "563ce8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_ce(query):\n",
    "    # Initializations\n",
    "    top_hits = [None, None, None]  # To store the top 3 hits\n",
    "    \n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]\n",
    "    \n",
    "    cross_inp = [[query, evidence_documents[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx, hit in enumerate(hits):\n",
    "        hit['cross-score'] = cross_scores[idx]\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    \n",
    "    for i, hit in enumerate(hits[:3]):\n",
    "        if i < len(top_hits):\n",
    "            top_hits[i] = evidence_documents[hit['corpus_id']]\n",
    "\n",
    "    return top_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7c6ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./factcheck/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./factcheck/lib/python3.6/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./factcheck/lib/python3.6/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./factcheck/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in ./factcheck/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ae80459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in ./factcheck/lib/python3.6/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in ./factcheck/lib/python3.6/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23bb0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"/home/elson/Claims_withGeminiAnnotation.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df3793e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "# Convert all claims to strings\n",
    "df['claim'] = df['claim'].astype(str)\n",
    "\n",
    "# Apply the function and expand the returned list into separate columns\n",
    "df[['top_1_minilm_ce', 'top_2_minilm_ce', 'top_3_minilm_ce']] = df['claim'].apply(lambda claim: pd.Series(search_ce(claim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2ecfee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>filename</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>GOLD EXPLANATION</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>gemini_label</th>\n",
       "      <th>gemini_explanation</th>\n",
       "      <th>top_1_minilm_ce</th>\n",
       "      <th>top_2_minilm_ce</th>\n",
       "      <th>top_3_minilm_ce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/content/drive/MyDrive/images/myths on urticaria</td>\n",
       "      <td>mythsonurticaria1.jpeg</td>\n",
       "      <td>Eating chocolate will cause acne.</td>\n",
       "      <td>SUPPORTED</td>\n",
       "      <td>https://www.jaad.org/article/S0190-9622(16)013...</td>\n",
       "      <td>The chocolate consumption group had a statisti...</td>\n",
       "      <td>Skin</td>\n",
       "      <td>REFUTED</td>\n",
       "      <td>There is no scientific evidence to support the...</td>\n",
       "      <td>The link of chocolate to acne vulgaris was rep...</td>\n",
       "      <td>A 2021 systematic review of 53 studies (11 int...</td>\n",
       "      <td>Fig 2 demonstrates that the chocolate consumpt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/content/drive/MyDrive/images/myths on urticaria</td>\n",
       "      <td>mythsonurticaria1.jpeg</td>\n",
       "      <td>You can get a cold from being in the rain.</td>\n",
       "      <td>SUPPORTED</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/17705968/</td>\n",
       "      <td>Exposure to cold has often been associated wi...</td>\n",
       "      <td>General Health</td>\n",
       "      <td>REFUTED</td>\n",
       "      <td>The common cold is caused by viruses, not by b...</td>\n",
       "      <td>The data available suggest that exposure to co...</td>\n",
       "      <td>This mechanism can explain why a person who ex...</td>\n",
       "      <td>As a general observation, wet hair in cold wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/content/drive/MyDrive/images/myths on urticaria</td>\n",
       "      <td>mythsonurticaria1.jpeg</td>\n",
       "      <td>Stress can cause acne.</td>\n",
       "      <td>SUPPORTED</td>\n",
       "      <td>https://medicaljournalssweden.se/actadv/articl...</td>\n",
       "      <td>Based on this study, increased stress does not...</td>\n",
       "      <td>Skin</td>\n",
       "      <td>SUPPORTED</td>\n",
       "      <td>There is evidence to suggest that stress can t...</td>\n",
       "      <td>This finding provides physiological support to...</td>\n",
       "      <td>The impact of pyschological stress on acne.</td>\n",
       "      <td>Both active acne and post-inflammatory hyperpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/content/drive/MyDrive/images/myths on urticaria</td>\n",
       "      <td>mythsonurticaria1.jpeg</td>\n",
       "      <td>You can prevent acne by washing your face more...</td>\n",
       "      <td>NOT ENOUGH INFORMATION</td>\n",
       "      <td>https://www.tandfonline.com/doi/full/10.1080/0...</td>\n",
       "      <td>Washing and over-the-counter cleansers are com...</td>\n",
       "      <td>Skin</td>\n",
       "      <td>REFUTED</td>\n",
       "      <td>Washing your face more often does not prevent ...</td>\n",
       "      <td>Purpose: Washing and over-the-counter cleanser...</td>\n",
       "      <td>Patients can also be advised to pat dry their ...</td>\n",
       "      <td>Treatment of acne should be started early to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/content/drive/MyDrive/images/myths on vascula...</td>\n",
       "      <td>mythsonvascularsurgery3.jpeg</td>\n",
       "      <td>Varicose veins are caused by standing too much.</td>\n",
       "      <td>SUPPORTED</td>\n",
       "      <td>https://www.sjweh.fi/article/562</td>\n",
       "      <td>For men working mostly in a standing position,...</td>\n",
       "      <td>Vascular</td>\n",
       "      <td>NOT ENOUGH INFORMATION</td>\n",
       "      <td>While standing for long periods of time can co...</td>\n",
       "      <td>Varicose veins are superficial veins in the su...</td>\n",
       "      <td>Varicose veins are caused by poorly functionin...</td>\n",
       "      <td>Varicose veins are caused by poorly functionin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              folder  \\\n",
       "0   /content/drive/MyDrive/images/myths on urticaria   \n",
       "1   /content/drive/MyDrive/images/myths on urticaria   \n",
       "2   /content/drive/MyDrive/images/myths on urticaria   \n",
       "3   /content/drive/MyDrive/images/myths on urticaria   \n",
       "4  /content/drive/MyDrive/images/myths on vascula...   \n",
       "\n",
       "                       filename  \\\n",
       "0        mythsonurticaria1.jpeg   \n",
       "1        mythsonurticaria1.jpeg   \n",
       "2        mythsonurticaria1.jpeg   \n",
       "3        mythsonurticaria1.jpeg   \n",
       "4  mythsonvascularsurgery3.jpeg   \n",
       "\n",
       "                                               claim                   label  \\\n",
       "0                  Eating chocolate will cause acne.               SUPPORTED   \n",
       "1         You can get a cold from being in the rain.               SUPPORTED   \n",
       "2                             Stress can cause acne.               SUPPORTED   \n",
       "3  You can prevent acne by washing your face more...  NOT ENOUGH INFORMATION   \n",
       "4    Varicose veins are caused by standing too much.               SUPPORTED   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.jaad.org/article/S0190-9622(16)013...   \n",
       "1          https://pubmed.ncbi.nlm.nih.gov/17705968/   \n",
       "2  https://medicaljournalssweden.se/actadv/articl...   \n",
       "3  https://www.tandfonline.com/doi/full/10.1080/0...   \n",
       "4                   https://www.sjweh.fi/article/562   \n",
       "\n",
       "                                    GOLD EXPLANATION        CATEGORY  \\\n",
       "0  The chocolate consumption group had a statisti...            Skin   \n",
       "1   Exposure to cold has often been associated wi...  General Health   \n",
       "2  Based on this study, increased stress does not...            Skin   \n",
       "3  Washing and over-the-counter cleansers are com...            Skin   \n",
       "4  For men working mostly in a standing position,...        Vascular   \n",
       "\n",
       "             gemini_label                                 gemini_explanation  \\\n",
       "0                 REFUTED  There is no scientific evidence to support the...   \n",
       "1                 REFUTED  The common cold is caused by viruses, not by b...   \n",
       "2               SUPPORTED  There is evidence to suggest that stress can t...   \n",
       "3                 REFUTED  Washing your face more often does not prevent ...   \n",
       "4  NOT ENOUGH INFORMATION  While standing for long periods of time can co...   \n",
       "\n",
       "                                     top_1_minilm_ce  \\\n",
       "0  The link of chocolate to acne vulgaris was rep...   \n",
       "1  The data available suggest that exposure to co...   \n",
       "2  This finding provides physiological support to...   \n",
       "3  Purpose: Washing and over-the-counter cleanser...   \n",
       "4  Varicose veins are superficial veins in the su...   \n",
       "\n",
       "                                     top_2_minilm_ce  \\\n",
       "0  A 2021 systematic review of 53 studies (11 int...   \n",
       "1  This mechanism can explain why a person who ex...   \n",
       "2        The impact of pyschological stress on acne.   \n",
       "3  Patients can also be advised to pat dry their ...   \n",
       "4  Varicose veins are caused by poorly functionin...   \n",
       "\n",
       "                                     top_3_minilm_ce  \n",
       "0  Fig 2 demonstrates that the chocolate consumpt...  \n",
       "1  As a general observation, wet hair in cold wea...  \n",
       "2  Both active acne and post-inflammatory hyperpi...  \n",
       "3  Treatment of acne should be started early to p...  \n",
       "4  Varicose veins are caused by poorly functionin...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a629d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"/home/elson/topk3_minilm.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b99a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
