{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758487ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elson/factcheck/lib/python3.6/site-packages/ipykernel_launcher.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm.autonotebook import tqdm\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GenericDataLoader:\n",
    "\n",
    "    def __init__(self, data_folder: str = None, prefix: str = None, corpus_file: str = \"corpus.jsonl\", query_file: str = \"queries.jsonl\",\n",
    "                 qrels_folder: str = \"qrels\", qrels_file: str = \"\"):\n",
    "        self.corpus = {}\n",
    "        self.queries = {}\n",
    "        self.qrels = {}\n",
    "\n",
    "        if prefix:\n",
    "            query_file = prefix + \"-\" + query_file\n",
    "            qrels_folder = prefix + \"-\" + qrels_folder\n",
    "\n",
    "        self.corpus_file = os.path.join(data_folder, corpus_file) if data_folder else corpus_file\n",
    "        self.query_file = os.path.join(data_folder, query_file) if data_folder else query_file\n",
    "        self.qrels_folder = os.path.join(data_folder, qrels_folder) if data_folder else None\n",
    "        self.qrels_file = qrels_file\n",
    "\n",
    "    @staticmethod\n",
    "    def check(fIn: str, ext: str):\n",
    "        if not os.path.exists(fIn):\n",
    "            raise ValueError(\"File {} not present! Please provide accurate file.\".format(fIn))\n",
    "\n",
    "        if not fIn.endswith(ext):\n",
    "            raise ValueError(\"File {} must be present with extension {}\".format(fIn, ext))\n",
    "\n",
    "    def load_custom(self) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "\n",
    "        self.check(fIn=self.corpus_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.query_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.qrels_file, ext=\"tsv\")\n",
    "\n",
    "        if not len(self.corpus):\n",
    "            logger.info(\"Loading Corpus...\")\n",
    "            self._load_corpus()\n",
    "            logger.info(\"Loaded %d Documents.\", len(self.corpus))\n",
    "            logger.info(\"Doc Example: %s\", list(self.corpus.values())[0])\n",
    "\n",
    "        if not len(self.queries):\n",
    "            logger.info(\"Loading Queries...\")\n",
    "            self._load_queries()\n",
    "\n",
    "        if os.path.exists(self.qrels_file):\n",
    "            self._load_qrels()\n",
    "            self.queries = {qid: self.queries[qid] for qid in self.qrels}\n",
    "            logger.info(\"Loaded %d Queries.\", len(self.queries))\n",
    "            logger.info(\"Query Example: %s\", list(self.queries.values())[0])\n",
    "\n",
    "        return self.corpus, self.queries, self.qrels\n",
    "\n",
    "    def load(self, split=\"test\") -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "\n",
    "        self.qrels_file = os.path.join(self.qrels_folder, split + \".tsv\")\n",
    "        self.check(fIn=self.corpus_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.query_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.qrels_file, ext=\"tsv\")\n",
    "\n",
    "        if not len(self.corpus):\n",
    "            logger.info(\"Loading Corpus...\")\n",
    "            self._load_corpus()\n",
    "            logger.info(\"Loaded %d %s Documents.\", len(self.corpus), split.upper())\n",
    "            logger.info(\"Doc Example: %s\", list(self.corpus.values())[0])\n",
    "\n",
    "        if not len(self.queries):\n",
    "            logger.info(\"Loading Queries...\")\n",
    "            self._load_queries()\n",
    "\n",
    "        if os.path.exists(self.qrels_file):\n",
    "            self._load_qrels()\n",
    "            self.queries = {qid: self.queries[qid] for qid in self.qrels}\n",
    "            logger.info(\"Loaded %d %s Queries.\", len(self.queries), split.upper())\n",
    "            logger.info(\"Query Example: %s\", list(self.queries.values())[0])\n",
    "\n",
    "        return self.corpus, self.queries, self.qrels\n",
    "\n",
    "    def load_corpus(self) -> Dict[str, Dict[str, str]]:\n",
    "\n",
    "        self.check(fIn=self.corpus_file, ext=\"jsonl\")\n",
    "\n",
    "        if not len(self.corpus):\n",
    "            logger.info(\"Loading Corpus...\")\n",
    "            self._load_corpus()\n",
    "            logger.info(\"Loaded %d Documents.\", len(self.corpus))\n",
    "            logger.info(\"Doc Example: %s\", list(self.corpus.values())[0])\n",
    "\n",
    "        return self.corpus\n",
    "\n",
    "    def _load_corpus(self):\n",
    "\n",
    "        num_lines = sum(1 for i in open(self.corpus_file, 'rb'))\n",
    "        with open(self.corpus_file, encoding='utf8') as fIn:\n",
    "            for line in tqdm(fIn, total=num_lines):\n",
    "                line = json.loads(line)\n",
    "                self.corpus[line.get(\"_id\")] = {\n",
    "                    \"text\": line.get(\"text\"),\n",
    "                    \"title\": line.get(\"title\"),\n",
    "                }\n",
    "\n",
    "    def _load_queries(self):\n",
    "\n",
    "        with open(self.query_file, encoding='utf8') as fIn:\n",
    "            for line in fIn:\n",
    "                line = json.loads(line)\n",
    "                self.queries[line.get(\"_id\")] = line.get(\"text\")\n",
    "\n",
    "    def _load_qrels(self):\n",
    "\n",
    "        reader = csv.reader(open(self.qrels_file, encoding=\"utf-8\"),\n",
    "                            delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        next(reader)\n",
    "\n",
    "        for id, row in enumerate(reader):\n",
    "            query_id, corpus_id, score = row[0], row[1], int(row[2])\n",
    "\n",
    "            if query_id not in self.qrels:\n",
    "                self.qrels[query_id] = {corpus_id: score}\n",
    "            else:\n",
    "                self.qrels[query_id][corpus_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a9f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict\n",
    "\n",
    "class BaseSearch(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def search(self, \n",
    "               corpus: Dict[str, Dict[str, str]], \n",
    "               queries: Dict[str, str], \n",
    "               top_k: int, \n",
    "               **kwargs) -> Dict[str, Dict[str, float]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857e60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dot_score(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = dot_prod(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    return torch.mm(a, b.transpose(0, 1))\n",
    "\n",
    "def cos_sim(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8939a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from typing import Dict\n",
    "import heapq\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# DenseRetrievalExactSearch is parent class for any dense model that can be used for retrieval\n",
    "# Abstract class is BaseSearch\n",
    "class DenseRetrievalExactSearch(BaseSearch):\n",
    "    \n",
    "    def __init__(self, model, batch_size: int = 128, corpus_chunk_size: int = 50000, **kwargs):\n",
    "        #model is class that provides encode_corpus() and encode_queries()\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.score_functions = {'cos_sim': cos_sim, 'dot': dot_score}\n",
    "        self.score_function_desc = {'cos_sim': \"Cosine Similarity\", 'dot': \"Dot Product\"}\n",
    "        self.corpus_chunk_size = corpus_chunk_size\n",
    "        self.show_progress_bar = kwargs.get(\"show_progress_bar\", True)\n",
    "        self.convert_to_tensor = kwargs.get(\"convert_to_tensor\", True)\n",
    "        self.results = {}\n",
    "    \n",
    "    def search(self, \n",
    "               corpus: Dict[str, Dict[str, str]], \n",
    "               queries: Dict[str, str], \n",
    "               top_k: int, \n",
    "               score_function: str,\n",
    "               return_sorted: bool = False, \n",
    "               **kwargs) -> Dict[str, Dict[str, float]]:\n",
    "        # Create embeddings for all queries using model.encode_queries()\n",
    "        # Runs semantic search against the corpus embeddings\n",
    "        # Returns a ranked list with the corpus ids\n",
    "        if score_function not in self.score_functions:\n",
    "            raise ValueError(\"score function: {} must be either (cos_sim) for cosine similarity or (dot) for dot product\".format(score_function))\n",
    "            \n",
    "        logger.info(\"Encoding Queries...\")\n",
    "        query_ids = list(queries.keys())\n",
    "        self.results = {qid: {} for qid in query_ids}\n",
    "        queries = [queries[qid] for qid in queries]\n",
    "        query_embeddings = self.model.encode_queries(\n",
    "            queries, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_tensor=self.convert_to_tensor)\n",
    "          \n",
    "        logger.info(\"Sorting Corpus by document length (Longest first)...\")\n",
    "\n",
    "        corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get(\"title\", \"\") + corpus[k].get(\"text\", \"\")), reverse=True)\n",
    "        corpus = [corpus[cid] for cid in corpus_ids]\n",
    "\n",
    "        logger.info(\"Encoding Corpus in batches... Warning: This might take a while!\")\n",
    "        logger.info(\"Scoring Function: {} ({})\".format(self.score_function_desc[score_function], score_function))\n",
    "\n",
    "        itr = range(0, len(corpus), self.corpus_chunk_size)\n",
    "        \n",
    "        result_heaps = {qid: [] for qid in query_ids}  # Keep only the top-k docs for each query\n",
    "        for batch_num, corpus_start_idx in enumerate(itr):\n",
    "            logger.info(\"Encoding Batch {}/{}...\".format(batch_num+1, len(itr)))\n",
    "            corpus_end_idx = min(corpus_start_idx + self.corpus_chunk_size, len(corpus))\n",
    "\n",
    "            # Encode chunk of corpus    \n",
    "            sub_corpus_embeddings = self.model.encode_corpus(\n",
    "                corpus[corpus_start_idx:corpus_end_idx],\n",
    "                batch_size=self.batch_size,\n",
    "                show_progress_bar=self.show_progress_bar, \n",
    "                convert_to_tensor = self.convert_to_tensor\n",
    "                )\n",
    "\n",
    "            # Compute similarites using either cosine-similarity or dot product\n",
    "            cos_scores = self.score_functions[score_function](query_embeddings, sub_corpus_embeddings)\n",
    "            cos_scores[torch.isnan(cos_scores)] = -1\n",
    "\n",
    "            # Get top-k values\n",
    "            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(top_k+1, len(cos_scores[1])), dim=1, largest=True, sorted=return_sorted)\n",
    "            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n",
    "            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n",
    "            \n",
    "            for query_itr in range(len(query_embeddings)):\n",
    "                query_id = query_ids[query_itr]                  \n",
    "                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):\n",
    "                    corpus_id = corpus_ids[corpus_start_idx+sub_corpus_id]\n",
    "                    if corpus_id != query_id:\n",
    "                        if len(result_heaps[query_id]) < top_k:\n",
    "                            # Push item on the heap\n",
    "                            heapq.heappush(result_heaps[query_id], (score, corpus_id))\n",
    "                        else:\n",
    "                            # If item is larger than the smallest in the heap, push it on the heap then pop the smallest element\n",
    "                            heapq.heappushpop(result_heaps[query_id], (score, corpus_id))\n",
    "\n",
    "        for qid in result_heaps:\n",
    "            for score, corpus_id in result_heaps[qid]:\n",
    "                self.results[qid][corpus_id] = score\n",
    "        \n",
    "        return self.results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3de118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import List, Dict,Union\n",
    "import numpy as np\n",
    "class SPubMedBERT:\n",
    "    def __init__(self, model_path: Union[str, Tuple] = None, sep: str = \" \", **kwargs):\n",
    "        self.sep = sep     \n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.model.to(device)\n",
    "        if isinstance(model_path, str):\n",
    "            self.q_model = SentenceTransformer(model_path)\n",
    "            self.doc_model = self.q_model\n",
    "\n",
    "    def encode_queries(self, queries: List[str], batch_size: int = 16, **kwargs) -> Union[List[Tensor], np.ndarray, Tensor]:\n",
    "            return self.q_model.encode(queries, batch_size=batch_size, **kwargs)\n",
    "\n",
    "    def encode_corpus(self, corpus: Union[List[Dict[str, str]], Dict[str, List]], batch_size: int = 8, **kwargs) -> Union[List[Tensor], np.ndarray, Tensor]:\n",
    "        if type(corpus) is dict:\n",
    "            sentences = [(corpus[\"title\"][i] + self.sep + corpus[\"text\"][i]).strip() if \"title\" in corpus else corpus[\"text\"][i].strip() for i in range(len(corpus['text']))]\n",
    "        else:\n",
    "            sentences = [(doc[\"title\"] + self.sep + doc[\"text\"]).strip() if \"title\" in doc else doc[\"text\"].strip() for doc in corpus]\n",
    "        return self.doc_model.encode(sentences, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b40771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 350, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "device = \"cuda:3\" \n",
    "model_path = 'output/NeuML/pubmedbert-base-embeddings-v3-msmarco/'\n",
    "model = SentenceTransformer(model_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128ea746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "def mrr(qrels: Dict[str, Dict[str, int]], \n",
    "        results: Dict[str, Dict[str, float]], \n",
    "        k_values: List[int]) -> Tuple[Dict[str, float]]:\n",
    "    \n",
    "    MRR = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        MRR[f\"MRR@{k}\"] = 0.0\n",
    "    \n",
    "    k_max, top_hits = max(k_values), {}\n",
    "    logging.info(\"\\n\")\n",
    "    \n",
    "    for query_id, doc_scores in results.items():\n",
    "        top_hits[query_id] = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)[0:k_max]   \n",
    "    \n",
    "    for query_id in top_hits:\n",
    "        query_relevant_docs = set([doc_id for doc_id in qrels[query_id] if qrels[query_id][doc_id] > 0])    \n",
    "        for k in k_values:\n",
    "            for rank, hit in enumerate(top_hits[query_id][0:k]):\n",
    "                if hit[0] in query_relevant_docs:\n",
    "                    MRR[f\"MRR@{k}\"] += 1.0 / (rank + 1)\n",
    "                    break\n",
    "\n",
    "    for k in k_values:\n",
    "        MRR[f\"MRR@{k}\"] = round(MRR[f\"MRR@{k}\"]/len(qrels), 5)\n",
    "        logging.info(\"MRR@{}: {:.4f}\".format(k, MRR[f\"MRR@{k}\"]))\n",
    "\n",
    "    return MRR\n",
    "\n",
    "def recall_cap(qrels: Dict[str, Dict[str, int]], \n",
    "               results: Dict[str, Dict[str, float]], \n",
    "               k_values: List[int]) -> Tuple[Dict[str, float]]:\n",
    "    \n",
    "    capped_recall = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        capped_recall[f\"R_cap@{k}\"] = 0.0\n",
    "    \n",
    "    k_max = max(k_values)\n",
    "    logging.info(\"\\n\")\n",
    "    \n",
    "    for query_id, doc_scores in results.items():\n",
    "        top_hits = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)[0:k_max]   \n",
    "        query_relevant_docs = [doc_id for doc_id in qrels[query_id] if qrels[query_id][doc_id] > 0]\n",
    "        for k in k_values:\n",
    "            retrieved_docs = [row[0] for row in top_hits[0:k] if qrels[query_id].get(row[0], 0) > 0]\n",
    "            denominator = min(len(query_relevant_docs), k)\n",
    "            capped_recall[f\"R_cap@{k}\"] += (len(retrieved_docs) / denominator)\n",
    "\n",
    "    for k in k_values:\n",
    "        capped_recall[f\"R_cap@{k}\"] = round(capped_recall[f\"R_cap@{k}\"]/len(qrels), 5)\n",
    "        logging.info(\"R_cap@{}: {:.4f}\".format(k, capped_recall[f\"R_cap@{k}\"]))\n",
    "\n",
    "    return capped_recall\n",
    "\n",
    "\n",
    "def hole(qrels: Dict[str, Dict[str, int]], \n",
    "               results: Dict[str, Dict[str, float]], \n",
    "               k_values: List[int]) -> Tuple[Dict[str, float]]:\n",
    "    \n",
    "    Hole = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        Hole[f\"Hole@{k}\"] = 0.0\n",
    "    \n",
    "    annotated_corpus = set()\n",
    "    for _, docs in qrels.items():\n",
    "        for doc_id, score in docs.items():    \n",
    "            annotated_corpus.add(doc_id)\n",
    "    \n",
    "    k_max = max(k_values)\n",
    "    logging.info(\"\\n\")\n",
    "    \n",
    "    for _, scores in results.items():\n",
    "        top_hits = sorted(scores.items(), key=lambda item: item[1], reverse=True)[0:k_max]\n",
    "        for k in k_values:\n",
    "            hole_docs = [row[0] for row in top_hits[0:k] if row[0] not in annotated_corpus]\n",
    "            Hole[f\"Hole@{k}\"] += len(hole_docs) / k\n",
    "\n",
    "    for k in k_values:\n",
    "        Hole[f\"Hole@{k}\"] = round(Hole[f\"Hole@{k}\"]/len(qrels), 5)\n",
    "        logging.info(\"Hole@{}: {:.4f}\".format(k, Hole[f\"Hole@{k}\"]))\n",
    "\n",
    "    return Hole\n",
    "\n",
    "def top_k_accuracy(\n",
    "        qrels: Dict[str, Dict[str, int]], \n",
    "        results: Dict[str, Dict[str, float]], \n",
    "        k_values: List[int]) -> Tuple[Dict[str, float]]:\n",
    "    \n",
    "    top_k_acc = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        top_k_acc[f\"Accuracy@{k}\"] = 0.0\n",
    "    \n",
    "    k_max, top_hits = max(k_values), {}\n",
    "    logging.info(\"\\n\")\n",
    "    \n",
    "    for query_id, doc_scores in results.items():\n",
    "        top_hits[query_id] = [item[0] for item in sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)[0:k_max]]\n",
    "    \n",
    "    for query_id in top_hits:\n",
    "        query_relevant_docs = set([doc_id for doc_id in qrels[query_id] if qrels[query_id][doc_id] > 0])\n",
    "        for k in k_values:\n",
    "            for relevant_doc_id in query_relevant_docs:\n",
    "                if relevant_doc_id in top_hits[query_id][0:k]:\n",
    "                    top_k_acc[f\"Accuracy@{k}\"] += 1.0\n",
    "                    break\n",
    "\n",
    "    for k in k_values:\n",
    "        top_k_acc[f\"Accuracy@{k}\"] = round(top_k_acc[f\"Accuracy@{k}\"]/len(qrels), 5)\n",
    "        logging.info(\"Accuracy@{}: {:.4f}\".format(k, top_k_acc[f\"Accuracy@{k}\"]))\n",
    "\n",
    "    return top_k_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a90dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrec_eval\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "class EvaluateRetrieval:\n",
    "    \n",
    "    def __init__(self, retriever: BaseSearch = None, k_values: List[int] = [1,3,5,10,100,1000], score_function: str = \"cos_sim\"):\n",
    "        self.k_values = k_values\n",
    "        self.top_k = max(k_values)\n",
    "        self.retriever = retriever\n",
    "        self.score_function = score_function\n",
    "            \n",
    "    def retrieve(self, corpus: Dict[str, Dict[str, str]], queries: Dict[str, str], **kwargs) -> Dict[str, Dict[str, float]]:\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Model/Technique has not been provided!\")\n",
    "        return self.retriever.search(corpus, queries, self.top_k, self.score_function, **kwargs)\n",
    "    \n",
    "    def rerank(self, \n",
    "            corpus: Dict[str, Dict[str, str]], \n",
    "            queries: Dict[str, str],\n",
    "            results: Dict[str, Dict[str, float]],\n",
    "            top_k: int) -> Dict[str, Dict[str, float]]:\n",
    "    \n",
    "        new_corpus = {}\n",
    "    \n",
    "        for query_id in results:\n",
    "            if len(results[query_id]) > top_k:\n",
    "                for (doc_id, _) in sorted(results[query_id].items(), key=lambda item: item[1], reverse=True)[:top_k]:\n",
    "                    new_corpus[doc_id] = corpus[doc_id]\n",
    "            else:\n",
    "                for doc_id in results[query_id]:\n",
    "                    new_corpus[doc_id] = corpus[doc_id]\n",
    "                    \n",
    "        return self.retriever.search(new_corpus, queries, top_k, self.score_function)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(qrels: Dict[str, Dict[str, int]], \n",
    "                 results: Dict[str, Dict[str, float]], \n",
    "                 k_values: List[int],\n",
    "                 ignore_identical_ids: bool=True) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], Dict[str, float]]:\n",
    "        \n",
    "        if ignore_identical_ids:\n",
    "            logger.info('For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to ignore this.')\n",
    "            popped = []\n",
    "            for qid, rels in results.items():\n",
    "                for pid in list(rels):\n",
    "                    if qid == pid:\n",
    "                        results[qid].pop(pid)\n",
    "                        popped.append(pid)\n",
    "\n",
    "        ndcg = {}\n",
    "        _map = {}\n",
    "        recall = {}\n",
    "        precision = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            ndcg[f\"NDCG@{k}\"] = 0.0\n",
    "            _map[f\"MAP@{k}\"] = 0.0\n",
    "            recall[f\"Recall@{k}\"] = 0.0\n",
    "            precision[f\"P@{k}\"] = 0.0\n",
    "        \n",
    "        map_string = \"map_cut.\" + \",\".join([str(k) for k in k_values])\n",
    "        ndcg_string = \"ndcg_cut.\" + \",\".join([str(k) for k in k_values])\n",
    "        recall_string = \"recall.\" + \",\".join([str(k) for k in k_values])\n",
    "        precision_string = \"P.\" + \",\".join([str(k) for k in k_values])\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {map_string, ndcg_string, recall_string, precision_string})\n",
    "        scores = evaluator.evaluate(results)\n",
    "        \n",
    "        for query_id in scores.keys():\n",
    "            for k in k_values:\n",
    "                ndcg[f\"NDCG@{k}\"] += scores[query_id][\"ndcg_cut_\" + str(k)]\n",
    "                _map[f\"MAP@{k}\"] += scores[query_id][\"map_cut_\" + str(k)]\n",
    "                recall[f\"Recall@{k}\"] += scores[query_id][\"recall_\" + str(k)]\n",
    "                precision[f\"P@{k}\"] += scores[query_id][\"P_\"+ str(k)]\n",
    "        \n",
    "        for k in k_values:\n",
    "            ndcg[f\"NDCG@{k}\"] = round(ndcg[f\"NDCG@{k}\"]/len(scores), 5)\n",
    "            _map[f\"MAP@{k}\"] = round(_map[f\"MAP@{k}\"]/len(scores), 5)\n",
    "            recall[f\"Recall@{k}\"] = round(recall[f\"Recall@{k}\"]/len(scores), 5)\n",
    "            precision[f\"P@{k}\"] = round(precision[f\"P@{k}\"]/len(scores), 5)\n",
    "        \n",
    "        for eval in [ndcg, _map, recall, precision]:\n",
    "            logger.info(\"\\n\")\n",
    "            for k in eval.keys():\n",
    "                logger.info(\"{}: {:.4f}\".format(k, eval[k]))\n",
    "\n",
    "        return ndcg, _map, recall, precision\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_custom(qrels: Dict[str, Dict[str, int]], \n",
    "                 results: Dict[str, Dict[str, float]], \n",
    "                 k_values: List[int], metric: str) -> Tuple[Dict[str, float]]:\n",
    "        \n",
    "        if metric.lower() in [\"mrr\", \"mrr@k\", \"mrr_cut\"]:\n",
    "            return mrr(qrels, results, k_values)\n",
    "        \n",
    "        elif metric.lower() in [\"recall_cap\", \"r_cap\", \"r_cap@k\"]:\n",
    "            return recall_cap(qrels, results, k_values)\n",
    "        \n",
    "        elif metric.lower() in [\"hole\", \"hole@k\"]:\n",
    "            return hole(qrels, results, k_values)\n",
    "        \n",
    "        elif metric.lower() in [\"acc\", \"top_k_acc\", \"accuracy\", \"accuracy@k\", \"top_k_accuracy\"]:\n",
    "            return top_k_accuracy(qrels, results, k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32039bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8841823/8841823 [00:40<00:00, 216843.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.56it/s]\n",
      "Batches: 100%|██████████| 391/391 [04:31<00:00,  1.44it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:52<00:00,  1.68it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:41<00:00,  1.76it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:34<00:00,  1.82it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:22<00:00,  1.93it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:20<00:00,  1.95it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:13<00:00,  2.02it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:12<00:00,  2.03it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:03<00:00,  2.13it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:06<00:00,  2.10it/s]\n",
      "Batches: 100%|██████████| 391/391 [03:00<00:00,  2.17it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:59<00:00,  2.18it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:54<00:00,  2.24it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:53<00:00,  2.25it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:50<00:00,  2.30it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:50<00:00,  2.29it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:46<00:00,  2.35it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:45<00:00,  2.36it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:41<00:00,  2.43it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:37<00:00,  2.48it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:40<00:00,  2.44it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:36<00:00,  2.49it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:39<00:00,  2.46it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:53<00:00,  2.26it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:41<00:00,  2.43it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:38<00:00,  2.47it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:35<00:00,  2.52it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:34<00:00,  2.53it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:32<00:00,  2.56it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:28<00:00,  2.64it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:32<00:00,  2.57it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:29<00:00,  2.61it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:27<00:00,  2.65it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:29<00:00,  2.61it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:28<00:00,  2.63it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:27<00:00,  2.64it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:24<00:00,  2.71it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:22<00:00,  2.74it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:23<00:00,  2.72it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:19<00:00,  2.81it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:17<00:00,  2.83it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:14<00:00,  2.92it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:14<00:00,  2.91it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:10<00:00,  3.01it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:11<00:00,  2.96it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:10<00:00,  2.99it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:05<00:00,  3.10it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:07<00:00,  3.07it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:12<00:00,  2.96it/s]\n",
      "Batches: 100%|██████████| 391/391 [02:03<00:00,  3.18it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:58<00:00,  3.29it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:55<00:00,  3.39it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:59<00:00,  3.27it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:52<00:00,  3.48it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:55<00:00,  3.39it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:50<00:00,  3.54it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:48<00:00,  3.60it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:47<00:00,  3.65it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:46<00:00,  3.67it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:46<00:00,  3.67it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:43<00:00,  3.77it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:43<00:00,  3.79it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:44<00:00,  3.75it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:41<00:00,  3.84it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:39<00:00,  3.93it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:40<00:00,  3.88it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:43<00:00,  3.80it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:40<00:00,  3.88it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:41<00:00,  3.83it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:40<00:00,  3.89it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:38<00:00,  3.96it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:39<00:00,  3.92it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:38<00:00,  3.96it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:40<00:00,  3.87it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:40<00:00,  3.89it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:38<00:00,  3.97it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:38<00:00,  3.98it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:37<00:00,  3.99it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.05it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:37<00:00,  4.02it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:39<00:00,  3.92it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.07it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.07it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:37<00:00,  4.00it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.04it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:35<00:00,  4.11it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:37<00:00,  4.01it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.04it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.07it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:34<00:00,  4.16it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:35<00:00,  4.10it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:34<00:00,  4.14it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.06it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:36<00:00,  4.07it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:34<00:00,  4.13it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:31<00:00,  4.28it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:33<00:00,  4.19it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:33<00:00,  4.19it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:31<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:33<00:00,  4.20it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:31<00:00,  4.29it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:32<00:00,  4.21it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:33<00:00,  4.16it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:32<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:34<00:00,  4.15it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:30<00:00,  4.32it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:30<00:00,  4.30it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:31<00:00,  4.30it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:29<00:00,  4.35it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:28<00:00,  4.42it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:28<00:00,  4.43it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:29<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:29<00:00,  4.38it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:29<00:00,  4.39it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:26<00:00,  4.51it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:29<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:27<00:00,  4.47it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:26<00:00,  4.53it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:26<00:00,  4.50it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:29<00:00,  4.39it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:25<00:00,  4.56it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:25<00:00,  4.56it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:23<00:00,  4.66it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:28<00:00,  4.43it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:24<00:00,  4.60it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:24<00:00,  4.61it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:25<00:00,  4.60it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:24<00:00,  4.61it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:26<00:00,  4.51it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:24<00:00,  4.62it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:23<00:00,  4.68it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:24<00:00,  4.63it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:21<00:00,  4.80it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:24<00:00,  4.60it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:21<00:00,  4.77it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:20<00:00,  4.84it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:20<00:00,  4.85it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:20<00:00,  4.86it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:22<00:00,  4.75it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:19<00:00,  4.93it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:20<00:00,  4.88it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:18<00:00,  4.95it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:20<00:00,  4.85it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:16<00:00,  5.08it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:18<00:00,  4.98it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:18<00:00,  4.99it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:16<00:00,  5.09it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:16<00:00,  5.14it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:17<00:00,  5.05it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:15<00:00,  5.21it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:13<00:00,  5.29it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:14<00:00,  5.25it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:14<00:00,  5.28it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:13<00:00,  5.33it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:12<00:00,  5.39it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:11<00:00,  5.46it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:10<00:00,  5.57it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:09<00:00,  5.61it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:09<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:07<00:00,  5.78it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:06<00:00,  5.84it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:08<00:00,  5.75it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:05<00:00,  5.97it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:04<00:00,  6.07it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:04<00:00,  6.04it/s]\n",
      "Batches: 100%|██████████| 391/391 [01:01<00:00,  6.37it/s]\n",
      "Batches: 100%|██████████| 391/391 [00:59<00:00,  6.55it/s]\n",
      "Batches: 100%|██████████| 391/391 [00:57<00:00,  6.79it/s]\n",
      "Batches: 100%|██████████| 391/391 [00:55<00:00,  7.03it/s]\n",
      "Batches: 100%|██████████| 391/391 [00:54<00:00,  7.17it/s]\n",
      "Batches: 100%|██████████| 391/391 [00:48<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 391/391 [00:42<00:00,  9.25it/s]\n",
      "Batches: 100%|██████████| 327/327 [00:29<00:00, 10.95it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'datasets/msmarco'\n",
    "model_path='output/NeuML/pubmedbert-base-embeddings-v3-msmarco/'\n",
    "\n",
    "#### Provide the data_path where nfcorpus has been downloaded and unzipped\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "#### Provide your custom model class name --> HERE\n",
    "model = DenseRetrievalExactSearch(SPubMedBERT(model_path))\n",
    "\n",
    "retriever = EvaluateRetrieval(model, score_function=\"cos_sim\") # or \"dot\" if you wish dot-product\n",
    "\n",
    "#### Retrieve dense results (format of results is identical to qrels)\n",
    "results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "#### Evaluate your retrieval using NDCG@k, MAP@K ...\n",
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3329bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NDCG@1': 0.75581, 'NDCG@3': 0.7053, 'NDCG@5': 0.69094, 'NDCG@10': 0.66196, 'NDCG@100': 0.53517, 'NDCG@1000': 0.60143} {'MAP@1': 0.02227, 'MAP@3': 0.05565, 'MAP@5': 0.07811, 'MAP@10': 0.12631, 'MAP@100': 0.28975, 'MAP@1000': 0.33878} {'Recall@1': 0.02227, 'Recall@3': 0.05825, 'Recall@5': 0.0827, 'Recall@10': 0.13515, 'Recall@100': 0.3978, 'Recall@1000': 0.63835} {'P@1': 0.90698, 'P@3': 0.83721, 'P@5': 0.79535, 'P@10': 0.72558, 'P@100': 0.29442, 'P@1000': 0.05547}\n"
     ]
    }
   ],
   "source": [
    "print(ndcg, _map, recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a146ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
